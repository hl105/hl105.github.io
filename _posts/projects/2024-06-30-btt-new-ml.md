---

title: "[BTT] New ML"
excerpt: "new machine learning concepts"
date: 2024-05-30
lastmod: 2024-07-30 23:04:29 -0400
last_modified_at: 2024-07-30 23:04:29 -0400
categories: project
tags: ML BTT ensemble_methods
classes:
toc: true
toc_label:
toc_sticky: true
header:
    image:
    teaser:
    overlay_image: ./assets/images/banners/default.png
sitemap:
    changefreq: daily
    priority: 1.0
author:
---

<!--postNo: 2024-07-30-->


stat review:


###  $R^2 $ (R-squared)

- **Definition**: Proportion of variance in the dependent variable explained by the independent variable(s) in the regression model.
- **Interpretation**: Indicates how well the model explains the variability of the data.
- **Formula**:
$R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}i)^2}{\sum{i=1}^n (y_i - \bar{y})^2} $

### RMSE (Root Mean Squared Error)

- **Definition**: The square root of the mean squared error, measuring the magnitude of the errors in predictions.
- **Interpretation**: Indicates how close the predicted values are to the actual values. Lower values indicate better fit.
- **Formula**:
$RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2} $

new models:


### Ensemble models


```r
estimators = [("DT", DecisionTreeRegressor(max_depth=8, min_samples_leaf=25)),
              ("LR", LinearRegression())
             ]
             
print('Implement Stacking...')

stacking_model = StackingRegressor(estimators=estimators, passthrough=False)
stacking_model.fit(X_train, y_train)
print('End')

# 1. Use the fitted model to make predictions on the test data
stacking_pred = stacking_model.predict(X_test)

# 2. Compute the RMSE 
stack_rmse = mean_squared_error(stacking_pred, y_test)

# 3. Compute the R2 score
stack_r2 = r2_score(stacking_pred, y_test)
```


###  **Gradient Boosted Decision Trees**


all notes from [https://developers.google.com/machine-learning/decision-forests/intro-to-gbdt](https://developers.google.com/machine-learning/decision-forests/intro-to-gbdt)


gradient boosting involves two types of models:

- a "weak" machine learning model, which is typically a decision tree.
- a "strong" machine learning model, which is composed of multiple weak models.

at each step, a new weak model is trained to predict the **error** of the current strong model. This is called **pseudo response** 


Gradient boosting is iterative: $F_{i+1} = F_i - f_i$ is repeated. $F_i$ is the strong model, $f_i$ is the weak model at step $i$. The goal is to update the strong model $F_i$ by adding a new weak model  that corrects the errors made by the strong model $F_i$


<figure>
                      <img src="https://res.cloudinary.com/df2rp6zoo/image/upload/v1722395072/js6ao3xscdxdgonak2ra.png" alt="">
                      <figcaption></figcaption>
                  </figure>


<figure>
                      <img src="https://res.cloudinary.com/df2rp6zoo/image/upload/v1722395073/qtozoz1ywurwqdppzwtk.png" alt="">
                      <figcaption></figcaption>
                  </figure>


second plot: strong model - ground truth


week model f_0 is trained to predict the residuals/errors from the previous step. So basically it’s trying to capture the general shape of the second error plot


<figure>
                      <img src="https://res.cloudinary.com/df2rp6zoo/image/upload/v1722395075/v3ifnc9nrzz5wjzqc2pj.png" alt="">
                      <figcaption></figcaption>
                  </figure>


shrinkage: = learning rate in neural networks. It’s just $F_{i+1} = F_i - vf_i$


…and so on 


 


