---

title: "some paper readings"
excerpt: "some readings for summer 2024 research"
date: 2024-06-03
lastmod: 2024-06-03 15:53:54 -0400
last_modified_at: 2024-06-03 15:53:54 -0400
categories: project
tags: audit paper search
classes:
toc: true
toc_label:
toc_sticky: true
header:
    image:
    teaser:
    overlay_image: ./assets/images/banners/default.png
sitemap:
    changefreq: daily
    priority: 1.0
author:
---

Note: **Many of the sentences or phrases in the first pass are directly copied from the original paper,** as the goal of this part of the review was to extract the RQs and write down how the paper answers them (first pass). Citations are at the end of the page. 


### Pass 1 (title, abstract, introduction, headings, conclusion)

- [**Assessing Google Search’s New Features in Supporting Credibility Judgments of Unknown Website**](https://doi.org/10.1145/3576840.3578277)**s**
	- RQ1: Are users familiar with the new Google features “About this page” and “More about this page”?
	- RQ2: Are the 9 W3C domain credibility signals useful?
	- answered by: user study with 25 undergrad students
- [The Media Coverage of the 2020 US Presidential Election Candidates through the Lens of Google's Top Stories](https://ojs.aaai.org/index.php/ICWSM/article/view/7352)
	- abstract:
		- many news sources, but we are only exposed to a certain few through news aggregators. Google top stories is one of them. A very small number of sources dominate the section, with a highly skewed distribution.
		- Dataset: duration -  1 year of 30 political candidate queries, frequency: 4-12 daily observation to measure the “freshness” of news stories
	- RQs
		- RQ1: Which News Sources does the Top Stories Algorithm Prefer?
			- **inequality of news sources:** 2,168 total news sources, but 1/3 of all articles were from only 8 news publishers
		- RQ2: Which Presidential Candidates do the News Sources Prefer?
			- **inequality of candidates**: top mentions of candidates: Biden, Warren, Sanders, Buttigieg (Excluding Trump — in office)
- [**The case for voter-centered audits of search engines during political elections**](https://dl.acm.org/doi/10.1145/3351095.3372835)
	- RQ: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate?
	- Four datasets:
		- a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections
		- a dataset of biased political phrases used in a large-scale Google audit ahead of
		the 2018 U.S. election
		- Google’s “related searches” phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women)
		- autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019.
	- Introduction:
		- motivation: Trump lost the popular vote in 2016, but Google cited a conspiracy blog and claimed Trump won on the top search result page for the query “final vote count 2016”
		- Why is auditing necessary?
			- Unlike Twitter which alerted its users of false content generated by Russia’s Internet Research Agency, Google simply fixes the problem without the same transparency. How many users searched for the problematic query? How did they fix it?
			- some might say it’s a protective measure — what if the hackers exploit the solution to improve their methods? → the lack of exposure of disinformation on the web is harmful to the public — e.g. Dylan Roof’s hate crime that started from searching “black on white crime” on Google. (Data void)
		- Three methods to detect political bias on search platforms:
			- Third-party manipulation:
				- “Google bombing” in the early 2000s
			- Ranking Bias
				- “search engine manipulation effect”, tied to “filter bubbles”
			- ecosystem bias
				- consider the complexity of search platforms — users, content providers, ranking, etc.
		- How should we design search engine audits that are voter-centric?
			- theory of “information cues”: voters prefer to take shortcuts to get informed about elections.
			- biased searches: need to come from voters themselves
			- beyond Candidate Names: voters first search who are the candidates → then modify the search so that they are more specfic
			- unreliable localization: ___ “near me” ← localized suggestions
		- conclusion: future search engine audits go beyond identifying whether their ranking algorithms are biased, but instead, take a broader ecosystem approach.
- [**Capturing the Aftermath of the Dobbs v. Jackson Women’s Health Organization Decision in Google Search Results across the U.S.**](https://ojs.aaai.org/index.php/ICWSM/article/view/22214)
	- Dataset: more than 1.74 million Google SERPs collected in the aftermath of the Dobbs v. Jackson Women’s Health PRganization Decision. Can be used to answer questions such as:
		- How do Google Search results change following an impactful real-world event, such as the U.S. Supreme Court decision on June 24, 2022 to overturn Roe v. Wade?
		- What do they tell us about the nature of event-driven content, generated by
		various participants in the online information environment?
	- Dataset Summary:
		- 65 locations (using Google localized search), June 24th to July 17th 2022. 1,698 search phrases.
		- ~ 1.7 million HTML pages,  ~20k unique URLs from  ~5k websites in organic search results,  ~17k  unique URLs from  ~2k websites in top stories.
		- [Dataset Link](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2FYFAH9X)
- [**Opening Up the Black Box: Auditing Google’s Top Stories Algorithm**](https://cdn.aaai.org/ocs/18316/18316-78935-1-PB.pdf)
	- Audit of the Top Stories Panel (data collection, exploration, and analysis)
	- Suggests Google might be addressing the “filter bubble” issue by selecting less known publishers for the 3rd position in the Top Stories panel.
	- RQ1, RQ2: A novel audit of the Google’s Top stories panel that pro-
	vides insights into its algorithmic choices for selecting and ranking news publishers.
		- 1% of publishers (11 out of 1,125) produce 41% of total articles and are present
		in 46% of observations of the Top Stories panel.
		- the number of sources in the 3rd position is more than double that of sources in the 1st position.
	- RQ3: Evidence about the potential of using audit results from news aggregation platforms (e.g., Google) to answer questions relevant to media communication theory such as media selection bias (e.g., which publishers cover which stories)
		- What events or people are publishers choosing to report on any given day?
		- e.g. the top sources for the query “hilary clinton” were “Washington Examiner” and “Fox News”
		- “selection bias”  is indeed observed through hierarchal clustering of 65 publishers.

### All paper links:

- [https://dl.acm.org/doi/10.1145/3576840.3578277](https://dl.acm.org/doi/10.1145/3576840.3578277)
- [https://ojs.aaai.org/index.php/ICWSM/article/view/7352](https://ojs.aaai.org/index.php/ICWSM/article/view/7352)
- [https://dl.acm.org/doi/10.1145/3351095.3372835](https://dl.acm.org/doi/10.1145/3351095.3372835)
- [https://ojs.aaai.org/index.php/ICWSM/article/view/22214](https://ojs.aaai.org/index.php/ICWSM/article/view/22214)
- [https://cdn.aaai.org/ocs/18316/18316-78935-1-PB.pdf](https://cdn.aaai.org/ocs/18316/18316-78935-1-PB.pdf)

	![giphy.gif](https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExNHcyOXFzcWR5ZDI0ajduMzFpNmp6Yzh0bzQ0aXJ0MWEzZXZoMnZjYiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/rOkbPKO2dlXzO/giphy.gif)
