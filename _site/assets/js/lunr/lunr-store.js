var store = [{
        "title": "welcome to my blog",
        "excerpt":"    under construction!   Jekyll  instructions   Installation   # instructions commands    What to do   more things to do   Reference  some links  ","categories": ["life"],
        "tags": ["Jekyll","blog_logistics"],
        "url": "/life/welcome/",
        "teaser": null
      },{
        "title": "stable diffusion",
        "excerpt":"  I asked GPT to give me a nice metaphor for stable video diffusion, liked it, so I’m putting it here:   1. Understanding the Task:      Metaphor: Imagine you’re an artist who receives a detailed description of a scene and your job is to paint it as accurately as possible.   Example: The description says, “A sunset over a mountain range with a river flowing through the valley.”   2. The Initial Sketch (Noise):      Metaphor: Before starting, you randomly splash paint on the canvas to create a base layer.   Example: You cover the canvas with random colors and patterns, which looks nothing like the final image.   3. Diffusion Process (Gradual Refinement):      Metaphor: You slowly start refining the chaotic splashes into something more recognizable by repeatedly smoothing and adding details.   Example: You begin to see hints of the sunset, mountains, and river as you refine the random splashes into more structured forms.   4. The Generative Model:      Metaphor: Imagine having a guidebook that tells you how to convert random splashes into specific parts of the scene step by step.   Example: The guidebook says, “If you see a splash of red in this area, turn it into the sky, blending it with shades of orange and yellow for the sunset.”   5. Training the Model (Learning from Examples):      Metaphor: Before you became a skilled artist, you practiced a lot by looking at thousands of scenes and trying to paint them. Over time, you learned patterns and techniques to create realistic paintings from descriptions.   Example: You practiced by painting various sunsets, mountains, and rivers, learning how different elements look and how to represent them on canvas.   6. Noise to Image Transformation:      Metaphor: Each time you refine the canvas, you look at the guidebook and adjust the painting accordingly, gradually turning the random splashes into a coherent scene.   Example: You repeatedly refer to the guidebook, adjust the colors, shapes, and details, and eventually, the chaotic splashes transform into a beautiful sunset over a mountain range with a river.   7. Mathematical Magic:      Metaphor: Behind the scenes, there’s a complex set of rules (mathematical equations) that guide your transformations. These rules ensure that each refinement step brings you closer to the final image.   Example: The guidebook contains these rules, which are learned from your practice sessions. These rules tell you how to mix colors, shape objects, and add details accurately.   8. Inference (Generating New Images):      Metaphor: Now that you’re a skilled artist with a well-practiced guidebook, you can take any new description and confidently paint a scene from it.   Example: Given a new description like “A forest in autumn with leaves falling,” you can start with random splashes and, using your guidebook, refine them into a detailed and accurate painting of the scene.   Key Components of Stable Diffusion:      Latent Space:            Metaphor: Think of latent space as a magical realm where rough ideas and forms of images live.       Example: It’s like your mind’s eye where you visualize rough shapes and colors before they become a detailed painting.           Forward and Reverse Processes:            Forward Process:                    Metaphor: Adding more and more noise to a clear image until it becomes unrecognizable.           Example: Like scribbling over a clear drawing until it’s just a mess of lines.                       Reverse Process:                    Metaphor: Starting with a noisy image and carefully removing the noise to reveal the clear image underneath.           Example: Like erasing the scribbles bit by bit to reveal the original drawing.                           Conditioning on Text:            Metaphor: The text description acts like a detailed instruction manual that guides you on what the final image should look like.       Example: Without the instruction manual, you wouldn’t know whether to paint a sunset or a cityscape.           Putting It All Together:   When you input a text description into Stable Diffusion:      Start with Noise: The process begins with a noisy canvas (random noise).   Guided Refinement: Using the model (trained guidebook), it iteratively refines the noise based on the text description.   Final Image: After several refinement steps, the noise transforms into a clear and detailed image that matches the description.   ","categories": ["project"],
        "tags": ["gpt","stable_diffusion","metaphor"],
        "url": "/project/stable-diffusion/",
        "teaser": null
      },{
        "title": "notion to blog post md",
        "excerpt":"Now why have I not been writing on this blog? It’s just too much work writing in markdown, creating a file with a specific format, and uploading it. So I created a quick notion to blog direct conversion pipeline.   Creating the notion-to-md.js file   adoption of  https://github.com/souvikinator/notion-to-md   const { Client } = require(\"@notionhq/client\"); const { NotionToMarkdown } = require(\"notion-to-md\"); const fs = require('fs'); // or // import {NotionToMarkdown} from \"notion-to-md\";  const notion = new Client({     auth: \"YOUR_SECRET_KEY\", });  // passing notion client to the option const n2m = new NotionToMarkdown({ notionClient: notion });  (async () =&gt; {     try{         const mdblocks = await n2m.pageToMarkdown(process.argv[2]);         const mdString = n2m.toMarkdownString(mdblocks);         console.log(mdString.parent); //how we pass stdout to shell     }catch (error) {         console.error(error);       } })();   running this file      install nvm   curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.3/install.sh | bash           load nvm        export NVM_DIR=\"$([ -z \"${XDG_CONFIG_HOME-}\" ] &amp;&amp; printf %s \"${HOME}/.nvm\" || printf %s \"${XDG_CONFIG_HOME}/nvm\")\"  [ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\"                Install Node.js version 12.18.1       (current ver throws error :  Failed to convert page to Markdown: TypeError [ERR_INVALID_ARG_TYPE]: The \"data\" argument must be of type string or an instance of Buffer, TypedArray, or DataView. Received an instance of Object)        nvm install 12.18.1  nvm use 12.18.1           node -v should show v12.18.1            Connect your notion page             Create an internal integration and get your API key       Go to the page you want to go and choose +Add Connections       Get the link of the page on the search bar and get the last part (i.e. the page id): it should be something like 7c5e1cf7e4c34a5585f829533b17d3d9           Creating a Notion → blog post pipeline using a Shell Script   it’s still a lot of work (for a lazy person like me) to activate the environment with nvm, run that line, and move the output file into the blog post folder, so I’m going to make a shell script with all the commands.   Note that I am using a conda environment “blog”           create notion-to-blog.sh file        #!/bin/bash \t  # Activate Conda environment  source /Users/ihoonsun/anaconda3/etc/profile.d/conda.sh  conda activate blog \t  # Prompt the user to choose a category  echo \"Select a category for the blog post:\"  select category in \"project\" \"life\"  do      case $category in          project ) output_dir=\"./_posts/projects\"; break;;          life ) output_dir=\"./_posts/life\"; break;;          * ) echo \"Invalid option. Please select a number from the list.\";;      esac  done \t  # Ask the user for the title of the blog post in a regular sentence  read -p \"Enter the title of the blog post: \" title \t  # Generate date and format the title into a filename  date_now=$(date +\"%Y-%m-%d\")  title_formatted=$(echo \"$title\" | tr '[:upper:]' '[:lower:]' | tr -s '[:space:]' '-' | sed 's/[^a-zA-Z0-9\\-]//g')  # Avoid trailing hyphen if title is empty after formatting  filename=\"${date_now}-$(echo $title_formatted | sed 's/-$//').md\"  output_path=\"$output_dir/$filename\" \t  # Check if the file already exists  if [ ! -f \"$output_path\" ]; then      echo \"*Awesome* You are making a new post! ღ'ᴗ'ღ\" \t      # Ask for new Notion page ID to update content      read -p \"Enter the Notion page ID for the new content: \" page_id \t      # Ask the user for header details      read -p \"Enter the excerpt for the blog post: \" excerpt      read -p \"Enter tags for the blog post (space-separated): \" tags      read -p \"Enter overlay image file name (should be in assets/images/banners). Enter 'default.png' to use a template: \" overlay_image      last_modified_at=$(date +\"%Y-%m-%d %H:%M:%S %z\") \t      # Run the Node.js script, get the markdown content, and add the header      node notion-to-md.js $page_id | cat &lt;(echo -e \"---\\n  title: \\\"$title\\\"  excerpt: \\\"$excerpt\\\"  date: $date_now  lastmod: $last_modified_at  last_modified_at: $last_modified_at  categories: $category  tags: $tags  classes:  toc: true  toc_label:  toc_sticky: true  header:      image:      teaser:      overlay_image: ./assets/images/banners/$overlay_image  sitemap:      changefreq: daily      priority: 1.0  author:  ---\\n  &lt;!--postNo: $date_now--&gt;\\n\") - &gt; \"$output_path\" \t      echo \"Markdown file created at $output_path\"  else      echo \"Updating the blog post...\"      # Extract the header from existing file      header=$(awk 'BEGIN {printOn=0;} /^---$/ {if (printOn) {print; exit;} else {printOn=1;}} printOn {print;}' \"$output_path\")      echo $header \t          # Ask for new Notion page ID to update content      read -p \"Enter the Notion page ID for the new content: \" page_id \t      # Get new content and replace the old content      updated_content=$(node notion-to-md.js $page_id)      echo \"$header  $updated_content\" &gt; \"$output_path\" \t      echo \"$output_path was successfully updated!\" \t     git add \"$output_path\"     git commit -m \"Updated blog post: $title\"     git push  fi #end of if block \t \t \t                make the script executable        chmod +x notion-to-blog.sh                 run the script        ./notion-to-blog.sh           Now we have a simple program where the script asks the user the info it needs.   The resulting file is automatically saved in the path like this: ./hl105.github.io/_posts/projects/2024-05-30-7c5e1cf7e4c34a5585f829533b17d3d9.md   …and this is how this post was made!     ","categories": ["project"],
        "tags": ["notion","blog","markdown"],
        "url": "/project/notion-to-blog-post-md/",
        "teaser": null
      },{
        "title": "[BTT] New ML",
        "excerpt":"  stat review:   $R^2 $ (R-squared)      Definition: Proportion of variance in the dependent variable explained by the independent variable(s) in the regression model.   Interpretation: Indicates how well the model explains the variability of the data.   Formula: $R^2 = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}i)^2}{\\sum{i=1}^n (y_i - \\bar{y})^2} $   RMSE (Root Mean Squared Error)      Definition: The square root of the mean squared error, measuring the magnitude of the errors in predictions.   Interpretation: Indicates how close the predicted values are to the actual values. Lower values indicate better fit.   Formula: $RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2} $   new models:   Ensemble models   estimators = [(\"DT\", DecisionTreeRegressor(max_depth=8, min_samples_leaf=25)),               (\"LR\", LinearRegression())              ]               print('Implement Stacking...')  stacking_model = StackingRegressor(estimators=estimators, passthrough=False) stacking_model.fit(X_train, y_train) print('End')  # 1. Use the fitted model to make predictions on the test data stacking_pred = stacking_model.predict(X_test)  # 2. Compute the RMSE  stack_rmse = mean_squared_error(stacking_pred, y_test)  # 3. Compute the R2 score stack_r2 = r2_score(stacking_pred, y_test)    Gradient Boosted Decision Trees   all notes from https://developers.google.com/machine-learning/decision-forests/intro-to-gbdt   gradient boosting involves two types of models:      a “weak” machine learning model, which is typically a decision tree.   a “strong” machine learning model, which is composed of multiple weak models.   at each step, a new weak model is trained to predict the error of the current strong model. This is called pseudo response   Gradient boosting is iterative: $F_{i+1} = F_i - f_i$ is repeated. $F_i$ is the strong model, $f_i$ is the weak model at step $i$. The goal is to update the strong model $F_i$ by adding a new weak model  that corrects the errors made by the strong model $F_i$                                                                                                                                         second plot: strong model - ground truth   week model f_0 is trained to predict the residuals/errors from the previous step. So basically it’s trying to capture the general shape of the second error plot                                                                      shrinkage: = learning rate in neural networks. It’s just $F_{i+1} = F_i - vf_i$   …and so on   ","categories": ["project"],
        "tags": ["ML","BTT","ensemble_methods"],
        "url": "/project/btt-new-ml/",
        "teaser": null
      },{
        "title": "web credibility blog posts",
        "excerpt":"Are women evil? Hacking Google’s search results:   https://medium.com/@enimust/are-women-evil-hacking-googles-search-results-eebfbbffe179   → In 2017, Google suggested evil to autocomplete the query “Are women __”. Why did this happen? The article that appears on the top of the search query contain the phrase many times and contain other signals that may have caused the autocomplete to happen. Thus the article suggests the use of “nutrition labels” for search results. The users can read the labels like “Who was the article written by?” or “Is this a reliable source?” and decide on their own whether the information is credible or not. After this article was written, Google implemented a feature (the three vertical dots icon) where when clicked, the information about the website is displayed. However, let’s be truthful — who clicks on those dots? It will be faster for the users to go to the page, quickly scan it, and get out if it is unsatisfactory. What would be an easier, faster way to let the users know the credibility of a website? This question remains to be answered.   Presidents in the Clan. SEO techniques to hack history:   https://medium.com/@enimust/presidents-in-the-klan-seo-techniques-to-hack-history-953e48dc413c   → This example clearly shows how there is a clear path to exploiting the Google search algorithm to get better rankings on the search result page. This particular example about the presidents in the Klan ended up in the best position possible — the featured snippet section. In high school, I wrote an article about a very niche field (cultural appropriation in Korea written in English). The next week, my article was in the Google snippet! I remember being very excited and texting a lot of friends. I still wonder what part of my article made it appear there, in bold, looking very important. Maybe it was the sources I cited, or the addition of a Korean translation, or the date it was created. Anyways, this blog post also cites some options that may have influenced the article full of fake news making it to the featured snippet. It also suggests some labels that could have prevented this from happening — maybe add an author, or some information about the article. Again, Google later implemented such a feature with the three dots that no one really clicks.   The fake news story that fooled even Maggie Haberman:   https://medium.com/@enimust/the-fake-news-story-that-fooled-even-maggie-haberman-c22453c47169   → How do I know whether a story that is going viral on Instagram, TikTok, etc. is fake news or not? I think about this question on a daily basis, probably because I spend a lot of time (I try to use screen time.. ) on Instagram and discover just scrolling through whatever Instagram thinks I am interested in. When there is a “breaking news” story that I haven’t seen in the New York Times or anywhere that is confirmed to be credible, I first check the 1. number of comments (because one out of the 300 comments must have looked it up for me, right?), 2. the date (if the news is from a year ago but claims to be breaking news, it’s not), and content (are the documents, pictures, etc). But last month, when an Instagram post told me Tom Holland and Zendaya were getting married, I thought it was real — people were congratulating them in the comment section, the date was from yesterday, and there was a cute couple picture on the post! But I was too lazy to open Safari and triple-check, so I started spreading the news until my sister told me it was fake news. Oh well. So what feature would have stopped me from believing these two top Hollywood stars are getting married? Maybe an app that extracts text from a screenshot and tells me verified information about it? What easy verification methods are there that are easier than clicking three dots…      The information panels on Google and Facebook: https://medium.com/@enimust/the-information-panels-on-google-and-facebook-uncovering-their-blind-spots-2e8210b2e697   → We all know that some links on Google search results are sketchy. We make sure to check the .gov pages when we are looking for traveler information, or .edu pages when we are looking for academic information. However, I don’t think I’ve ever questioned the information on the right-side info box. It just seemed right from the start, the one-sentence summary about whatever organization or website I was looking for. This article showed me that I was wrong — I looked up some of the celebrities with criminal records that I know, and it seems like their one-line description is just their Wikipedia page introduction.   More articles on Google search results (will read this weekend):      Are Google’s Top Stories biased? It’s complicated. https://medium.com/@enimust/are-googles-top-stories-politically-biased-it-s-complicated-e9c68e269ed9   It’s a Google drenched society, but we still suffer from information-drought: https://medium.com/@enimust/its-a-google-drenched-society-but-we-still-suffer-from-an-information-drought-4be35132b3d5   Google AI &amp; Health: https://ipscell.com/2024/05/google-ai-overviews-on-stem-cells-are-a-bust-so-far-endanger-public-health/   NYT article on Google’s AI overview feature: https://www.nytimes.com/2024/05/24/technology/google-ai-overview-search.html   → “Eat at least one small rock daily!” Says who? Geologists at UC Berkeley! This is what Google SGE told its users who searched “How many rocks should I eat.” This result, while it may be funny, is problematic because it cites some sources with some author that seems pretty credible. However, no UC Berkeley geologist said that — the source is The Onion, where the sources are made up for fun. But AI can’t tell whether humans should eat rocks or not. It saw a popular website with a lot of traffic, with sources and clear explanations, and decided to use it. While it is an attempt for Google to try to “label” its links, it does the job poorly and leads to more confusion for its users with its new AI feature.     ","categories": ["project"],
        "tags": ["sge","Google","serp","AI"],
        "url": "/project/web-credibility-blog-posts/",
        "teaser": null
      },{
        "title": "[QAI] R intro (week 0)",
        "excerpt":"  QAI Program Week 0. Notebook and lecture from the Quantitative Analysis Institute Summer Program.   Lecture - Basic R commands      Vectors:            similar to numpy array       ex) c(1,2,3,4,5) →  1  2  3  4  5,  c(1:3,5:12) →  1  2  3  5 10 11 12 (right inclusive)       5:10 is valid; but (5:7, 8:10) is invalid bc you need c to combine two expressions       length() to find len       NA for None       merge two vectors by c(vec1, vec2)       exp(vec1), log(vec1) ← applies to all elements in vec           Matrices:            ex) mat1 = matrix(3,nrow=2, ncol=2) #matrix with all 3s       dim(mat1), nrow(mat1), ncol(mat1)                rows and columns            ?matrix #help page \t\t  # first row, first column  mat1[1,1] \t\t  # all rows, column 1  mat1[,1] \t\t  # first row, all columns  mat1[1,] \t\t  # assign names to rows and columns  rownames(mat1) &lt;- c(\"Row1\", \"Row2\")  colnames(mat1) &lt;- c(\"Col1\", \"Col2\") \t\t  # see names of rows and columns  rownames(mat1) # \"Row1\" \"Row2\"  colnames(mat1) # \"Col1\" \"Col2\" \t\t  # view column 1  mat1[,\"Col1\"] \t\t  matrix(1:100, ncol=50)# nrow not assigned, R does it for you; 100/50 = 2 rows  # if both nrow, ncol not assigned, R makes 1 col                                matrix operations            # Element-by-Element Multiplication  mat1*mat2 \t\t  is.matrix(mat3)  is.vector(vec1)  as.matrix(vec1) # vertical matrix                           Datasets            Dataset Examination           \t  ?swiss \t  # EXAMINE THE DATASET  head(swiss) # see first few rows  tail(swiss) # see last few rows   swiss # see entire data set  dim(swiss)# number of rows and columns  ncol(swiss)   nrow(swiss)  colnames(swiss)  summary(swiss) #min/1st Quartile/mean/3rd Q/max \t \t  # R differentiates between \"matrix\" objects and \"data frame\" objects.  # When you do read.csv/read.table it'll be a dataframe  is.matrix(swiss)  is.data.frame(swiss)  swiss &lt;- as.data.frame(swiss) \t  swiss$Fertility #refer to a column (Fertility is col name)  #OR you can do this (not recommended)  attach(swiss)  Fertility  #OR you can also do:  swiss[,\"Fertility\"]  swiss[,1]           b. Dataset Summary        mean(swiss$Fertility)  var(swiss$Fertility)  summary(swiss$Agriculture) #for continuous variables  table(swiss$Education) #stem and leaf plot (for discrete variables)  table(swiss$Education, swiss$Examination) # how many times each combination appeared           c. Subsetting Dataset        FertilitySubset&lt;-swiss$Fertility[swiss$Agriculture&gt;50]  summary(swiss$Fertility[swiss$Agriculture&gt;50]) #fertitilty where agr &gt; 50  summary(swiss$Fertility[swiss$Agriculture&gt;50 &amp; swiss$Catholic&gt;50]) #AND           d. Plotting Dataset        hist(swiss$Fertility)  boxplot(swiss$Fertility)  plot(swiss$Agriculture, swiss$Fertility) \t  plot(swiss$Agriculture, swiss$Fertility,      main=\"Title Goes Here\",      xlab=\"X-axis label goes here\",      ylab=\"Y-axis label goes here\",      xlim=c(0,100),      ylim=c(0,100),      pch=21,      cex=5,      col=\"red\")                Logic        # What if we compare a vector of numbers to another number?  vec1&lt;-c(2,4,5,6)  vec1&gt;3 # R distributes the 3 for you \t  vec2&lt;-2:5  vec1==vec2 #TRUE                Conditioning        vec1[1] #1st el in vec1 (just like python list indexing)  #same as  vec1[c(TRUE, FALSE, FALSE, FALSE)]  vec1[vec1==vec2] #same logic as above                Graphics       \t  par(mfrow=c(1,2)) #mfrow = how to split window  hist(rnorm(100))  boxplot(rnorm(100))  par(las=1) # turns horizontal labels to make it easier to read \t  abline(v=10) # add a vertical line  abline(h=0, lwd=3) # add a horizontal line, and make it thick using lwd:  # add a line with intercept -3 and slope .1, change the line type via lty and the color via col  abline(-3,.1, lty=2,col=\"orange\") \t  # For plots, here's an excerpt from the RPrimer:  plot(swiss$Agriculture, swiss$Fertility,      main=\"Title Goes Here\",      xlab=\"X-axis label goes here\",      ylab=\"Y-axis label goes here\",      xlim=c(0,100),      ylim=c(0,100),      pch=1, #point character (circle dot, triangle dot, x dot, etc.)      cex=5, # default point size will be multiplied by this number      col=\"red\") \t  colors() # names of available colors \t                Packages        install.packages(\"perm\") #just once  library(perm) #import                importing a dataset        getwd()  setwd()  d=read.csv(file=\"path\",header=TRUE)           ","categories": ["R"],
        "tags": ["qai","R","statistics"],
        "url": "/r/qai-r-intro-week-0/",
        "teaser": null
      },{
        "title": "some paper readings",
        "excerpt":"Note: Many of the sentences or phrases in the first pass are directly copied from the original paper, as the goal of this part of the review was to extract the RQs and write down how the paper answers them (first pass). Citations are at the end of the page.   Pass 1 (title, abstract, introduction, headings, conclusion)      Assessing Google Search’s New Features in Supporting Credibility Judgments of Unknown Websites            RQ1: Are users familiar with the new Google features “About this page” and “More about this page”?       RQ2: Are the 9 W3C domain credibility signals useful?       answered by: user study with 25 undergrad students           The Media Coverage of the 2020 US Presidential Election Candidates through the Lens of Google’s Top Stories            abstract:                    many news sources, but we are only exposed to a certain few through news aggregators. Google top stories is one of them. A very small number of sources dominate the section, with a highly skewed distribution.           Dataset: duration -  1 year of 30 political candidate queries, frequency: 4-12 daily observation to measure the “freshness” of news stories                       RQs                    RQ1: Which News Sources does the Top Stories Algorithm Prefer?                            inequality of news sources: 2,168 total news sources, but 1/3 of all articles were from only 8 news publishers                                   RQ2: Which Presidential Candidates do the News Sources Prefer?                            inequality of candidates: top mentions of candidates: Biden, Warren, Sanders, Buttigieg (Excluding Trump — in office)                                                   The case for voter-centered audits of search engines during political elections            RQ: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate?       Four datasets:                    a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections           a dataset of biased political phrases used in a large-scale Google audit ahead of   the 2018 U.S. election           Google’s “related searches” phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women)           autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019.                       Introduction:                    motivation: Trump lost the popular vote in 2016, but Google cited a conspiracy blog and claimed Trump won on the top search result page for the query “final vote count 2016”           Why is auditing necessary?                            Unlike Twitter which alerted its users of false content generated by Russia’s Internet Research Agency, Google simply fixes the problem without the same transparency. How many users searched for the problematic query? How did they fix it?               some might say it’s a protective measure — what if the hackers exploit the solution to improve their methods? → the lack of exposure of disinformation on the web is harmful to the public — e.g. Dylan Roof’s hate crime that started from searching “black on white crime” on Google. (Data void)                                   Three methods to detect political bias on search platforms:                            Third-party manipulation:                                    “Google bombing” in the early 2000s                                               Ranking Bias                                    “search engine manipulation effect”, tied to “filter bubbles”                                               ecosystem bias                                    consider the complexity of search platforms — users, content providers, ranking, etc.                                                                   How should we design search engine audits that are voter-centric?                            theory of “information cues”: voters prefer to take shortcuts to get informed about elections.               biased searches: need to come from voters themselves               beyond Candidate Names: voters first search who are the candidates → then modify the search so that they are more specfic               unreliable localization: ___ “near me” ← localized suggestions                                   conclusion: future search engine audits go beyond identifying whether their ranking algorithms are biased, but instead, take a broader ecosystem approach.                           Capturing the Aftermath of the Dobbs v. Jackson Women’s Health Organization Decision in Google Search Results across the U.S.            Dataset: more than 1.74 million Google SERPs collected in the aftermath of the Dobbs v. Jackson Women’s Health PRganization Decision. Can be used to answer questions such as:                    How do Google Search results change following an impactful real-world event, such as the U.S. Supreme Court decision on June 24, 2022 to overturn Roe v. Wade?           What do they tell us about the nature of event-driven content, generated by   various participants in the online information environment?                       Dataset Summary:                    65 locations (using Google localized search), June 24th to July 17th 2022. 1,698 search phrases.           ~ 1.7 million HTML pages,  ~20k unique URLs from  ~5k websites in organic search results,  ~17k  unique URLs from  ~2k websites in top stories.           Dataset Link                           Opening Up the Black Box: Auditing Google’s Top Stories Algorithm            Audit of the Top Stories Panel (data collection, exploration, and analysis)       Suggests Google might be addressing the “filter bubble” issue by selecting less known publishers for the 3rd position in the Top Stories panel.       RQ1, RQ2: A novel audit of the Google’s Top stories panel that pro-   vides insights into its algorithmic choices for selecting and ranking news publishers.                    1% of publishers (11 out of 1,125) produce 41% of total articles and are present   in 46% of observations of the Top Stories panel.           the number of sources in the 3rd position is more than double that of sources in the 1st position.                       RQ3: Evidence about the potential of using audit results from news aggregation platforms (e.g., Google) to answer questions relevant to media communication theory such as media selection bias (e.g., which publishers cover which stories)                    What events or people are publishers choosing to report on any given day?           e.g. the top sources for the query “hilary clinton” were “Washington Examiner” and “Fox News”           “selection bias”  is indeed observed through hierarchal clustering of 65 publishers.                           All paper links:      https://dl.acm.org/doi/10.1145/3576840.3578277   https://ojs.aaai.org/index.php/ICWSM/article/view/7352   https://dl.acm.org/doi/10.1145/3351095.3372835   https://ojs.aaai.org/index.php/ICWSM/article/view/22214        https://cdn.aaai.org/ocs/18316/18316-78935-1-PB.pdf             ","categories": ["project"],
        "tags": ["audit","paper","search"],
        "url": "/project/some-paper-readings/",
        "teaser": null
      },{
        "title": "[QAI] Statistical Testing (week 1)",
        "excerpt":"  sampling and bias      unit:            thing we are studying       often row in dataset       ex) people, households, mic, bags of dirt           target population:            set of units you’d like to learn about       ex) all Wellesley students, all US households, …           estimand:            a number that we wish we knew in order to answer the research question       the estimand could be calculated in a straightforward way if we had data of every unit in the target population           Sampling:            the link between target population and the data you have           Census:            an attempt to collect info from all units in the target population       ex) US Census       problems: resource heavy, misses certain subgroups systematically — ex) homeless ppl       big data / data science: sometimes we can calculate the estimand because we do have all the units in target population           Sample population / sampling frame            def: set of units with some chance of being included in your dataset       ex) households with phone numbers listed       it is very possible for parts of sample population to not be in target population       goal: choose a data collection method such that sample pop is as similar as possible to target population       sample:                    set of units for which you attempt to collect data           can be also used to describe units in your data set           we have most control over choosing a sample from sample population                       respondents: set of units actually in your data set       ex) interested in surveying Wellesley students. Puts all Wellesley students’ names in a hat and draws 50. But only 30 ppl responded when contacted                    people in sample: 50.           target population: all Wellesley students.           sample population: all Wellesley students.           respondents: 30 who responded                           Sampling methods            haphazard; convenience → likely not representable       simple random sample (SRS)                    all units in sample pop are in a hat, a predetermined number of units is selected           all subsets of size n have the same probability of being the sample           SRS is default assumption for most common statistical methods                       stratified sampling: group units based on characteristics, take SRS from each group       cluster sampling: divide units into clusters, do a SRS on clusters (pick all of one cluster) → for convenience       systematic sampling: include every kth unit → sequential. ex) exit poll       Bernoulli Sampling: flip a coin for each unit to decide whether they are in your sample → sequential           Bias            non-response bias: respondents are not representative of sample       selection bias: when the sample is not representative of the target population, because either -                    sample population is not representative of target           sample not rep of sample population                           Comparing two groups, and summary table            Assigning units in sample to groups:                    parallels sampling from a group           goal is to create groups that are representative of each other           haphazard or any of the random strategies we listed                       assigning groups, sample from population:                    if assigning group is random - infer causation.           if sample from group is random - easy to generalize           both random → very rare, but ideal           not random, random → survey           random, not random → lab experiments           not random, not random → most studies                           Intro to Hypothesis Testing      R.A. Fisher’s Lady tasting tea            8 cups of tea. 4 milk first, 4 tea first. Lady’s job is to pick out the 4 that was poured milk first       possibilities: 4 correct, …, 0 correct           Counting:            4 correct → 1 way       3 correct → 4x4 = 16 ways       2 correct → 6x6=36 ways       1 correct → 4x4 = 16 ways       0 correct → 1 way           Comparing truth to distribution:            p-value: assuming the lady is guessing at random, the prob she should have gotten all 4 correct is 1/70.       she did get all 4 correct! Evidence contradicts that she was guessing at random.           Hypothesis Tests:            data @ 2: reference distribution — we can check how extreme (surprising) is the value we actually saw?       null hypothesis: assumption about target population       test statistic: something you can calculate from the sample that you actually have           Intro to Non-Parametric Tests      Defining terms:            hypothesis tests: proof by contradiction       null Hypothesis (H_0): based on the assumption, typically that there is no effect or no pattern       statistic: number that can be calculated from data       test statistic: statistic used to calculate H_0       distribution: a list of possible values of a random numeric quantity, along with their probabilities       reference distribution: distribution of the test statistic, assuming H_0 is true.           Randomization test assuming simple random sample            HCAS harvard case example: of those offered help. 76% won. Of those not offered help, 72% won.       pretend: only 3 ppl in study.                                 0       0       1 (won)                       help (T)       no help (C)       no help (C)                 C       T       C                 C       C       T                                                                               difference      diff: 0 (mean win rate for T)-0.5 (for C)=-0.5   0-0.5 = -0.5   1-0 = 1   2/3 prob that get a diff of -0.5   1/3 prob that I get a diff of 1   d. H_0: no impact of offer of help from HCAS on outcomes   e. Assume: SRS with one T and two C       Bernoulli randomization and p-values            Bernoulli randomization means that you flip a coin for each unit, rather than drawing a prespecified number of units out of a hat.       useful when units arrive one by one so you can’t randomize all at once.       Assuming: Bernoulli randomization (not SRS)                                 0       0       1                                       T       C       C                                 C       T       C                                 C       C       T                                 T       T       C                                 T       C       T                                 C       T       T                                 T       T       T                                 C       C       C                           none of the p-vals are small — I won’t be surprised if something happens 1/2 of the time? no. This is because it’s a tiny dataset.   differences   0-0.5=-0.5   0-0.5=-0.5   1-0=1   0-1=-1   0.5-0=0.5   0.5-0=0.5   0.33-?? = ??   ??-0.33 = ??   left side p-val: 3/6 = 1/2   right-side p-val: 5/6   one-sided: 1/2   two-sided: 1   c. p-value:  - probability of observing a value of the statistic that is at least as extreme as actually observed, if H_0 is true. - left-sided p-vaue: prob that test statistic is at least as small as actually observed, if H_0 true. - right-side p-value: prob … at least as large … - one-sided p-value: min(left, right) - two-sided p-value: 2xone-sided value - most common to report 2-sided p-value, but I should specify what p-value I’m reporting - If p-value is small, perhaps H_0 is not true. “reject H_0” - If p-value is big, no reason to doubt the null “failed to reject H_0” - A common cutoff is 0.05, but not for any reason - A p-value less then 0.01 or 0.001 is equivalent to 0 “p&lt;0.001” 4. Non-parametric test: next steps: 1. lots of assumptions 2. randomization test: take advantage of the fact that we randomized the test 3. permutation test is equivalent: apply the algo for randomization test for a situation where we did not randomize 4. benefits: useful for any sample scheme, any sample size. you can also use any test statistic. No distributional assumptions such as normality.   Q: Can we carry out the steps of a randomization test if the study was not actually randomized?   A: Yes. The steps in the test work perfectly will if the two groups were not created randomly. However, in that case we can’t justify the test by saying that each of these other randomizations could have occurred if the groups did not cause the outcome. Instead, we justify the test by saying that the group labels could have been allocated in any of these ways if the outcomes are not related to the groups.   Rank Sum Tests      Rank sum test            ex setup: suppose we compare the effectiveness of the old drug and the new drug. Suppose we measure how many months they lived after we assign the drugs. 4 ppl.       2 ppl for old drug A lived 3, 7 months each       new drug B lived 0, 12 months                    3 7 0 12           A A B B           A B A B           .. continue random allocation → make histogram of  difference of means → get p value                       what if instead of 12 months, it’s &gt;12? (person’s still alive)…what do we do?           Rank sum test details            we can’t take mean of 0 and &gt; 12. so instead, w.t. transform our data so that we can represent &gt;12 in a useful way — convert to numbers rank in dataset.       back to example                    3 7 0 &gt;12 (assigned)           2 3 1 4 (ranks) → T (add ranks of ppl in group A)           A A B B → 5           A B A B  → 3           A B B A  → 6           B B A A →  5           B A B A  → 7           B A A B → 4                       T = sum of the ranks in the smaller group. We now have reference distribution!       The randomization we actually saw (data) is the first row. → Q: are we surprised to see a rank sum of 5 if null is true? No! bc the 5 is the middle value i expect to see           Why sum of ranks?            the test statistics we used was sum of ranks instead of diff of means.       suppose the values was 3 3 0 &gt; 12. Then you average the ranks: 2.5 2.5 1 4       If I know there’s 4 nums, we know there are 4 ranks. If I know sum is 10 (1+2+3+4), and the sum of ranks in group A is 5, I know that the sum of ranks in group B is 5. So I just have to keep track of 1 group.           When to use the rank sum            when you have censored data (very common in medical data like &gt;12)       when you have outliers: when you have 120000 instead of 12: if you take average with this, this outlier will completely drive the output. This rank sum test is resistant to outliers.       when you have small dataset bc no assumptions (e.g. normal distribution). Other methods, like t-tests, estimate the reference distribution by making assumptions.           More Non-parametric Tests      Ways to approximate the reference distribution, rather than calculating it exactly            exact method                    (what we’ve been doing) listing all ways to allocate units into 2 groups. AAABB, AABAB, …           problem: too many ways to allocate units                       approximate exact:                    take a SRS of the ways to allocate the units into 2 groups. Use those allocations only to produce an approximation to the exact reference distribution           most common plan                       normal approximation:                    works sometimes, only if the mean and variance of the reference distribution is known ahead of time, and we know the reference distribution would be normal.           workers for rank sum bc “expected value” E(T)  = n_1(N+1)/2, var(T) = n_1n_2(N+1)/12           Y: -20, -11, 5,7, …, 2100, 3000           rank: 1, 2, 3, 4, 5, …, 99, 100           treat: A, A, B, A, …, A,B           rank depends on N, not data values           uniform reference distribution: we know the distribution (histogram) of the ranks without seeing the data → just flat bc there’s one of each rank                           Central Limit Theorem            If you get the rank sum distributoin from the uniform rank distribution, it looks like a normal distribution.       def: regardless of population distribution, sum of random samples will be approximately normally distributed (in most circumstances)       but then why is it helpful that converting to ranks gives us a uniform distribution?       CLT says when we draw a large sample from a data set and record the sum (or mean) of the values in the sample, if we repeatedly draw different samples, the sums (or means) will look approximately normal. However, the meaning of the word “large” depends on the distribution of the original data: the weirder the distribution, the larger sample size we need in order for the CLT to be true. So, the advantage of converting the data to ranks is that we know the uniform distribution is not too weird (no outliers, symmetric), and the CLT will work even for a small sample size.           Facts about rank sum            no distributional assumptions       outliers are not a problem       censoring not a problem       randomization/permutation distribution depends on the sample size, not the data itself       big or small sample size is fine       when to avoid rank sum:                    lots of ties                           ","categories": ["R"],
        "tags": ["qai","R","statistics"],
        "url": "/r/qai-statistical-testing-week-1/",
        "teaser": null
      },{
        "title": "[QAI] Non-param tests, CLT, param tests intro (week 2)",
        "excerpt":"  again, all notes from QAI   Non-parametric tests for pairs      sign test            example: number of students who got an A                                      student           fall           spring           diff                                           1           0           1           1                             2           3           4           1                             3           2           1           -1                             4           2           4           2                           avg: 3/4                                  student           fall           spring           diff                                           1           0           1           1                             2           4           3           -1                             3           2           1           -1                             4           4           2           -2                           avg: -3/4       These two datasets would be equally likely if we assume that fall and spring As are equally likely       Get all the avgs and create histogram - find the statistic actually observed - find how extreme it is (same with what we did for p-value)       Conducting a sign test            instead of avg, we look at if the diff is positive or not       cross out any ties                                             student           fall           spring           pos?                                           1           0           1           1                             2           3           4           1                             3           2           1           0                             4           2           4           1                           sign test: test stat is number of differences that are positive       d. so just k=3       f. Under null hypothesis, each row we have is just coin flip, 0.5 prob → so if we just know how many pairs there are in dataset, I know what the reference distribution will look like. So I expect to see a sum of coin flips of 2 (0.5+0.5+0.5+0.5). We know all the cases where we have 0 heads, 1 heads, … 4 heads. (head = pos diff). So we can calculate the p-value of k=3. The number of coin flips distribution is called the binomial distribution.       g. Binomial distribution        - symmetric,  - looks more like a normal distribution bigger the sample size and the prob closer to 0.5. But sign test the null hypothesis assumes 0.5, so as number of pairs increases, it’s essentially a normal distribution.  - back to e.g.: N(4/2,4*1/2*1/2) = N(2,1) = N(#pairs/2, #pairs/4)  - (k-mean)/var ~ N(0,1)  - known as  “back of an envelope” bc it should only appear on the back of an envelope not in a published paper. (should not typically be used in a paper because there are other tests that use more of the information that is available to you.)           Conducting a signed rank test:            randomization/permutation test for paired data, using “signed rank statistic”       To calculate statistic:                    take absolute value of difference between outcomes within each pair           rank abs diffs           S = sum of ranks for pairs where original diff was positive           in R: willcoxtest(..paired = true)           e.g.                                                          student               fall               spring               pos?               diff                               diff                               rank(               diff               )                                                               1               0               1               1               1               1               1                                                                                                         2               3               5               1               2               2               2                                                                                                         3               4               0               0               -4               4               3                                                                                                         4               2               2                                                                                                                                                                           S = 1+2=3               Why?                    Same advantages as any other non-parametric rand/perm test           same  advantages as rank sum but also same disadvantages/throw away actual data           pairs                           d.what makes S big/small?        - lots of positive differences &amp; the pos diffs are large in magnitude           Sampling Variability and Measures of Dispersion      Polls            Q: poll of 500 ppl in Oklahoma (3.8 million) to predict 2012 presidential election outcome of state. how large a poll would you need in California (38 million) to produce an equally precise prediction?           It’s like a bowl of soup            A: precision of estimate x depend on sample size, but not in the number of units in population.       suppose we have a bowl of soup. I’m going to take a spoonful of soup. I’m trying to figure out if the soup is too salty or not. What influences the guess? →                    is the soup mixed well?           size of my spoon — if too small hard, if really big it might be easier.           variabliilty of soup: if big carrot on spoon vs tomato                       does not matter how big the soup bowl is           Centers            statistics: want to summarize a data into useful information       what if we have really small dataset?                    e.g. 4 students 0 0 2 6 (# of stat courses taken)           how do we convey the pattern in this data without reciting the actual numbers?                       middle: mean, median, mode       dispersion (how spread out): min, max, range(max-min) 25%, 75%, IQR(Q3-Q1)           Mean/Median Absolute Deviation:            abs(val-median): (0-1) (0-1) (2-1) (6-1) → get median of these       you can do the same thing with mean → get mean           Variance, mean, and notation            variance: instead of taking absolute value, you square the differences  &amp; divide by #       N: population size n: sample size           Variance            middle is mean (µ). 95% of dataset is between µ-2σ and µ+2σ .           Central Limit Theorem      Distributions of Sample Means            standard normal distribution: a normal distribution with mean 0 and variance 1.       population: Y~N(65,4) ← mean = 65, variance = 4       If I randomly generate samples from population and each time I record the mean, I would expect those means to be very close to 65.  ȳ~(65,4/100)           Central Limit Theorem            as long as variance in population isn’t infinite, when we randomly sample independently from inital population, the distribution of the sample means follows a normal distribution.                                                                                 Expected Value            µ: population mean       ȳ: sample mean       ȳ = (y1+y2+…+yn)/n       E(ȳ) = E(1/n*sum(1 to n)_yi)                                                                                 Variance of sample means                                                                                                            Q: when do we need a bigger sample size for CLT to be applied?      Variability of the Data: If the data has high variability (large standard deviation), a larger sample size is needed for the sample mean to be approximately normal.   Shape of the Population Distribution: If the population distribution is heavily skewed or has heavy tails, a larger sample size is needed.   Parametric Hypothesis Tests, Part 1 (Z-tests and t-tests)      Using the CLT to generate a reference distribution            suppose $H_0$: µ = 65.       Test statistic: $\\bar{Y} = 64.5$       Reference distribution (=under null hypothesis): $\\bar{Y} \\sim N(65, \\frac{4}{100})$ if someone told us the variance and sample size       64.6 is more than two standard deviations away from mean → p-value less than 2.5%           Z-tests            still suppose $H_0$: µ = 65.       instead of $\\bar{Y}$ use $\\frac{\\bar{Y}-65}{\\sqrt{\\frac{4}{100}}} = \\frac{64.5-65}{\\sqrt{\\frac{4}{100}}} = -2.5$       reference distribution: ($\\frac{\\bar{Y}-65}{\\sqrt{\\frac{4}{100}}} $) $\\sim N(0,1)$       we don’t have to assume that the population distribution is normal becuase the reference distribution follows the CLT — it’s normal regardless of population distribution.           Standard normal distribution            but what do we do when we don’t know the population variance? → t-test           Toward the t-test: sample variance            we need to estimate the population variance using the data       $S^2 = \\frac{(y_1-\\bar{y})^2 +(y_2-\\bar{y})^2 +…+ (y_n-\\bar{y})^2}{n-1}$ is the estimate of the population variance using sample                why n-1 instead of n??? intution 1 $\\mathbb{E}(s^2) = \\sigma^2$ unbiased                                                                                                                                                                           intution 2: order to calculate the sample variance, i have to first calculate sample mean → used first piece of information       n-1 is called degrees of freedom                    intution 3: if we have two points on plot, we can estimate the slope. If we have three points, we can talk about variance too, since we only need two points to talk about slope and we have one piece of information left to talk about variance.           intuition 4: we are dividing by a smaller number than dividign by n, so the overall number is a bit bigger → since we are estimating the variance, we are inflating the uncertainty.                           One-sample t-test            still suppose $H_0$: µ = 65.       test statistic: $\\frac{\\bar{Y}-65}{\\sqrt{\\frac{S^2}{100}}} = \\frac{\\bar{Y}-\\mu}{\\sqrt{\\frac{S^2}{n}}} $ (variance not known anymore)       reference distribution: $\\frac{\\bar{Y}-65}{\\sqrt{\\frac{S^2}{100}}} \\sim t_{99}$ ← for smaller sample sizes (smaller degrees of freedom) it’s like a normal distribution with a wider (Fatter) tailer — higher chance of getting extreme value since we don’t know what variance we’ll get       we don’t rely on CLT anymoore bc S^2 depends on shape of population distribution. If pop distribution is normal → follows chi-square distribution. If not, Idk. So we have to assume population distribution is normal for t-test.                Two-sample test              using CLT. You can just add varainces together bc we’re assuming independence       null hypothesis: population mean in the first group is equal to the population mean of the second gorup                                                                                     b. $H_0: \\mu_x = \\mu_y$ → $H_0: \\mu_x - \\mu_y = 0$       c. test statistic: $\\frac{\\bar{X}-\\bar{Y}-0}{\\sqrt{\\frac{\\sigma_x^2}{n_x}+\\frac{\\sigma_y^2}{n_y}}}$       d. reference distribution: standard normal N(0,1)       Two-sample t-test, unpooled            of course, we don’t usually know the variances of the populations. So we use two-sample t-tests       use $\\frac{\\bar{X}-\\bar{Y}-0}{\\sqrt{\\frac{S_x^2}{n_x}+\\frac{S_y^2}{n_y}}}$← unpooled or welch t-statistic                    had to assume both populations are normal and independent                       reference distribution: $t_{df}$ approx. If sample size big enough it’s just standard normal.           Two-sample t-test,pooled            if we can assume that the variances are the same in the populations, then things are a bit simplier       $\\frac{\\bar{X}-\\bar{Y}-0}{\\sqrt{\\frac{S_p^2}{n_x}+\\frac{S_p^2}{n_y}}}  = \\frac{\\bar{X}-\\bar{Y}-0}{s_p\\sqrt{\\frac{1}{n_x}+\\frac{1}{n_y}}} $ ← pooled t-statistic       reference distribution: exactly follows a t distribution of $t_{n_x+n_y-2}$                    why? $s_p $ follows chi-square distribution                                                                                                               $S_p^2 = \\frac{s_x^2(n_x-1)+s_y^2(n_y-1)}{n_x+n_y-2}$                   Parametric Hypothesis Tests, Part 2 (example, paried tests, confidence intervals)      example            you check the assumptions for t-tests by looking at the visualizations (boxplot, histogram, etc.). If data is heavily skewed/ has a lot of outliers, try applying log       find mean, variance of the two samples from two populations       using that, calculate the t-statistic                    why do we pool? More complex methods such as regression and anova are generalizations of the pooled t-statistic.           we know that t-distribution is bascially a normal distribution with big enough sample size. If we know that the sample size is so small that it’s not a normal distirbution, we should not bother with a t-test and just use a randomazation and permutation test.                           Confidence intervals            we come up with the range of values that if they were the null, would not be contradicted by our data       $H_0: \\mu = 65$ ← I’m either going to reject this null hypothesis or not       let’s say we reject it. what if $\\mu = 64.3$ and it’s not contradicted by the data?       The set of null hypotheses not contradicted by the data are going to be the values close to the mean I actually observed = confidence interval                the probability that the upper and lower bounds contain true mean $\\mu$ is 0.95                                                                                                 Paired t-test                     we want to maintian the paired structure — who is paried with whom                                                                                             paired t-test is identical to one-sample test of differences       $H_0: \\mu_x=\\mu_y $ or $\\mu_d=0$ (population mean of within pair differences)                $t = \\frac{\\bar{x}-\\bar{y}}{\\frac{s_d^2}{d}}$ ← denominator: sample variance of differences divided by number of pairs d                                                                                                    null hypothesis of randomization tests (rank sum, sign test, signed rank test, etc.) → population distributions are the same   t-tests: use null hypotheses related to population means   ","categories": ["R"],
        "tags": ["qai","CLT","statistics","R","sign_test"],
        "url": "/r/qai-non-param-tests-clt-param-tests-intro-week-2/",
        "teaser": null
      },{
        "title": "[QAI] R tutorials",
        "excerpt":"  Notebook and lecture from the Quantitative Analysis Institute Summer Program.   week 0  - Basic R commands      Vectors:            similar to numpy array       ex) c(1,2,3,4,5) →  1  2  3  4  5,  c(1:3,5:12) →  1  2  3  5 10 11 12 (right inclusive)       5:10 is valid; but (5:7, 8:10) is invalid bc you need c to combine two expressions       length() to find len       NA for None       merge two vectors by c(vec1, vec2)       exp(vec1), log(vec1) ← applies to all elements in vec           Matrices:            ex) mat1 = matrix(3,nrow=2, ncol=2) #matrix with all 3s       dim(mat1), nrow(mat1), ncol(mat1)                rows and columns            ?matrix #help page \t\t  # first row, first column  mat1[1,1] \t\t  # all rows, column 1  mat1[,1] \t\t  # first row, all columns  mat1[1,] \t\t  # assign names to rows and columns  rownames(mat1) &lt;- c(\"Row1\", \"Row2\")  colnames(mat1) &lt;- c(\"Col1\", \"Col2\") \t\t  # see names of rows and columns  rownames(mat1) # \"Row1\" \"Row2\"  colnames(mat1) # \"Col1\" \"Col2\" \t\t  # view column 1  mat1[,\"Col1\"] \t\t  matrix(1:100, ncol=50)# nrow not assigned, R does it for you; 100/50 = 2 rows  # if both nrow, ncol not assigned, R makes 1 col                                matrix operations            # Element-by-Element Multiplication  mat1*mat2 \t\t  is.matrix(mat3)  is.vector(vec1)  as.matrix(vec1) # vertical matrix                           Datasets            Dataset Examination           \t  ?swiss \t  # EXAMINE THE DATASET  head(swiss) # see first few rows  tail(swiss) # see last few rows   swiss # see entire data set  dim(swiss)# number of rows and columns  ncol(swiss)   nrow(swiss)  colnames(swiss)  summary(swiss) #min/1st Quartile/mean/3rd Q/max \t \t  # R differentiates between \"matrix\" objects and \"data frame\" objects.  # When you do read.csv/read.table it'll be a dataframe  is.matrix(swiss)  is.data.frame(swiss)  swiss &lt;- as.data.frame(swiss) \t  swiss$Fertility #refer to a column (Fertility is col name)  #OR you can do this (not recommended)  attach(swiss)  Fertility  #OR you can also do:  swiss[,\"Fertility\"]  swiss[,1]           b. Dataset Summary        mean(swiss$Fertility)  var(swiss$Fertility)  summary(swiss$Agriculture) #for continuous variables  table(swiss$Education) #stem and leaf plot (for discrete variables)  table(swiss$Education, swiss$Examination) # how many times each combination appeared           c. Subsetting Dataset        FertilitySubset&lt;-swiss$Fertility[swiss$Agriculture&gt;50]  summary(swiss$Fertility[swiss$Agriculture&gt;50]) #fertitilty where agr &gt; 50  summary(swiss$Fertility[swiss$Agriculture&gt;50 &amp; swiss$Catholic&gt;50]) #AND           d. Plotting Dataset        hist(swiss$Fertility)  boxplot(swiss$Fertility)  plot(swiss$Agriculture, swiss$Fertility) \t  plot(swiss$Agriculture, swiss$Fertility,      main=\"Title Goes Here\",      xlab=\"X-axis label goes here\",      ylab=\"Y-axis label goes here\",      xlim=c(0,100),      ylim=c(0,100),      pch=21,      cex=5,      col=\"red\")                Logic        # What if we compare a vector of numbers to another number?  vec1&lt;-c(2,4,5,6)  vec1&gt;3 # R distributes the 3 for you \t  vec2&lt;-2:5  vec1==vec2 #TRUE                Conditioning        vec1[1] #1st el in vec1 (just like python list indexing)  #same as  vec1[c(TRUE, FALSE, FALSE, FALSE)]  vec1[vec1==vec2] #same logic as above                Graphics       \t  par(mfrow=c(1,2)) #mfrow = how to split window  hist(rnorm(100))  boxplot(rnorm(100))  par(las=1) # turns horizontal labels to make it easier to read \t  abline(v=10) # add a vertical line  abline(h=0, lwd=3) # add a horizontal line, and make it thick using lwd:  # add a line with intercept -3 and slope .1, change the line type via lty and the color via col  abline(-3,.1, lty=2,col=\"orange\") \t  # For plots, here's an excerpt from the RPrimer:  plot(swiss$Agriculture, swiss$Fertility,      main=\"Title Goes Here\",      xlab=\"X-axis label goes here\",      ylab=\"Y-axis label goes here\",      xlim=c(0,100),      ylim=c(0,100),      pch=1, #point character (circle dot, triangle dot, x dot, etc.)      cex=5, # default point size will be multiplied by this number      col=\"red\") \t  colors() # names of available colors \t                Packages        install.packages(\"perm\") #just once  library(perm) #import                importing a dataset        getwd()  setwd()  d=read.csv(file=\"path\",header=TRUE)           week 3   boxplots   boxplot(vec1, main = \"title\", ylim= c(0,11)) boxplot(vec1, vec2, vec3) # how to put multiple plots with same scale  groups = c('sophomore', 'junior', 'first-year', 'junior') boxplot(vec1~groups) #vec1: set of values, groups: what group each val is in. ~ means by  boxplot(iris$Sepal.length~iris$Species) # this col grouped by diff col   objects   output = t.test(vec1, vec2)  #output is object output$p.value # you can check attributes using names(output) command   by, apply, which   by(iris$Sepla.Length, iris$Species, mean) # applies a func to a vector after grouping based on another vector apply(iris[,1:4], 2, mean) # applies a function to all the rows(1) or columns(2) of matrix/dataframe which(iris$Species=='virginica') # provides the indices of a vector where a particular condition is true   if, ifelse   if (6&gt;7) { \tobject = 3 }  output = ifelse(iris$Species=='virginica', \"red\", \"blue\")   saving, writing, reading   save(iris, file=\"...R\") # save an R object load(file=\"...R\") # load a saved R object write.csv(iris, file=\"...\") # save a matrix/dataframe to a csv  pdf(file=\"....pdf\") # write graphics to pdf. or you can also do jpeg(file=\"....jpeg\") plot(1:10); plot(2:30) dev.off()    replace na values R   d[is.na(d)] &lt;- 0    drop na   library(tidyr) student %&gt;% drop_na(maths)   week 4   Splines    ### Splines  # There are lots of functions and packages available that relate to splines. # One basic one is \"spline\", which takes in a set of (x,y) pairs and returns a larger set of (x,y) pairs that fills in all the gaps. For example: spline(x=c(1,2,3),y=c(3,16,6),n=9) # the default is to return three times as many points as you provide, but you can change \"n\" to alter this # useful to save the output: splineout&lt;-spline(x=c(1,2,3),y=c(3,16,6),n=9) # because then you can access splineout$x splineout$y plot(c(1,2,3),c(3,16,6)) points(splineout$x,splineout$y,type='l',col=\"red\")     Regression Trees    ### Trees # use the package \"party\" # install the package just once install.packages(\"party\") # run the library command every time (will get warning messages, but that's okay) library(party)  # save the output as an object # specify the outcome vector, then \"~\", then the predictor vectors with \"+\" in between treeoutput1&lt;-ctree(iris$Sepal.Length~iris$Sepal.Width+iris$Petal.Length) # equivalently, if these are all columns in the same data set: treeoutput1&lt;-ctree(Sepal.Length~Sepal.Width+Petal.Length, data=iris) # default graph has way too much information: plot(treeoutput1) # can simplify the output for yourself if the final nodes are a continuous variable: plot(treeoutput1, inner_panel=node_inner(treeoutput1, pval=FALSE, id=FALSE), terminal_panel=node_boxplot(treeoutput1, id=FALSE)) # if you don't want the plots at the bottom, you can do this: plot(treeoutput1, inner_panel=node_inner(treeoutput1, pval=FALSE, id=FALSE), terminal_panel=node_terminal(treeoutput1, id=FALSE))   # if you are trying to predict a categorical variable treeoutput2&lt;-ctree(Species~Petal.Width+Petal.Length, data=iris) # can simplify the output for yourself: plot(treeoutput2, inner_panel=node_inner(treeoutput2, pval=FALSE, id=FALSE), terminal_panel=node_barplot(treeoutput2, id=FALSE)) # if you don't want the plots at the bottom, you can do this: plot(treeoutput2, inner_panel=node_inner(treeoutput2, pval=FALSE, id=FALSE), terminal_panel=node_terminal(treeoutput2, id=FALSE))  ### Random forests are a way to take into account the fact ### that there is uncertainty in your choice of tree. ### A slightly different data set might have led ### to a different tree.  # You can also run a random forest, but you need another package # install the package just once install.packages(\"randomForest\") # run the library command every time library(randomForest)  # see help file for randomForest to understand some of the options # the first argument looks like the first argument to ctree # the default number of trees to \"grow\" is 500, but you can change it with the argument ntree # there are also defaults for things like nodesize, maxnodes, etc. # The \"importance\" argument tells the function to save information about how important each variable is rFout&lt;-randomForest(Sepal.Length~Sepal.Width+Petal.Length, data=iris, \t\tntree=600, importance=TRUE) # It can be hard to know how to interpret the output of a random Forest. You don't end up with just one tree to view. One useful piece of output is the list of \"importance\" values, which indicate how crucial each predictor is for predicting the outcome. rFout$importance # both of these measures indicate how much better the predictions are when you include the variable. So, higher numbers are better.   More plots    #### Graphics:  # Suppose you want to add a legend to a plot. # Look up the help page for the legend command. # Start with a plot: plot(1:10,1:10, pch=c(rep(1,5),rep(2,5)), \t\tcol=rep(c(1,2),5)) # then add the legend: legend(\"bottomright\", # specify the location, either with a phrase like this or with coordinates \tlegend=c(\"type1\",\"type2\",\"type3\",\"type4\"), # labels here \tpch=c(1,2,1,2), # symbol types for four points to be plotted \tcol=c(1,1,2,2) # colors for four points to be plotted \t) \t# you don't have to use both pch and col. If it's only color, try the \"fill\" argument to obtain rectangles filled in with the different colors. For lines, use lty and lwd to specify different types of lines. \t # Sometimes it's helpful to add elements to a plot after you've created it. # legend is one example # You've already seen abline, which adds a line # You can also plot additional points. # Start with a plot: plot(1:10,1:10, type='o') # Then add more points with \"points\" points(2:5,3:6, col=\"red\", type=\"l\")  # The \"type\" argument works for both plot and points. # The default is \"p\", which makes points. # \"l\" connects the points to form a line # \"o\" plots the points and also connects them  # The function \"lines\" is the same as \"points\" with type=\"l\" # If there's a difference between them, I don't know what it is  # You can also add text anywhere on a plot using the command \"text\". # first, specify where you want the text by providing x and y coordinates  # then, the \"labels\" argument contains the text # You have to have a plot window open, such as the plot you just made text(x=5,y=6,labels=\"Example Text\", cex=.5, col=\"red\")  # Any of the first three arguments can be a vector text(x=c(4,8), y=c(4,4), labels=c(\"First\",\"Second\"), cex=1.5, col=\"blue\")  # If you try to add text and don't see it, make sure the x and y values are within the range shown in your plot!    ## Creating your own functions ## Ever find yourself repeating the same steps many times? You can write your own R functions, with input and output. # Here is a simple example:  # Run this code myfirstfunction&lt;-function(x) { \t \ty&lt;-x^2 \treturn(y) }  # And then use your function: myfirstfunction(3)  # For example, perhaps you want to run a clustering algorithm and make a plot like the one created above for the first two variables, and you'll want to do this for multiple data sets. myclusterfunction&lt;-function(data, numclusters) { \t \tkoutput&lt;-kmeans(data,numclusters)  \tplot(data[,1],data[,2], \tcol=koutput$cluster, \t\txlab=colnames(data)[1], \t\tylab=colnames(data)[2]) \tlegend(\"topleft\", \tlegend=c(1:numclusters), \tfill=c(1:numclusters)) \t \treturn(koutput) \t }  # now you can use this to generate the same output as above: swissoutput&lt;-myclusterfunction(data=swiss,numclusters=3) # but you can also use it for other data sets, such as the subset of iris containing numeric variables irisoutput&lt;-myclusterfunction(data=iris[,1:4],numclusters=5)     # Graphical functions to check out: # polygon (also helpful: seq, rev) plot(1:10,1:10, col=\"white\",ylim=c(-100,100)) sequence&lt;-seq(1,10,.01) polygon(x=c(sequence, rev(sequence)), \t\ty=c(-sequence^2, rev(sequence^2)), \t\tcol=\"blue\")  # grid, segments grid() segments(x0=c(1,2),y0=c(1,1), \t\tx1=c(20,20),y1=c(30,60), \t\tlwd=5)  # plot(..., axes=FALSE) then: axis, box plot(1:10,axes=FALSE) box() axis(3) axis(4) # can change labels and spacing of axes, too  # gray, rainbow, heat.colors, colors() plot(1:10,pch=16,cex=5,col=gray(seq(.1,1,.1))) plot(1:10,pch=16,cex=5,col=rainbow(10)) plot(1:10,pch=16,cex=5,col=heat.colors(10)) colors()  # layout: fancier than par(mfrow=c(...)) layout(matrix(c(1,2,1,3),nrow=2)) hist(1:10) plot(1:10) boxplot(1:10) # restore to default par(mfrow=c(1,1))  # image image(matrix(1:10,nrow=2)) # related: heatmap, contour, stripchart, colorRampPalette  # check out popular graphics packages: lattice, ggplot2      ggplot2   basics   # 2 ways to build a graph, 1) qplot(), 2) ggplot() # This tutorial is going to cover ggplot() since we have more control and can produce more complicated graphs  #brief summary of qplot() #qplot() is for making a 'quick' plot. It allows you to create a complete plot given data, geom, and aes mappings qplot(x=carat, y=price, data=diamonds, geom='point')  #ggplot begins a plot that you can add layers to.  #this creates the exact same plot as the qplot() above ggplot(data=diamonds, aes(x=carat, y=price))+geom_point()  #Basic idea of the structure of ggplot, specify parts of the plot and then add a plus sign between them.  #we add 'layers' with the plus sign  #watch how we can build up this graph by running each of the 3 lines of code one by one ggplot() ggplot(data=diamonds, aes(x=carat, y=price)) ggplot(data=diamonds, aes(x=carat, y=price))+geom_point()    aes    #-------------------------------aes function ------------------------------- #aes stands for aesthetics  #The aes function is for generating aesthetic mappings that describe how variables in the data set are mapped to aesthetics of geoms #basically it is a function that is used to detail what is displayed and how it is displayed. you don't need to use the $ to specifiy the variables anymore  #you should see a blank graph if you run the code below. This is because you only specified how the variables are going to be mapped on the x and y axis.  ggplot(diamonds, aes(x= carat, y=price))  #-------------------------------- Geoms------------------------------------------------------------------------------------- #geoms are the names for the types of shapes that represent the data on the chart  #Two types of geoms #1) geom_point &lt; works on individual data points  #2) collective geom that requires a collection of data, like a box plot or a histogram  #can think of geom_histogram as adding a layer ggplot()+geom_point()  #scatter plot ggplot(diamonds, aes(x= carat, y=price))+geom_point() #line ggplot(diamonds, aes(x= carat, y=price))+geom_abline() #histogram ggplot(diamonds, aes(x= carat))+geom_histogram(binwidth = 0.1, bins =15) #box plot ggplot(diamonds, aes(x= carat, y=price, group=cut))+geom_boxplot(outlier.color=\"red\")  #please go on to this wonderful site and look at all the geoms,  # there are around 30 geoms so it would talk to long to talk about each # http://docs.ggplot2.org/current/ (this link doesn't work anymore, use the one below) https://ggplot2.tidyverse.org/reference/  #----------------------------------- more on the aes function---------------------------------------------------------------- # so far we have only used the aes to specify the variables that we want to map onto the x and y axes # we can use the aes for more than that!   #aes shape #this means we want to map the diamond's cut onto the shape of the points ggplot(diamonds, aes(x= carat, y=price, shape=cut))+geom_point()  #aes size ggplot(diamonds, aes(x= carat, y=price, size=color))+geom_point(color=\"black\")  #aes color ggplot(diamonds, aes(x= carat, y=price, color=color))+geom_point()  #editing aes with the iris data set  ggplot(iris, aes(x= Sepal.Length, y=Petal.Length, color=Species))+geom_point(cex=2) ggplot(iris, aes(x= Sepal.Length, y=Petal.Length, size=Species))+geom_point(color=\"dark blue\") ggplot(iris, aes(x= Sepal.Length, y=Petal.Length, shape=Species))+geom_point(color=\"dark blue\", cex=4) ggplot(iris, aes(x= Sepal.Length, y=Petal.Length, alpha=Species))+geom_point(color=\"dark blue\", cex=4)  #aes with continuous variable #the color gives us a gradient color bar ggplot(diamonds, aes(x= carat, y=price, color=depth))+geom_point()  #we can also map a variable onto the type of line. This is a kinda terrible example below but you get the point that we can use different types of lines to represent another variable.  ggplot(diamonds, aes(x= carat, y=price, linetype=cut))+geom_line()  #aes and histograms #even though there is only one variable here we can visually include another  by coloring the bars by the cut #notice the difference between the fill and the color ggplot(diamonds, aes(x = carat, fill = cut))+geom_histogram(binwidth=.1, bins=15) ggplot(diamonds, aes(x = carat,color=cut))+geom_histogram(binwidth=.1, bins=15)  #remember the aes is for specifying how we want to  map variables onto the plot.  #See what happens when we try to assign color in the aes function to a color ggplot(diamonds, aes(x=carat, y=price, color=\"light blue\"))+geom_point() #Assigning the color in the aes function to an actual color creates a factor by color called light blue which doesn't exist in the diamonds dataset. We assign color, shape, or size in the aes fuction to a variable to specify how we want to map that variable onto the plot.   #if you want the points to be blue you need to specify the color in the geom as a argument in the geom ggplot(diamonds, aes(x=carat, y=price ))+geom_point(color=\"light blue\")  #can make things more see through adding alpha ggplot(diamonds, aes(x=carat, y=price ))+geom_point(color=\"light blue\", alpha=0.5)    layers     #--------------------------------Idea of layers---------------------------------------------------------------- #each layer (thing we add on with +) can use its own data frame, variables and aesthetics #instead of specifying the aes mapping in the ggplot() function we can specify the aes mapping within the geom,  ggplot(diamonds, aes(x=carat, y=price, color=depth))+geom_point() ggplot(diamonds, aes(x=carat, y=price ))+geom_point(aes(color=depth))  #All of these 3 lines of code make the same plot ggplot(diamonds, aes(x=carat, y=price ))+geom_point(aes(color=cut)) ggplot(diamonds, aes(x=carat, y=price, color=cut ))+geom_point() ggplot()+geom_point(data=diamonds, aes( x=carat, y=price, color=cut))  #the '+' operator just allows the code to be broken up into spearate lines  p&lt;-ggplot(diamonds, aes(x=carat, y=price )) p+geom_point(aes(color=cut))  #the latest layer's specifications override the ones below #for example if I specify that I want to map cut onto the color of the points on the graph but then I say I want the points to be blue in the geom function, the points will be blue ggplot(diamonds, aes(x=carat, y=price, color=cut ))+geom_point(color=\"light blue\")     scales   #basic plot #start with this plot ggplot(diamonds, aes(x=carat, y=price ))+geom_point(aes(color=cut))  #changing the colors #then we can change the colors of the points using scale_color_manual. Give it a list of colors  ggplot(diamonds, aes(x=carat, y=price ))+geom_point(aes(color=cut))+ scale_color_manual(values=c(\"light pink\", \"light yellow\", \"light blue\", \"thistle1\", \"orange\"))  #changing the shape #If we specified the shape instead of the color to represent the cut of a diamond we can manually change the shapes ggplot(diamonds, aes(x=carat, y=price ))+geom_point(aes(shape=cut))+scale_shape_manual(values=c(16,17,18,17,15))  #editing the x axis #we can specify the limits, labels, and ticks of the x axis  ggplot(diamonds, aes(x=carat, y=price ))+geom_point(aes(color=cut))+scale_x_continuous(\"Diamond Carat\", limits=c(0,6))  #for the y axis ggplot(diamonds, aes(x=carat, y=price ))+geom_point(aes(color=cut))+scale_y_continuous(\"Diamond Price\", limits=c(0,20000))  #labels ggplot(diamonds, aes(x=carat, y=price ))+geom_point(aes(color=cut))+labs(x=\"Carat\", y=\"Price\", title=\"Diamonds\")  #quickly add title with ggtitle() ggplot(diamonds, aes(x=carat, y=price ))+geom_point(aes(color=cut))+ggtitle(\"Diamonds\")  #we can more conveniently change x  and y axis limits using xlim and y lim.  ggplot(diamonds, aes(x=carat, y=price ))+geom_point(aes(color=cut))+ xlim(c(0,10))+ylim(c(0,20000))   #all of these functions have a help page! please look ?scale_color_manual ?scale_x_continuous #also look at the ggplot2 cheatsheet    faceting    #-------------------------------------------------Faceting--------------------------------------------- #grouping data into different plots, i'm going to use the iris data set for this because it makes more sense in this case #Faceting displays subsets of data in different panels #When there are 2 discrete variables a grid can be laid out by specificying the variable on either side of the ~  #facets, vertical strips, showing carat vs price for each type of diamond cut ggplot(diamonds, aes(carat, price))+geom_point() + facet_grid(.~cut) #facets, horizontal strips, showing carat vs price for each type of diamond cut ggplot(diamonds, aes(carat, price))+geom_point() + facet_grid(cut~.)  #facets of the type of cut and color of the diamond ggplot(diamonds, aes(carat, price))+geom_point() + facet_grid(cut~color)    stats    #--------------------------------------------------Stats---------------------------------------------- #stats apply statistical transformations that summarize the data. It is an alternate way to build a layer like a geom #stat functions and geom functions both combine a stat with a geom to make a layer #these two are the same  ggplot(diamonds, aes(carat))+stat_bin(geom=\"bar\") ggplot(diamonds, aes(carat))+geom_bar(stat=\"bin\")   #---------Stat Smooth (important subsection of stats)------------- # stat smooth is for creating a fitted line.  #look at help page see that \"geom_smooth and stat_smooth are effectively aliases: they both use the same arguments\" #Use geom_smooth unless you want to display the results with a non-standard geom. ?geom_smooth ?stat_smooth #notice stat smooth is the same as geom_smooth ggplot(diamonds, aes(carat, price))+stat_smooth() ggplot(diamonds, aes(carat, price))+geom_smooth()  #What they mean by \"unless you want to display the results with a non-standard geom\" is the line of code below #if for some reason we wanted our fitted line to be dots or bars instead of a line we can specify the geom in the stat smooth function. We cannot do this in the geom_smooth function, this would just make points ontop of the smoothed line ggplot(diamonds, aes(carat, price))+geom_smooth()+geom_point()  #\"non standard geom with stat smooth\" ggplot(diamonds, aes(carat, price))+stat_smooth(geom=\"point\") ggplot(diamonds, aes(carat, price))+stat_smooth(geom=\"bar\")  #The default method is the loess regression (locally weighted scatterplot smoothing) but we can change that to lm. glm, gam, rlm. You should be familiar with the lm regression which is the least square regression ggplot(diamonds, aes(carat, price))+geom_point() + stat_smooth(method=\"lm\", level=0.95)+scale_y_continuous(name=\"price\", limits=c(0,20000) )  #-----------------  #stat_unique remove duplicates ggplot(diamonds, aes(carat, price))+geom_point(color=\"grey\") +stat_unique()  #if we have split it up factors by mapping the diamond cut to color then adding a stat_smooth line will add one for each factor ggplot(diamonds, aes(carat, price, color=cut))+geom_point() + stat_smooth(method=\"lm\")  #another cool stat function is stat_summary ?stat_summary #the blue dots are all the data point's for each diamond and the black dots are the mean at each x value.  ggplot(diamonds, aes(carat, price))+geom_point(color=\"blue\") +stat_summary(fun.y=mean, geom=\"point\")  #we can specify the geom to be bars and change the function as well  #this graph is a terrible idea but a good example of what you can do ggplot(diamonds, aes(carat, price))+geom_point(color=\"blue\") +stat_summary(fun.y=max, geom=\"bar\")  #I can also write your own function and set the fun.y to be my function # You can see that my function equal 5000 for any x. myFunction&lt;-function(x){ (x/x)+5000 } #if we set the function for the y axis to be myFunction we should get a straight red line at y=5000 made up of points ggplot(diamonds, aes(carat, price))+geom_point(color=\"blue\") +stat_summary(fun.y=myFunction, geom=\"point\", color=\"red\" )  #--------------------------------------------------Annotation------------------------------------ #can add text to the plot using the annotate function ggplot(diamonds, aes(carat, price))+geom_point()+annotate(\"text\", x=3, y=5000, label = \"words on the graph\")  #can add math notation to the plot using plot math notation, make sure you say parse=TRUE ?plotmath ggplot(diamonds, aes(carat, price))+geom_point(color=\"grey\")+annotate(\"text\", x=3, y=5000, label = \"sum(hat(x),i==1, n)\", parse=TRUE, cex=10)  #add rectangle ggplot(diamonds, aes(carat, price))+geom_point()+annotate(\"rect\", xmin=0, xmax=1.5, ymin=100, ymax=7000, fill = \"pink\", alpha = 0.5)  #here is the code from the intro video where I showed you how the order of the layers matters using the pink rectangle.   ggplot(diamonds, aes(carat, price))+geom_point()+annotate(\"rect\", xmin=0, xmax=1.5, ymin=100, ymax=7000, fill = \"pink\", alpha = 0.5) ggplot(diamonds, aes(carat, price))+annotate(\"rect\", xmin=0, xmax=1.5, ymin=100, ymax=7000, fill = \"pink\", alpha = 0.5)+geom_point()  #add line ggplot(diamonds, aes(carat, price))+geom_point()+annotate(\"segment\", x=2, xend=2, y=0, yend=20000, color=\"blue\")     themes    #------------------------------------------------Themes-------------------------------------------------------------------- #Themes allow you to control the non data elements of a ggplot object #1)there are theme elements, which refer to individual attributes of a graphic that are independent of the data like font size, axis ticks, grid lines, and background of a legend #use theme() to modify different elements  #start with basic plot ggplot(data=diamonds, aes(x=carat, y=price, color=cut))+geom_point()  #we can modify, background, legend, and the grid ggplot(data=diamonds, aes(x=carat, y=price, color=cut))+geom_point()+ggtitle(\"diamonds\")+theme( \taxis.text= element_text(size=14, family=\"Helvetica\", color=\"red\"), #changing axis text font and size \tlegend.text=element_text(color=\"white\", family=\"Helvetica\"), \tlegend.key= element_rect(fill=\"black\"), \tlegend.background=element_rect(fill=\"black\"),  \tlegend.position = c(0.9,0.3), \tpanel.grid.major=element_line(color=\"white\"),  \tpanel.background=element_rect(fill=\"black\"), \taxis.ticks=element_line(color=\"red\") )    #theme functions define the settings of a collection of theme elements for creating specific style graphics #minimal ggplot(diamonds, aes(carat, price))+geom_point(color=\"grey52\")+theme_minimal() #I like to use the minimal theme and make the points blue to look like the plotly scatter plots before I use ggplotly which I will talk about below ggplot(diamonds, aes(carat, price))+geom_point(color=\"dodgerblue3\")+theme_minimal()  #theme_classic, White background no gridlines ggplot(diamonds, aes(carat, price))+geom_point(color=\"grey52\")+theme_classic()  #theme_grey, grey background default ggplot(diamonds, aes(carat, price))+geom_point(color=\"grey52\")+theme_grey()  #there is an additional package called ggthemes install.packages(\"ggthemes\") library(ggthemes) ggplot(diamonds, aes(carat, price))+geom_point(color=\"grey52\")+theme_solarized() ggplot(diamonds, aes(carat, price))+geom_point(color=\"grey52\")+theme_gdocs() #meant to look like google docs ggplot(diamonds, aes(carat, price))+geom_point(color=\"grey52\")+theme_stata() #meant to look like stata    saving plots    #----------------------------------------------saving plots and data sets with plots-------------------------------------- #we can save plots and the data sets that created it  #say I just finished creating this messed up plot and I wanted to save it  p&lt;-ggplot(diamonds, aes(x=carat, y=price ))+geom_point(aes(color=color))+ scale_color_manual(values=c(\"light pink\", \"light yellow\", \"light blue\", \"thistle1\", \"orange\", \"olivedrab1\", \"salmon1\"))+scale_shape_manual(values=c(16,17,18,17,15,1,1,))+scale_x_continuous(name=\"diamond carat\", limits=c(0,3.75) )+scale_y_continuous(name=\"price in dollars\", limits=c(0,20000) )+labs(x=\"diamond carat\", y=\"price in dollars\", title=\"The relationship between a diamond's carat and price\")+facet_grid(.~cut)+stat_smooth(method=\"lm\", color=\"grey52\")+annotate(\"text\", x=3, y=5000, label = \"diamonds\", cex=2)+annotate(\"rect\", xmin=0, xmax=1.5, ymin=100, ymax=7000, fill = \"pink\", alpha = 0.5)+theme(panel.background = element_rect(fill = \"aliceblue\"))+geom_smooth(aes(fill=cut))  #we can save the data for your plot into your working directory.  save(p, file=\"Diamonds.RData\")  #we can save the image using ggsave ggsave(\"Diamonds.png\")  #now we can load the data set we used to create the image load(\"Diamonds.RData\")   interactive    #------------------------------------Making the plot interactive with ggplotly------------------------------------------------ #ggplotly works by serializing ggplot2 figures into plotly's universal graph JSON which is built using JavaScript #Basically we use the ggplotly function to open ggplot2 figures in the web browser as an HTML widget that is created using javascript  #First create your plot using ggplot() p&lt;-ggplot()+geom_point(data=diamonds, aes( x=carat, y=price, color=cut))  #then put your plot into the ggplotly function and you will get an interactive graph ggplotly(p)    week 5   Regression    ### Regression in R  # The command for a linear regression is \"lm\". # To run a regression, give a name to the output of lm, # and write (outcome vector) ~ (predictor vector) lm1&lt;-lm(iris$Sepal.Length~iris$Sepal.Width) # To view some of the output: summary(lm1) # This gives you the coefficient estimates and the t-statistics and p-values for tests of the null hypotheses that each coefficient is equal to zero. # At the bottom, there is also an F-statistic and p-value for a test comparing the specified regression model to the equal means model. The null hypothesis is that the equal means model is sufficient.  # To access particular components of the output: lm1$coef # coefficient estimates lm1$resid # vector of residuals lm1$fitted.values # vector of fitted values (predicted means) # To view confidence intervals for the parameter estimates: confint(lm1)  # If the vectors come from the same data frame, it's usually simpler to run the regression this way: lm1&lt;-lm(Sepal.Length~Sepal.Width, data=iris)  # If you plot the data: plot(iris$Sepal.Width, iris$Sepal.Length) # then you can use \"abline\" to add the fitted regression line: abline(lm1$coef)  # Check regression assumptions by plotting the residuals against the fitted values. We won't have time to go into the details here, but if you see a pattern, the assumptions could be false. plot(lm1$fitted.values, lm1$resid)  # R also has some default plots for regression. I suggest creating your own, though, as these tend to include information you don't need and/or haven't yet studied. plot(lm1)  # To add multiple predictors, put plus signs between the predictors: lm2&lt;-lm(Sepal.Length~Sepal.Width+Petal.Width, data=iris) summary(lm2)  # To add an interaction, use \":\" lm3&lt;-lm(Sepal.Length~Sepal.Width+Petal.Width + Sepal.Width:Petal.Width,data=iris) summary(lm3) #(we didn't talk about interactions between continuous covariates in this pilot course, but, mathematically, it's just a third term added to the model for the means with a coefficient multiplied by the product of two predictors. There intuition is that the relationship between one predictor and the outcome depends on the other predictor, if there is an interaction.)  # To tell R that you want both predictors and an interaction, you can use \"*\" as a shortcut for the previous line of code: lm3&lt;-lm(Sepal.Length~Sepal.Width*Petal.Width, data=iris)     shiny   ## ####################### ## ## Intro to Shiny Tutorial ## ## ####################### ## #Shiny lets you create a manipulable window  #Shiny app is maintained by computer running R  #install and open shiny install.packages(\"shiny\") library(shiny)  #### Example Shiny App ####  ## Open birthweight dataset on risk factors associated with low infant birth weight  install.packages(\"MASS\") library(\"MASS\")  #explore and clean up variable names ?birthwt d &lt;- birthwt head(d) dim(d) names(d) colnames(d) &lt;-c(\"Birthweight Less Than 2.5kg\", \"Mother's Age\", \"Mother's Weight in lbs at Last Menstrual Period\", \"Mother's Race\", \"Mother Smokes\", \"Number of Previous Premature Labours\", \"History of Hypertension\", \"Uterine Irritability\",\"Number of Physician Visits During 1st Trimester\", \"Birthweight in Grams\")  #create subsets using median mother's age table(d$\"Mother's Age\") median(d$\"Mother's Age\") ## Median age is 23  d$\"Mother over 23\" &lt;- 0 d$\"Mother over 23\"[d$\"Mother's Age\" &gt; 23] &lt;- 1  d$\"Mother under 23\" &lt;- 1                            #the indicator for mothers&gt;23 yo tells you mothers who are older AND younger than 23  #                                                   so you don't need an indicator for mothers&lt;23 yo but this was created for ease to create  #                                                   the label in the dropdown menu in the app below- it's completely possible to create this app #                                                   without multiple indicators! d$\"Mother under 23\"[d$\"Mother's Age\" &gt; 23] &lt;- 0 d$\"No subset\" &lt;- 1  #run code for example shiny app (lines 42 - 87) you will not understand this right now- this is just to show you some possibilities! You can return to this after you complete the module to better understand it  ## with subgroups   ui &lt;- fluidPage(   headerPanel('Birthweight Data Set t-Test'),   sidebarPanel(     selectInput('subset', '1. Pick a Subset', choice=names(d[,c(11:13)]), selected = colnames(d)[11]),     selectInput('valcat', '2. Pick a Categorical Variable (0=No, 1=Yes)', choices=names(d[,c(5,7,8)]),  selected =colnames(d)[5] ),     selectInput('valcont', '3. Pick a Continuous Variable', choice=names(d[,c(3,10)]), selected =colnames(d)[3]), \tradioButtons(\"col1\", \"4. Select Color for Histogram\", choices=c(\"Blue\", \"Red\", \"Purple\",\"Green\"), selected = \"Green\"), \tradioButtons(\"col2\", \"5. Select Color for Boxplot 1\", choices=c(\"Blue\", \"Red\", \"Purple\", \"Green\"), selected = \"Blue\"), \tradioButtons(\"col3\", \"6. Select Color for Boxplot 2\", choices=c(\"Blue\", \"Red\", \"Purple\",\"Green\"), selected = \"Red\"), \ttableOutput('table')   ),   mainPanel(   \tplotOutput('hist'),     plotOutput('boxplot'),     h5(textOutput('txt')),     htmlOutput('text'),   ) )  server &lt;- function(input, output) { \toutput$hist &lt;- renderPlot({ \t\tdata1 &lt;- subset(d, d[,input$subset] &gt; 0 , select=c(\"Mother's Weight in lbs at Last Menstrual Period\", \"Mother Smokes\", \"History of Hypertension\", \"Uterine Irritability\",\"Birthweight in Grams\",\"Mother over 23\", \"Mother under 23\",\"No subset\")) \t\thist(data1[,input$valcont], main= paste(\"Histogram of\", input$valcont), xlab=input$valcont, col=input$col1) \t\t})   \toutput$boxplot &lt;- renderPlot({   \t\tdata1 &lt;- subset(d, d[,input$subset] &gt; 0 , select=c(\"Mother's Weight in lbs at Last Menstrual Period\", \"Mother Smokes\", \"History of Hypertension\", \"Uterine Irritability\",\"Birthweight in Grams\",\"Mother over 23\", \"Mother under 23\",\"No subset\"))   \t\tboxplot(data1[,input$valcont]~data1[,input$valcat], main = paste(\"Boxplots of\", input$valcont, \"by\", input$valcat), ylab = input$valcont, xlab=input$valcat, col=c(input$col2, input$col3))   \t\t})   \toutput$txt &lt;- renderText({   \t\tpaste(\"Results of t-test of\", input$valcont, 'by', input$valcat)   \t})     output$text &lt;- renderUI({     \tdata1 &lt;- subset(d, d[,input$subset] &gt; 0 , select=c(\"Mother's Weight in lbs at Last Menstrual Period\", \"Mother Smokes\", \"History of Hypertension\", \"Uterine Irritability\",\"Birthweight in Grams\",\"Mother over 23\", \"Mother under 23\",\"No subset\"))     \tt &lt;- t.test(data1[,input$valcont]~data1[,input$valcat])     \tstr1 &lt;- paste(\"t-Statistic:\", round(t$\"statistic\", digits = 3))     \tstr2 &lt;- paste(\"Degrees of Freedom:\",round(t$\"parameter\", digits =0))     \tstr3 &lt;- paste(\"p-value:\", round(t$\"p.value\", digits = 3))     \tstr4 &lt;- paste(\"Lower bound of 95% Confidence Interval:\", round(t$\"conf.int\"[1], digits = 3))   \t     str5 &lt;- paste(\"Upper bound of 95% Confidence Interval:\", round(t$\"conf.int\"[2], digits = 3))     str6 &lt;- paste(\"Mean\", input$valcont, \" in Group 0:\", round(t$\"estimate\"[1], digits = 3))     str7 &lt;- paste(\"Mean\", input$valcont, \" in Group 1:\", round(t$\"estimate\"[2], digits = 3))     HTML(paste(str1, str2, str3, str4, str5, str6, str7, sep = '&lt;br/&gt;'))     })     output$table &lt;- renderTable({subset(d, d[,input$subset] &gt; 0 , select=c(input$valcat,input$valcont))}) } shinyApp(ui = ui, server = server)  #hit stop to stop running app (or esc)    #_______________________________________________________________________________________________  ##### Basic Shiny Template ##### ui&lt;-fluidPage() server&lt;-function(input, output){} shinyApp(ui=ui, server=server) #hit stop to stop running app (or esc)  ##### User Interface (UI) ##### #Functions for laying out the user interface for your application  #Add elements to your app as arguments in the UI fluidPage() ui&lt;-fluidPage(\"STAT260\") server&lt;-function(input, output){} shinyApp(ui=ui, server=server) #hit stop to stop running app (or esc)  #You can move elements of your app to be displayed in different panels  ui&lt;-fluidPage( \theaderPanel(\"WELCOME\"), \tsidebarPanel(\"TO\"), \tmainPanel(\"STAT260\")) server&lt;-function(input, output){} shinyApp(ui=ui, server=server) #hit stop to stop running app (or esc)  ## INPUTS &amp; OUTPUTS ## #Build your app around INPUTS and OUTPUTS that you add to your UI fluidPage() #Input: Things that user toggles, used to provide values to app #outputs: R objects that user sees, things that respond when user changes input  #add inputs and outputs to your app as arguments to fluid page  #_______________________________________________________________________________________________ #INPUTS  #SYNTAX OF INPUT FUNCTIONS selectInput(inputId=\"input\", label=\"Label for input\", ...)  #inputID : input name to be used later in the server function *You can name this anything!  #label: label that is displayed on the page for the input #...: input specific arguments  #EXAMPLES OF INPUT TYPES # sliderInput creats a slider (needs a starting, min and max value for the slider) ui&lt;-fluidPage( sliderInput(inputId=\"num\", label=\"Choose a number\", value=25, min=1, max=100)  ) server&lt;-function(input, output){} shinyApp(ui=ui, server=server) #hit stop to stop running app (or esc)  # selectInput creates drop down menu (needs a list of choices and can have a selected default choice) ui&lt;-fluidPage( selectInput(inputId=\"var\", label=\"Choose a variable:\", choices=c(\"Option 1\", \"Option 2\", \"Option 3\"), selected = \"Option 1\"))  server&lt;-function(input, output){} shinyApp(ui=ui, server=server) #hit stop to stop running app (or esc)  #radioButton creates a multiple choice list (needs a list of choices and a selected default choice) ui&lt;-fluidPage( radioButtons(inputId=\"var\", label=\"Choose a variable:\", choices=c(\"Option 1\", \"Option 2\", \"Option 3\"), selected = \"Option 1\") ) server&lt;-function(input, output){} shinyApp(ui=ui, server=server)  #_______________________________________________________________________________________________ #OUTPUTS  #SYNTAX OF OUTPUT FUNCTIONS selectOutput(outputId=\"output\", label=\"Label for input\", ...) #outputID : output name to be used later in the server function *You can name this anything!   #EXAMPLES OF OUTPUT TYPES  #plotOutput() #adds plot #tableOutput() #adds interactive table #htmlOutput() #adds raw HTML #imageOutput() #adds image #textOutput() #adds text block #VerbatimTextOutput() #adds text block of raw R output   #example of plot output function with selectInput ui&lt;-fluidPage(     selectInput('valcont', 'Pick a Continuous Variable', choice=names(d[,c(2,3)]), selected =colnames(d)[2]),     plotOutput(outputId=\"hist\") ) server&lt;-function(input, output){} shinyApp(ui=ui, server=server)  ui &lt;- fluidPage(   #header   headerPanel('Birthweight Data Set Visualizations'),   #drop down choices in sidebar   sidebarPanel(     selectInput('valcat', 'Pick a Categorical Variable (0=No, 1=Yes)', choices=names(d[,c(1,5,7)])),     selectInput('valcont', 'Pick a Continuous Variable', choice=names(d[,c(2,3,6)])),   ),   #make room for histogram and boxplot   mainPanel(     plotOutput('hist'),     plotOutput('boxplot')   ) )  #__________________________________________________________________________ ##### Server Function ##### #We write the instructions on how to build the app inside the server function by creating output objects  #basic layout of server function  server&lt;-function(input, output){ \t\t#output objects  \t}  ### 3 STEPS TO BUILIDING OUTPUT OBJECTS ##   #Step 1: Save output object as output$outputID (which you defined in the ui) #don't run #Lets take the ui from the earlier example ui&lt;-fluidPage(   selectInput('valcont', 'Pick a Continuous Variable', choice=names(d[,c(2,3)])),   plotOutput(outputId=\"hist\") ) server&lt;-function(input, output){ \toutput$hist  # NEW OUTPUT OBJECT } shinyApp(ui=ui, server=server)  #Step 2: Build objects to display with a render function #don't run #Example render functions: #renderPlot() #creates reactive plot #renderTable() #creates reactive table  #renderText() #creates reactive text block #renderUI() #creates reactive HTML #renderImage() #creates reactive image #renderPrint() #prints output - often paired w verbatimtextOutput()  #Lets see what this looks like for our histogram example  ui&lt;-fluidPage(   selectInput('valcont', 'Pick a Continuous Variable', choice=names(d[,c(2,3)])),   plotOutput(outputId=\"hist\") ) server&lt;-function(input, output){   output$hist &lt;- renderPlot({ \t  hist(data) \t\t}) } shinyApp(ui=ui, server=server)  #Step 3: Use input values with input$inputID (which you defined in the ui)  ui&lt;-fluidPage(   selectInput('valcont', 'Pick a Continuous Variable', choice=names(d[,c(2,3)])),   plotOutput(outputId=\"hist\") ) server&lt;-function(input, output){   output$hist &lt;- renderPlot({ \t  hist(d[,input$valcont]) #CALLING INPUT DATA \t\t}) } shinyApp(ui=ui, server=server)  #within the hist() function, you can include all of the parameters that you would usually use to modify a histogram  ui&lt;-fluidPage(   selectInput('valcont', 'Pick a Continuous Variable', choice=names(d[,c(2,3)])),   plotOutput(outputId=\"hist\") ) server&lt;-function(input, output){   output$hist &lt;- renderPlot({ \t\thist(d[,input$valcont], main= paste(\"Histogram of\", input$valcont), xlab=input$valcont, col=\"Blue\") #Note: you can use paste() to combine consistent text with the name of reactive inputs so that the text will change as the user changes the inputs  \t\t}) } shinyApp(ui=ui, server=server)  #Challenge q - try writing the server on your own before looking at the answer below           ","categories": ["R"],
        "tags": ["QAI","R"],
        "url": "/r/qai-r-tutorials/",
        "teaser": null
      },{
        "title": "[QAI] Power, Data Cleaning and Management (Dplyr, tidyR) (week 3)",
        "excerpt":"  all notes and code from the QAI Program   Robustness and Power      Robustness &amp; Distribution of p-values if null is true            def: method works out even if the assumptions aren’t true       If the null hypothesis of a test is true, what’s the probability that the p-value will be less than 0.05? → 0.05 duh           p-values have a uniform distribution when the null hypothesis is true            better explanation: for a uniform distribution where the values fall between 0 and 1 - which is the case for the distribution of p-values when the null hypothesis is true - the probability of obtaining a number less than (or equal to) x is equal to x.           Error types and independence            Type 1 error: rejecting the null hypothesis when the null hypothesis is true       Type 2 error: Failing to reject the null hypothesis when the null hypothesis is false       Power: probability of rejecting the null hypothesis when a particular alternative hypothesis is true                    What happens to t-tests when assumptions are false?                             independece                            subgroups of units similar to each other (cluster effect)               units vary over time (serial effect)               units vary across space (spatial effect)               standard error calculations incorrect; more advance calculation needed → need diff method               check: think carefully about hw data was collected; sometimes graphics can help               so t-tests are not robust to the independence assumption                                   normality                            similarity of shapes               sample sizes (relative and absolute)               outliers                                   equality of variance                            sample sizes (relative and absolute)     2. a procedure is robust to an assumption if it is valid when the assumption is not met     3. a procedure is resistant if it does not change when a small part of the data changes (e.g. outliers)                                                        Normality                                                                                    validity: robustness to normality assumption   t-tests fairly robust to departures from normality, especially in large samples   when the sample size are not equally, t-test more sensitive to skewedness and long-tailedness   for small samples, t-tests somewhat sensitive to markedly different skewedness in two groups   check: graphics (don’t need tests for normality!) → Q-Q plot straight?   when assumption violated: t-test usually still valid; or, use non-parametric test; or transform            Equal population variances                                                                                 when sample sizes are equal, the pooled t-test is fairly robust to unequal variances   when sample sizes are unequal, the pooled t-test is typically not valid for unequal variances, the unpooled “welch” t-test  is a robust alternative   check: graphics (or, test such as Levene, but then have to make decision based on p-value cutoff) → boxplots   when assumption violated: pooled t-test, usually valid if sample sizes equal; or, unpooled t-test; or, transform            t-tests sensitive to  outliers       rank-sum test resistant alternative       source of outliers:                    measurement error           wrong population           heavy tail                            Outliers                                                                                                                                                                                            small sample size                                                                      small sample size → use permulation test                                                                         permutation test just cares if the distributions look the same   Non-parametric tests            Why does the rank sum test lead to p-values less than 5% more than 5% of the time when the populations have equal means but different variances? → null hypothesis is false. The null hypothesis of a t-test is that the means are equal, but the null hypothesis of a non-parametric test like the rank sum is that the distributions are the same, not just the means.           Power            probability of rejecting the null hypothesis when a specific alternative hypothesis is true       in general: power is tied to a specific alternative hypothesis and thus  you need to specify an alternative hypothesis in order to calculate power → The power is the probability that your test will correctly notice when the null is not true. You are much more likely to notice if the truth is very far from the null than if the truth is close to the null. For example, in a two-sample t-test, the null hypothesis is that the difference in population means is zero. Assume for a moment that the two populations both have variance 1, and consider two samples of size 100. If the true difference is not zero but 400 billion, you will certainly get a tiny p-value and correctly reject the null, right? But if the true difference is .0000001, you will definitely not end up with a data set that allows you to rule out the possibility that the true difference is zero.       tests are more powerful when sample sizes are larger       non-parametric test are somewhat less powerful than t-tests, when normality assumptions met bc knowing normality of distribution gives you more information           Log transformations for t-tests      Why use log transformations &amp; Interpreting a diff in mean logs on the original scale            we want to use our data to look more normal to better support the methods we use       Any time data consists of times / distance/ money / positive values, it’s likely that the histogram of the values is skewed to the right.       Take log! Will look approximately normal now                                                                                 how do I interpret original value from log?            $D = mean(log(x)) - mean(log(y)) \\sim median(log(x)) - median(log(y)) = log(median(x)) - log(median(y)) = log(\\frac{med(x)}{med(y)})$       $e^{log(\\frac{med(x)}{med(y)}} = e^D$, so $\\frac{med(x)}{med(y)} = e^D$       this is how we got from log scale D back to original scale medians           Interpreting a confidence interval for the difference in mean logs, on the original scale            $P(LB &lt; mean(log(x)) - mean(log(y)) &lt; UB) = 0.95$       $P(e^{LB} &lt; \\frac{med(x)}{med(y)} &lt; e^{UB}) = 0.95$           Data Cleaning and Management   getwd() setwd(\"Documents/\")  install.packages(\"foreign\") library(foreign)   summary(brfss$BLDSUGAR) table(is.na(brfss$BLDSUGAR))   # how to add new empty column brfss$timesperyr=rep(NA, nrow(brfss))  # Set don't know and refused to NA brfss$timesperyr[brfss$BLDSUGAR==777]=NA brfss$timesperyr[brfss$BLDSUGAR==999]=NA  brfss$DKorRefused&lt;-ifelse(brfss$BLDSUGAR==777 | brfss$BLDSUGAR==999, 1, 0)  # If responded \"never\", set new variable to zero brfss$timesperyr[!is.na(brfss$BLDSUGAR) &amp; brfss$BLDSUGAR==888]=0   # If number between 401 and 499, already in terms of times per year, but have to subtract 400 brfss$timesperyr[!is.na(brfss$BLDSUGAR) &amp; \t\t brfss$BLDSUGAR&gt;400 &amp; brfss$BLDSUGAR&lt;500]= brfss$BLDSUGAR[!is.na(brfss$BLDSUGAR) &amp;  \t\t\t\t\t\tbrfss$BLDSUGAR&gt;400 &amp; brfss$BLDSUGAR&lt;500]-400 table(brfss$timesperyr)\t\t   # Types of variables: numeric, character, factor, date,... class(brfss$timesperyr) is.numeric(brfss$timesperyr) is.numeric(brfss$EmployClean) class(brfss$EmployClean) is.character(brfss$EmployClean)  brfss$EmployCleanFactor=as.factor(brfss$EmployClean) is.factor(brfss$EmployCleanFactor)  #date object brfss$newdate&lt;-as.Date(brfss$IDATE, \"%m%d%Y\")  # Multiple ways to create small data set containing only the variables you care about # If the desired variables are part of the data set: subset&lt;-brfss[,c(\"newdate\",\"timesperyr\",\"DKorRefused\",\"EmployCleanFactor\")]  # If the desired variables are not necessarily part of the data set, combine vectors as below # For example, suppose I create a variable but don't include it in the data set TimesPerYrHigh&lt;-ifelse(brfss$timesperyr&gt;2000,1,0) subset&lt;-data.frame(TimesPerYrHigh, brfss$timesperyr) # to create a matrix, rather than a data set: subset&lt;-cbind(TimesPerYrHigh, brfss$timesperyr)  #To write multiple variables to a spreadsheet, the variables need to be part of the same data frame or matrix object write.csv(subset, file=paste(filepath,\"SubsetData.csv\",sep=\"\"))   # Other R Commands  # the \"by\" command does something to a variable, in groups based on another variable by(brfss$timesperyr, brfss$EmployCleanFactor, mean)    # the \"merge\" command combines multiple data sets with a variable in common.    Factor variables are categorical variables that can be either numeric or string variables.   Missing Data   tiny = read.csv(file=\"MissingData.csv\", header=TRUE) summary(tiny) is.na(tiny) #when there are values that R doesn't recognizes as missing editedtiny=read.csv(file=\"MissingData.csv\", header=TRUE, na.strings=c(\"NaN\", \"NA\", \"N/A\", \"-999\", \"\"))    Dates   # Creates a combined birthday in Month Day Year format (combine multiple columns) combined = with(small, paste(Month.of.Birth, Day.of.Birth, Year.of.Birth)) small[\"Full.Birthday\"] = combined  # Creates a numerical birthday BirthdayNum &lt;- as.Date(small$\"Full.Birthday\", format='%B %d %Y')  # If you had the dates in a different format # %d - day of the month (number) # %m - month (number) # %b - month (3 letter abbreviation) # %B - month (full name) # %y - year (2 digit) # %Y - year (4 digit)  # Examples with different formats of the same date as.Date(\"Oct 15 14\", format='%b %d %y') as.Date(\"October 15th, 2014\", format='%B %dth, %Y') as.Date(\"10/15/14\", format='%m/%d/%y') as.Date(\"15/10/2014\", format='%d/%m/%Y') as.Date(\"10.15.14\", format='%m.%d.%y')  # Special note for months (and days) that are single digits as.Date(\"02012011\", format='%m%d%Y') as.Date(\"212011\", format='%m%d%Y') as.Date(\"2.1.2011\", format='%m.%d.%Y')  # R internally keeps track of dates as \"days since 01/01/1970\" with negative numbers for dates that occurred prior to 01/01/1970 # Can convert the output to this internal form as.numeric(as.Date(\"2.1.1970\", format='%m.%d.%Y'))  # Gets today's date in the same YYYY-MM-DD format as BirthdayNum today &lt;- Sys.Date()  # Package called lubridate is helpful when manipulating dates install.packages(\"lubridate\") library(\"lubridate\")   # Can see what day of the week it is today wday(today, label = TRUE)  # Can see what day of the week people were born on weekdayOfBirth &lt;- wday(BirthdayNum, label = TRUE)  # Create an interval between 2 dates int &lt;- interval(BirthdayNum, today)  # Converts the interval into a period (age in years, months, days, hours, minutes, and seconds) wholeAge &lt;- as.period(int)  # Obtain the years of age from the whole age ageYears &lt;- wholeAge$year  # Adds the whole age birthday to the data frame small[\"Age\"] &lt;- ageYears  # Can see the difference in the way that birthdays and ages are stored when they are plotted par(las=1) plot(BirthdayNum, ageYears, \txlab=\"Calendar Birthday\", \tylab=\"Age in Years\")   Reshaping and Manipulating Data (Tidyverse)      Introduction to Dplyr   #Tibbles are data frames in R with unique printing and subsetting defaults, which are useful when working with large datasets.  health &lt;- as_tibble(ad_health) names(health) names(health) &lt;- c(\"ISO\", \"Location\", \"Year\", \"Source\", \"Indicator\", \"Age\", \"Value\", \"Description\", \"Reference Period\", \"Notes\") summary(health)   #Dplyr is a helpful data manipulation package with five main functions: filter, select, arrange, mutate, and summarise   # 1.) Filter   # Filter is used to view a particular subset of the data   # Example:  # View the percentage of women, between the ages of 18 to 19 years old, in Albania who received antenatal care   # Original R  health[health$Age==\"18-19 yrs\" &amp; health$Location==\"Albania\" &amp; health$Indicator ==\"ANC_1T\"  |health$Age==\"18-19 yrs\" &amp; health$Location==\"Albania\" &amp; health$Indicator == \"ANC1\" | health$Age==\"18-19 yrs\" &amp; health$Location==\"Albania\" &amp; health$Indicator == \"ANC4\" | health$Age==\"18-19 yrs\" &amp; health$Location==\"Albania\" &amp; health$Indicator == \"ANC8\", ]  # Dplyr    health %&gt;% #read as \"then\" \t\tfilter(Age == \"18-19 yrs\", Location == \"Albania\", Indicator == \"ANC_1T\" | Indicator==\"ANC1\" | \tIndicator == \"ANC4\" | Indicator == \"ANC8\") # does not replace original df, assign to new var    # 2.) Select  # Select allows you to view only columns of interest in a dataset   # Example:  # View only the location, indicator, and year columns of the health dataset   # Original R  health[,c(\"Location\", \"Year\", \"Indicator\")]  # Dplyr  health %&gt;%  \t\tselect(Location, Year, Indicator)   #More functionality with \"select\" in Dplyr  # a.) Select allows you to view columns in a sequence   # Example:  # View all of the columns between Location and Value in the health dataset, except the one named Source  health %&gt;%  \t\tselect(Location:Value) %&gt;%  \t\tselect(-Source)  # b.) Select also allows you to view columns with a specific word in the title  health %&gt;%  \t\tselect(contains(\"Indicator\"))\t\t \t\t # Example: Combining Filter &amp; Select  # View only locations where the percentage of females who use contraceptives is greater than 20   # Original R  health[health$Indicator==\"CPMODHS\" &amp; health$Value &gt; 20, c(\"Location\", \"Year\", \"Indicator\", \"Value\")]  # Dplyr  health %&gt;%  \tselect(Location, Year, Indicator, Value) %&gt;%  \tfilter(Indicator == \"CPMODHS\", Value &gt; 20)      Introduction to TidyR   #TidyR is a helpful data reshaping package with four main functions: spread , gather, unite, and separate   # 1.) Spread  # Spread is a useful function for widening long data. Spread is used to convert two columns into multiple columns. The spread function is formatted as so:  # spread(data, key, value) where key is defined as the column whose values will be used as the column headings and value as the values you wish to populate the new cells.  #h_spread &lt;- health %&gt;% spread(Indicator, Value) # A very common error while using spread is: \"Each row of output must be identified by a unique combination of keys\". If this error appears, we must first group the dataset by our key as in the following example.   #Example:  # Create new columns based on each indicator variable. Our key column will be \"Indicator\" and our value column will be \"Value\".  # Firstly, create a subset of the data that only includes the columns of Location, Year, Indicator, and Value   sub_health &lt;- health %&gt;%  \t\tselect(Location, Year, Indicator, Value)  h &lt;- sub_health%&gt;%  \tgroup_by(Indicator) %&gt;%  \tmutate(grouped_id = row_number()) %&gt;%  # create new variable called grouped_id \tspread(Indicator, Value) %&gt;%  \tselect(-grouped_id)  # Next, rename the columns \t names(h)\t\t names(h) &lt;- c(\"Location\", \"Year\",\"First Ante.Vis.\", \"Blood\", \"BP\", \"Urine\", \"1 &lt; Ante.Vis.\", \"4 &lt; Ante.Vis.\", \"8 &lt; Ante.Vis.\", \"Disease.Care\", \"Diarrhoea\", \"Fever\", \"Pneumonia\", \"Contraceptive\", \"C.Section\", \"Family.Plan\", \"Institutional.Delivery\", \"Malaria.Treat\", \"Net.Children\", \"Net.Women\", \"Malaria.Test\", \"Salts\", \"Salts&amp;Zinc\", \"Post.Care.Baby\", \"Post.Care.Mom\", \"Delivery.Attendent.Inst.\", \"Delivery.Attendent\", \"Tobacco\") names(h) dim(h) print(as_tibble(h), n=30)  # Renaming columns using dplyr new_data &lt;- data %&gt;%   rename(id = student_id,          name = student_name,          subject = class)   # 2.) Gather  # Gather is a useful function for elongating wide data.   #Example:  # Merge the columns that contain information regarding a woman seeking care for a specific disease. This includes columns Diarrhoea through Pneumonia  health_Dis &lt;- h %&gt;%  \tgroup_by(Location) %&gt;%  \tselect(Location, Diarrhoea:Pneumonia) %&gt;%  \tgather(Disease, Val, Diarrhoea:Pneumonia) %&gt;% #gather diarrhoe:pneumonia into disease, val col \tprint( n=50) \t \t  # 3.) Unite  # Unite allows you to paste together multiple columns into one   # Example:  # Paste together the columns that contain information regarding the study location, which includes columns \"ISO\" and \"Location\" in the health dataset  health_Un &lt;- health %&gt;%  \t\t\t\tunite(Loc., ISO, Location, sep = \",\") %&gt;%  \t\t\t\tprint( n=30)\t\t\t\t \t\t\t\t\t\t # 4.) Separate  # Separate allows you to turn a single character column into multiple columns   # Example:  # Revert the Loc. column from the previous example back to separate columns for ISO, ie. country/area abbreviations, and location name\t\t\t\t health_Sep &lt;- health_Un %&gt;%  \t\t\t\tseparate(Loc., c(\"ISO\", \"Location\"), sep = \",\") %&gt;%  \t\t\t\tprint( n=30)    using Dplyr with our Cleaned Dataset   # Now that we have created new variables for each indicator, we can more easily view important sections of our dataset  # 1.) Filter   #Example: # View the locations surveyed in 2015 who reported more than 10% of women being tested for malaria   h %&gt;%  \t\tfilter(Year == \"2015\", Malaria.Test &gt; 10)  # 2.) Select  # View only the location, year, and tobacco use   h%&gt;%  \tselect(Location, Year, Tobacco)  # Example: Combining Filter &amp; Select  # View only locations where the percentage of females who use contraceptives is greater than 20  \t h %&gt;%  \tselect(Location, Year, Contraceptive) %&gt;% \tfilter(Contraceptive &gt; 20)\t  # OR   h %&gt;%  \tfilter(Contraceptive &gt; 20) %&gt;%  \tselect(Location, Year, Contraceptive )\t  # 3.) Arrange   # Example: # Sort location by the percentage of women who gave birth in an institution. Sort this from lowest to highest percentage of women.    h %&gt;%  \tselect(Location, Year, Institutional.Delivery) %&gt;%  \tarrange(Institutional.Delivery)\t  # OR   h %&gt;%  \tarrange(Institutional.Delivery) %&gt;%  \tselect(Location, Year, Institutional.Delivery)  # Example:  # Sort location by the percentage of women who had a skilled attendant for her institutional delivery. Sort this from highest to lowest percentage of women.   h %&gt;%   \tselect(Location, Year, Delivery.Attendent.Inst.) %&gt;%   \tarrange(desc(Delivery.Attendent.Inst.))  # 4.) Mutate  # Mutate allows you to add new variables to a dataset   # Example:  # Create a new variable that shows a ratio of the percentage of children who used mosquito nets to the percentage of women who used mosquito nets   # Original R  h_rat &lt;- h[,] h_rat$Ratio &lt;- h$Net.Children / h$Net.Women  h_rat[,c(\"Location\", \"Net.Women\", \"Net.Children\", \"Ratio\")]   # Dplyr  h %&gt;% \tselect(Location, Net.Women, Net.Children) %&gt;%  \tmutate(Ratio = Net.Children/Net.Women) \t  # Note, the order of operations matters when using the verb mutate. Notice, the variable \"Ratio\" would not print in this scenario.  \t h %&gt;% \tmutate(Ratio = Net.Children/Net.Women) %&gt;% \tselect(Location, Net.Women, Net.Children)  \t  # 5.) Summarise   # Summarise allows you to compute a summary statistic, such as mean, for each group in a dataset   # Example:  # Compute the average percent of women surveyed in each location who use tobacco   # Original R  h_sum &lt;- h[,] head(with(h_sum, tapply(Tobacco, Location, mean, na.rm=TRUE))) head(aggregate(Tobacco ~ Location, h_sum, mean), n=4)  # Dplyr  h %&gt;%  \tgroup_by(Location) %&gt;%  \tsummarise(avg_tob = mean(Tobacco, na.rm=TRUE)) %&gt;%  \tprint(n=4)  # Note, the order of operations matters when using the verb summarise. Notice, the variable \"Location\" would not exist in the code below. In this case, we would be calculating a single mean value for the percentage of women surveyed who use tobacco.   h %&gt;%  \tsummarise(avg_tob = mean(Tobacco, na.rm=TRUE)) %&gt;%   \tgroup_by(Location)    # 6.) Group By  # Group by allows you to perform operations on data that are grouped based on a certain variable   # Example:  # Compute the average percent of women surveyed in each location who use tobacco  h %&gt;%  \tgroup_by(Location) %&gt;%  \tsummarise(avg_tob = mean(Tobacco, na.rm=TRUE)) %&gt;%  \tprint(n=4)   hnew&lt;-h%&gt;%   mutate(across(everything(), ~replace(., is.na(.),0))) tail(hnew)   CPS %&gt;%   summarise(across(c(wage, educ, exper, age), mean))   df_2020_county = df_2020 %&gt;%   group_by(county_name,state_po,party) %&gt;%   mutate(id=row_number()) %&gt;% # adds a new column id that serves as a unique identifier for each row within each group.   spread(party,candidatevotes) %&gt;% #  takes key-value pairs and spreads them   select(-id)       there are multiple rows for the same party within a single group (i.e., multiple vote counts for the same party in the same county and state), spread() won’t know how to handle multiple values for the same key in a single group. It requires each combination of key columns and groups to be unique.   Ensuring Uniqueness: By adding an id column that assigns a unique row number to each entry within the groups, you essentially make each row unique even if the party and vote counts are the same.   ","categories": ["R"],
        "tags": ["qai","statistics","R","Dplyr,","tidyR,","robustness,","power"],
        "url": "/r/qai-power-data-cleaning-and-management-dplyr-tidyr-week-3/",
        "teaser": null
      },{
        "title": "random thoughts",
        "excerpt":"          connectivity:       dementia = losing track of k (how many edges we have used so far) in dp, depression = negative weight cycle            induction:       you firmly believe in what happened before, your experiences, then build upon your memories to get the n+1th, your next step            abstraction:       maybe the world we see is a highly abstracted world — especially when the moments where we say “i know what you mean” — the beauty of mathematics come from abstraction.       autoencoders: you close your eyes, try to remember your surroundings — do you remember every single detail? No, you just have “blobs” with some prominent features. It’s the same when we close our eyes for the last time. Maybe our memories are encoded into a tiny latent space, our spirit, to be decoded later            abstract algebra:       abelian, the simple condition that the eq stays the same after switching the order is a crazy useful fact. ex) fundamental thm of finite abelian groups            supernode:       when you don’t know where to start, zoom out and look at the whole picture       I’m glad that our first widely used ai chatbot isn’t correct all the time   waitpid: helicopter parents   np hard problems: sometimes, problems need to be hard   center Z(G): in group theory they are the ones that commute with all elements, i.e., they are the ones that stand firm. Even though they meet the same person in a different position, they still have the same value   The number 80 feels so small, but 80 years feels so long. 80 years can be replaced with 672,768,000 breaths. A lifetime feels so long. Can our brains truly understand infinity?   ","categories": ["life"],
        "tags": ["thoughts","life","cs","math","philosophy"],
        "url": "/life/random-thoughts/",
        "teaser": null
      },{
        "title": "[QAI] splines, trees, linear regression, significance (week 4)",
        "excerpt":"  all notes from week 4   Non-Parametic Models: Splines      def: piecewise polynomials going through given data points and satisfying certain continuity conditions   need to be careful about overfitting   user provides x and y vectors   spline-fitting interpolates, using straight lines, polynomials, or other curves   model for the mean:            Units with the same value of X will have various values of Y       We predict the mean of the values of Y for units who share a value of X       Regression/ANOVA is a parametric model for the mean           if there is really is a linear relationship between X and Y, the tree will try to approximate it with a bunch of horizontal line segments   Regression Trees      def: specific type of decision tree used for regression tasks, where the target variable is continuous   residual: diff between true y and predicted   Connecting splines, tree, and linear regression   one overall equation for multiple predictors      mean(y| x, w) = $\\beta_0+\\beta_1x+\\beta_2w+\\beta_3xw$. Last term is interaction   interaction:            relationship between x and mean might depend on w.       when the effector fo one predictor on outcome depends on another predictor       when w=0: mean = $\\beta_0 + \\beta_1x$       when w=1: mean = $(\\beta_0 + \\beta_2)+ (\\beta_1+\\beta_3)x$       so a no interaction model is a special case of th model that includes an interaction where $\\beta_3=0$           no interaction: when the effect of one predictor on outcome is independent of the other predictor            mean(y|x,w) = $\\beta_0 + \\beta_1x+\\beta_2w$       when w=0: mean = $\\beta_0+\\beta_1x$       when w=1: mean = $(\\beta_0+\\beta_2)+\\beta_1x$           Intro to linear regression      models to compare — parametric models            equal means model:                    mean(y|x) = some number.                        For any value of x, predict that mean(y) is equal to the observed overall mean                                                                                                                        separate means model                    predicts the mean of y that was observed in each group based on x                                                                                                        linear regression model:                    combination of these two                        mean(y|x) = ax+b                                                                                                                            linear regression assumptions            linearity:                    equation relating mean(y) to predictors is correct.           This is cruciual, and is usually not true.                                                                           This doesn’t mean the plot has to be a line! e.g. mean(y                   x) = a+bx+cx^2 → linearity assumption holds since the equation  to relate the predictor, the mean of the outcome, is true.                                                                               independence                    within and between subgroups based on x           one datapoint doesn’t tell us anything about other points                       normality                    of y-values for each subgroup based on X                                                                                                               residuals = actual y - predicted y           thus residual assumed to be normal           p-values, confidence interval, predictions are robust to normality assumption (exception coming)                       equal variance                    of y-values within each subgroup based on x                                                                                                               also known as homoskedasticy           required for p-values, CI to be correct                           Putting it all together            $mean(Y|X) = \\beta_0+\\beta_1X$       $[Y_i|X_i] \\sim N(\\beta_0+\\beta_1X_1, \\sigma^2)$           Practical vs. Statistical Significance      look at the t statisic equation. depends on n, the sample size. If it’s big, we’ll get a bigger t statistic.   You can’t make decisions only on p-values, if there’s a big difference between groups, you should try collecting more data and see if it holds. If there’s a tiny difference, maybe you’re getting statistical difference only because you collected a lot of data.   For a particular study, we expect that larger sample size will lead to bigger t-statistic and smaller p-value   ","categories": ["R"],
        "tags": ["QAI","splines","trees","linear_regression","significance"],
        "url": "/r/splines-trees-linear-regression-significance/",
        "teaser": null
      },{
        "title": "[QAI] Regression",
        "excerpt":"  all notes from QAI   Estimating regression coefficients                                                                                                                                                                                                                    Measuring the relationship between two variables: estimating $\\beta_1$                                                                                 Using correlation to estimate slope            r = sample corrleation                $\\hat\\beta_1 = r\\frac{s_y}{s_x}$ : the estimated slope of a linear regression line is just a version of counting the number of points in each quadrant                                                                                                    Justifying slope estimate as standardized correlation                                                                                       If $x_i = \\frac{x_i-\\bar{x}}{s_x}$ and $y_i= \\frac{y_i-\\bar{y}}{s_y}$, the slope of the regression line connected these standardized versions will be equal to the sample correlation r.  (bc we are creating versions of orginal variaables with s.d. 1)       ex)  If you calculate that sample correlation between income (in dollars) and age (in years) is 0.5 (making this up), and then you decide to report income in terms of thousands of dollars and recalculate, what will the sample correlation be equal to? → 0.5. The sample correlation does not change if you multiple on of the variables by a constant.           Residuals            equal means model: subtract the same mean       sepaerate mean model:subtract the means of each group           Justifying slope estimate by minimizing sums of squared residuals            $\\beta_1 = \\frac{\\sum_{i=1}^{n} (Y_i - \\bar{Y})(X_i - \\bar{X})}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}$       $\\text{SSR} = \\sum_{i=1}^{n} \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right)^2$, we want to minimze this       $\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{X}$, plugging everything in, we can estimate the slope           Justifying slope estimate by noticing that it is unbiased            so far, we’ve been talking about one dataset. But if our model ( the line) is true, the dataset is one of many that could have been generated.       you can repeatedly drawm samples of y for each x, and get estimates of  beta_0 and beta_1 → create a distribution of those estimates →  the average would be ur true beta_0 and beta_1       ex)  Consider the following simulation. We choose the following model for the mean: Mean(Y|X) = 2+3*X. We set the sample size to n=3, with X values 1, 2, and 3. Then, repeatedly, we simulate the values of Y for these three data points by calculating Mean(Y|X) and adding (standard normal, independent) noise. For each simulated set of Y’s, we estimate the slope of the regression of Y on X. We do this 1000 times, generating 1000 estimated slopes. What do we expect to see as the mean of these 1000 estimated slopes? →  3, which is the true slope of the line. This is the definition of unbiasedness: the average of our estimates over all possible data sets drawn from a certain model is equal to the true slope in the model.                    Maximum Likelihood Estimate                             says: use the estimates that make the data minimally suprising,     2. Weighted average of slopes                      \\[\\beta_1 = \\frac{\\sum_{i=1}^{n} (Y_i - \\bar{Y})(X_i - \\bar{X})}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}= \\frac{1}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2} \\sum_{i=1}^{n} (Y_i - \\bar{Y})(X_i - \\bar{X})= \\sum_{i=1}^{n} \\frac{1}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2} (Y_i - \\bar{Y})(X_i - \\bar{X})= \\sum_{i=1}^{n} \\frac{(Y_i - \\bar{Y})(X_i - \\bar{X})}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}= \\sum_{i=1}^{n} \\frac{(Y_i - \\bar{Y})(X_i - \\bar{X})(X_i - \\bar{X})}{(X_i - \\bar{X}) \\sum_{i=1}^{n} (X_i - \\bar{X})^2}= \\sum_{i=1}^{n} \\frac{(Y_i - \\bar{Y})(X_i - \\bar{X})^2}{(X_i - \\bar{X}) \\sum_{i=1}^{n} (X_i - \\bar{X})^2}= \\sum_{i=1}^{n} \\left( \\frac{(Y_i - \\bar{Y})(X_i - \\bar{X})^2}{(X_i - \\bar{X}) \\sum_{i=1}^{n} (X_i - \\bar{X})^2} \\right)\\]                                  you can see something “slope-like” at the end. think of the other part as a “weight” of the slope. The further x_i is from the x_bar, the bigger the value would be → the points that have x’s that are far away from x_bar would have a big influence on what the slope would be                                                                                                    diff point of view: we want to minimize SSR → more weight on               points that are far from middle                                       Inferences about regression      Comparison to t-tests                     take advantage of CLT to say                                                                                                         randomly pick some x, find the y on the line, and draw from normal dstiribtuion centered at that y with sigma squared variance. i.e, plug random x into $\\beta_0+\\beta_1x$ and add some normal error                   Hypothesis tests with linear regression            how far our estimate of the slope depends on  1. the residual variance 2. more x away from mean means bigger denominator → better estimate of our slope        \\[\\hat{\\beta}_1 \\sim N\\left(\\beta_1, \\sigma^2 \\left( \\frac{1}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2} \\right) \\right)\\]               \\[\\frac{\\hat{\\beta}_1 - \\beta_1}{\\sqrt{\\sigma^2 \\left( \\frac{1}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2} \\right)}} \\sim N(0, 1) \\frac{\\hat{\\beta}_1 - \\beta_1}{\\sqrt{\\hat\\sigma^2 \\left( \\frac{1}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2} \\right)}} \\sim t_{n-2}\\]               \\[P \\left( \\hat{\\beta}_1 + 2 \\sqrt{\\frac{\\hat{\\sigma}^2}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}} \\text{ includes } \\beta_1 \\right) = 0.95 \\]              variance of intercept:        \\[\\hat\\beta_0 \\sim  N(\\beta_0, \\sigma^2 \\left( \\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum (X_i - \\bar{X})^2} \\right) )\\]               \\[\\hat{\\mu}(Y|X=X_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_0\\]              (estimate mean of y given some x value)        \\[\\hat{\\beta}_0 + \\hat{\\beta}_1 X_0 \\sim N\\left( \\beta_0 + \\beta_1 X_0, \\sigma^2 \\left( \\frac{1}{n} + \\frac{(X_0 - \\bar{X})^2}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2} \\right) \\right) \\]              (if i repeatedly sample and fit line, it varies, but by CLT, values are centered at true slope/intercept           Intervals for regression      A simpler case            CI:                    A range of values such that we think there’s a 95% chance that this interval covers the true population mean. So the more data I collect, the narrower this interval would be           i.e., as sample size increases, the confidence intervals width will shrink towards zero, but the prediction interval will still be wide           $\\bar{Y} \\pm 2 \\, \\text{SE}(\\bar{Y}) \\Rightarrow \\bar{Y} \\pm 2 \\sqrt{\\frac{S^2}{n}} $                       this is NOT the same as a prediction interval for a future value of y.                    $Y: \\quad \\bar{Y} \\pm 2 \\sqrt{S^2 + \\frac{S^2}{n}}$           S^2: estimated variance of y values given $\\mu$           prediction interval will be wider then the confidence interval, because the actual data points vary around the mean                           Two types of confdience intervals for regression            confidence Intervals for coefficients: $\\beta_0, \\beta_1$                    $P\\left( \\hat{\\beta}_0 \\pm 2 \\, \\text{SE}(\\hat{\\beta}_0) \\text{ covers } \\beta_0 \\right) = 0.95 $           $P\\left( \\hat{\\beta}_1 \\pm 2 \\, \\text{SE}(\\hat{\\beta}_1) \\text{ covers } \\beta_1 \\right) = 0.95 $           $\\text{SE}(\\hat{\\beta}_0) = \\sqrt{\\hat{\\sigma}^2 \\left( \\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum (X_i - \\bar{X})^2} \\right)} $           $\\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\frac{\\hat{\\sigma}^2}{\\sum (X_i - \\bar{X})^2}} $                                                                       confidence intervals for mean( Y               X= X_0)                                                                                                                     $\\text{Mean}(Y                   X=X_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_0 $                                                                                                                                   $P \\left( (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_0) \\pm 2 \\sqrt{\\sigma^2 \\left( \\frac{1}{n} + \\frac{(X_0 - \\bar{X})^2}{\\sum (X_i - \\bar{X})^2} \\right)} \\text{ covers mean}(Y                   X=X_0) = (\\beta_0 + \\beta_1 X_0) \\right) = 0.95 $                                                                                        Prediction intervals for linear regression                                                                                 ","categories": ["R"],
        "tags": ["QAI","regression"],
        "url": "/r/qai-regression/",
        "teaser": null
      },{
        "title": "[QAI] ANOVA & Regression (week 7)",
        "excerpt":"  all notes from QAI   Regression and ANOVA are different ways of formatting output from the same model   One predictor, categorical or continuous   #data cleaning  attach(msleep)  plot(bodywt, sleep_total) plot(bodywt, log(sleep_total)) plot(log(bodywt), log(sleep_total)) plot(log(bodywt), log(sleep_total), \tcol=as.factor(vore)) \t logwt&lt;-log(bodywt)\t logsleep&lt;-log(sleep_total) vore&lt;-as.factor(vore) \t boxplot(logsleep~vore)  summary(vore)  levels(vore) levels(vore)&lt;-c(levels(vore),\"missing\") vore[is.na(vore)]&lt;-\"missing\"  summary(vore)  boxplot(logsleep~vore)   Q: How did we handle the missingness on the variable called “vore”?   A: Creating a factor level called “missing.” Especially when the variable is already a categorical variable, creating a category for missing values is a straightforward, assumption-free way to move forward with the analysis without dropping any rows that are missing on that variable. This method ignores any information in the rest of the data about best guesses at those missing values, though, as well as what you know from the context about how the pattern of missingness relates to the missing values.   ### Model 1: logsleep by vore  # Calculate means of logwt by vore by(logsleep,vore,mean)  #### Run ANOVA of logsleep by vore boxplot(logsleep~vore,ylim=c(0,3)) m1.aov&lt;-aov(logsleep~vore) summary(m1.aov) plot(m1.aov$fitted.values, m1.aov$resid) abline(h=0) m1.aov$fitted.values temp&lt;-data.frame(vore,round(cbind(logsleep, m1.aov$fitted.values, m1.aov$resid),3)) colnames(temp)&lt;-c(\"vore\",\"logsleep\",\"fitted\",\"resid\") temp  SSR.1&lt;-sum(m1.aov$resid^2) SSR.1 SST.1&lt;-sum((logsleep-mean(logsleep))^2) SSM.1&lt;-SST.1-SSR.1 SSM.1  #### Run regression of logsleep by vore plot(vore, logsleep, ylim=c(0,3)) m1.lm&lt;-lm(logsleep~vore) summary(m1.lm)  summary(m1.lm$resid)  plot(m1.lm$fitted.values, m1.lm$resid) abline(h=0) cbind(m1.aov$fitted.values, m1.lm$fitted.values) identical(m1.aov$resid,m1.lm$resid)  m1.lm$coef m1.aov$coef  by(logsleep,vore,mean)  summary(m1.aov$resid)  summary(m1.aov) anova(m1.lm)  # confirm residual standard error MSE&lt;-SSR.1/78 sqrt(MSE) # confirm R^2 SSM.1/SST.1 1-SSR.1/SST.1 # confirm adjusted R^2 1-(SSR.1/78)/(SST.1/82)    ### Model 2: logsleep by logwt  plot(logwt,logsleep) boxplot(logsleep~logwt) table(round(logwt)) boxplot(logsleep~round(logwt))  plot(logwt,logsleep)   #### Run regression of logsleep by logwt m2.lm&lt;-lm(logsleep~logwt) summary(m2.lm) plot(m2.lm$fitted.values, m2.lm$resid) abline(h=0)  plot(logwt,logsleep) abline(m2.lm)  #### Run ANOVA of logsleep by logwt m2.aov&lt;-aov(logsleep~logwt) summary(m2.aov) plot(m2.aov$fitted.values, m2.aov$resid) abline(h=0)   SSR.2&lt;-sum(m2.lm$resid^2) SSR.2 SST.2&lt;-sum((logsleep-mean(logsleep))^2) SSM.2&lt;-SST.2-SSR.2 SSM.2  (-7.388)^2  m2.lm$coef m2.aov$coef   Q: If you run an ANOVA, what is the fitted value of logsleep for the group herbivore?   A: The mean value of logsleep observed for the herbivore group.   Q: How do you get the SSR from an ANOVA output if the ANOVA model is named m1?   A: sum(m1$resid^2)   Q: Which of the following is the Mean Square Error in ANOVA?   A: Sum of Squared Residuals / Residual degrees of freedom   Q:  For a linear model with more than one coefficient specified, what does the p-value for each coefficient show?   A: The p-value for a particular coefficient is the result of a test comparing (i) the actual model that you ran with (ii) the same model with that particular term omitted.   Q: What does the bottom row output of the summary for the linear model “lm” show?   A: The bottom row summarizes an F-test comparing the entire model you specified to the equal means model.   Q:  What is the residual standard error?   A: SquareRoot(SSR/degrees of freedom)   Q: Check all that equal to SSM/SST:   A: R-Squared , 1-SSR/SST   Q: Suppose that the rows in your data set are US counties, and your outcome variable is the total number of people in each county who vote in an election. If your predictor variable is the state in which each county is located, and your research question is whether voting varies across states, which type of output would you rather see?   A: ANOVA. The ANOVA table will provide one p-value for the entire categorical state variable, reporting the results of an F-test that compares the equal means model to the model that predicts a separate mean for each state. The default regression output might report the results of the same F-test as an afterthought at the bottom of the output, but the regression output focuses on the separate coefficients for each state.   Q:  In the same context as the previous problem, if you are interested in which states have the highest and lowest numbers of voters, which type of output would you rather see?   A: Regression output. The ANOVA output will evaluate whether the state variable as a whole is useful for predicting vote counts, but the ANOVA table will not report which states have higher or lower vote counts. The regression output, though, includes the estimated coefficients for each state (except for one baseline state), so that you can assess which states have higher or lower predicted values. Each p-value in the regression output compares the model where every state has a different mean vote count to the model where every state has a different mean vote count, except that the state associated with that p-value has the same mean vote count as the baseline state. This may not be a particularly interesting test!   ### Model 3: logsleep by logwt and vore  plot(logwt, logsleep, \tcol=as.factor(vore)) boxplot(logsleep~round(logwt)+vore)  #### Run ANOVA of logsleep by vore and logwt m3.aov&lt;-aov(logsleep~vore+logwt) summary(m3.aov) plot(m3.aov$fitted.values, m3.aov$resid) abline(h=0)  SSR.3&lt;-sum(m3.aov$resid^2) SSR.3 SST.3&lt;-sum((logsleep-mean(logsleep))^2) SSM.3&lt;-SST.3-SSR.3 SSM.3  #### Run regression of logsleep by vore and logwt m3.lm&lt;-lm(logsleep~vore+logwt) summary(m3.lm) plot(m3.lm$fitted.values, m3.lm$resid) abline(h=0)  # confirm residual standard error sqrt(SSR.3/77) # confirm R^2 SSM.3/SST.3 1-SSR.3/SST.3 # confirm adjusted R^2 1-(SSR.3/77)/(SST.3/82)  # F-test shown in regression output no longer equivalent to F-tests in ANOVA output # but: (-7.633)^2  cbind(m3.lm$fitted.values, m3.aov$fitted.values)   Q: When there are multiple predictors in the model, can you use the residuals to directly calculate the sum of squared residuals?   A: Yes. The sum of squared residuals is always literally the sum of the squared residuals. Note that the residuals themselves change when we add terms to the model, because the fitted values change.   Q: You are trying to predict a student’s GPA using the sports that a student plays as a categorical variable. You think that some sports have a greater impact than others on a student’s GPA. For example, you think that playing basketball has a larger impact on a student’s GPA than playing field hockey. If you want to test this assumption, and if your research question is about trying to come up with a model that might include some of the possible sports but not all of them in one categorical variable, which model output should you look at?   A: Linear regression. ANOVA will not show the coefficients or significance of individual sports, only a test for the overall correlation of the categorical sport variable with GPA.   Q: If you are trying to decide whether you should include the categorical variable of a Wellesley student’s major in a model determining the student’s future median income, which output’s default would be most useful to the test whether the student’s major is an important predictor?   A: ANOVA. In this case, we are interested in whether the variable as a whole matters, not in comparing specific majors to each other.   Q: When you have multiple predictors, is the F-test shown at the bottom of the Linear Regression output also in the output of the ANOVA?   A: . The F-test in the Linear Regression output compares the entire model to equal means model, while the tests shown in ANOVA table examine each variable individually.   Categorical vs. continous   ### Categorical v. continuous variables  # Continuous variable - mistake to handle as categorical  summary(sleep_rem) # some missingness, for this section allow rows to drop boxplot(sleep_rem) plot(sleep_rem, logsleep)  summary(lm(logsleep~sleep_rem)) summary(aov(logsleep~sleep_rem))  boxplot(logsleep~as.factor(sleep_rem))  summary(lm(logsleep~as.factor(sleep_rem))) summary(aov(logsleep~as.factor(sleep_rem)))  # Categorical variable - mistake to handle as continuous  voreAsNum&lt;-as.numeric(vore) table(voreAsNum) summary(voreAsNum) summary(vore) table(vore,voreAsNum)  summary(lm(logsleep~voreAsNum)) summary(aov(logsleep~voreAsNum))  plot(voreAsNum,logsleep) abline(lm(logsleep~voreAsNum))   Q&amp;A Section   Q: What is one way to tell that we incorrectly handled a continuous variable as a factor or categorical in an ANOVA?   A: The degrees of freedom is not 1. A continuous variable will have a degrees of freedom of 1. On the other hand, if your categorical variable has more than one category and the degrees of freedom is shown to be 1, then you will know that the variable was incorrectly treated as continuous.   Q: Suppose you are trying to predict height and your model includes a variable for the length of an individual’s left foot and a variable for the length of an individual’s right foot. When you run a linear regression on this model, which of the following tests will likely be significant? Check all that apply.   A: The F-test at the bottom of the linear output will be significant because the model you are testing is more appropriate than the equal means model. However, the t-test associated with the left foot coefficient compares a model with only the right foot to the model with both feet - the feet are very correlated, so the right foot is good enough, and the p-value is large. The t-test associated with the right foot coefficient compares a model with only the left foot to the model with both feet - the left foot is good enough, and so the p-value is large. When you put two correlated variables in a model as predictors, the individual p-values for the coefficients will not be significant, because a model that only includes the other predictor is fine. However, these large p-values do not imply that the information in these variables is not important. Either left foot or right foot would be significant if only one of them is included in the model.   Q: Would it be sufficient to run a linear regression with several predictors and then omit the variables that are not significant?   A: No, the variables that are not significant could be important, but perhaps they are correlated with other predictors that you included in the model.   Q: Consider two predictor variables that, individually, would be significant for predicting an outcome variable. These predictor variables are highly correlated with one another. What will we see from an ANOVA output when we run, in one model, these two variables as predictors for the outcome variable?   A: The variable specified first would be significant and the variable specified second would not be significant. The default in R is that order matters in an ANOVA output. Therefore, the variable specified first will be statistically significant.   Q: Given the msleep data, suppose that you run a linear model to predict logsleep. You specify the predictors logwt and vore, in that order, with no interaction. Which of the following tests will be reported as part of the default regression output, in R?   A:      Comparison between equal means model and model that includes logwt and vore   Comparison between model that includes both logwt and vore, and model that includes only vore   Comparison between model that includes both logwt and vore, and model that includes both logwt and vore but assumes that herbivores have the same mean log sleep as the baseline vore (say, carnivores)   Q: which of the following tests will be reported as part of the default ANOVA output, in R?   A: Comparison between model that includes both logwt and vore, and model that includes only logwt:  is the test in the row labeled “vore” in the ANOVA output. There is no such row in the regression output, because separate tests for the coefficients of each level of the categorical variable (except for one baseline level) are shown on different rows.     - Comparison between equal means model and model that includes logwt and vore: the F-test shown at the bottom row of the regression output. If there is only one predictor, the bottom row of the regression output shows the same F-test that is shown in the corresponding ANOVA table. If there are multiple predictors, the bottom row of the regression output shows an F-test that is not equivalent to anything shown in the default ANOVA table. - Comparison between model that includes both logwt and vore, and model that includes only vore:  is the test in the row labeled \"logwt\" in the regression output. This test actually does not appear in the ANOVA if the predictors are specified in the order given here, but it would have if the predictors had been specified in the opposite order - Comparison between model that includes both logwt and vore, and model that includes both logwt and vore but assumes that herbivores have the same mean log sleep as the baseline vore (say, carnivores):  is the test in the row labeled \"herbi\" in the regression output. There is no such row in the ANOVA table, because the ANOVA reports tests of the entire categorical variable rather than coefficients for specific levels of the categorical variable.  Q: For which of the types of R output does the order of the predictors determine the tests that are reported?   A: ANOVA. The regression output will always show the results of tests comparing the entire model, to the entire model without each predictor. R's default ANOVA output will built up the predictors one by one, testing whether the first predictor alone is better than the equal means model; whether the first two predictors together are better than the first predictor alone; whether the first three predictors together are better than the first two predictors together; etc. Though it seems strange that the output would depend on the variable order, the tests shown in this ANOVA output are more likely to show us whether we have important predictors in the data set. If two predictors are highly correlated with each other and with the outcome, the tests shown in the linear model for each of the predictors may not be significant, because including just one of the two predictors is sufficient (aka \"multicollinearity\").    ","categories": ["R"],
        "tags": ["QAI","statistics","R","ANOVA","regression"],
        "url": "/r/qai-anova-regression/",
        "teaser": null
      },{
        "title": "[QAI] Model Selection (week 7)",
        "excerpt":"  Model Selection      Intro to model selection (parametric)   → which terms to include in a linear model   plan: choose a model that fits the data without using too manuy parameters   tradeoff: more parameters lead to better fit/smaller residuals but also lead to “over-fitting”   overfitting: when the model too closley describes the data, not useful for generalization      measure of model fit            R^2 = 1 - SSR (residual varaiblity) / SST (overall variability) = SSModel/SST       R^2: is the proportion of variability in the outcome variable that is due to the predictors in your model rather than noise       i.e. the proportion of varaiblity explained by your model       you can calculate R^2 from a linear regression model with any number of predictors or terms       if you do have one predictor/term (simple linear regression), then R^2 = r^2       R^2 increases (or at least does not decrease) every time a term is added to the model, even if the term is not useful       so you can’t just chagne the model with the highest R^2           R-squared is equal to 1 when # of parameters equals number of data points ( you are overfitting)            prefer: many more data points than parameters to estimate                when you consider a subset of predictors, either fewer predictors or a smaller range of any particular predictor, R^2 decreases.                   R^2 is not a measure of linearity              ","categories": ["R"],
        "tags": ["QAI","statistics","model"],
        "url": "/r/qai-model-selection-week-7/",
        "teaser": null
      },{
        "title": "loguru",
        "excerpt":"  I’ve used logging so far, but this seems easier to use / better functionailties so I’ll try using this now   pip install loguru  from loguru import logger logger.debug(\"Starting logging...\")  @logger.catch def my_function(x, y, z):     # An error? It's caught anyway!     return 1 / (x + y + z)   logger.trace(\"A trace message.\") logger.debug(\"A debug message.\") logger.info(\"An info message.\") logger.success(\"A success message.\") logger.warning(\"A warning message.\") logger.error(\"An error message.\") logger.critical(\"A critical message.\")   detailed error logs   # Caution, \"diagnose=True\" is the default and may leak sensitive data in prod logger.add(\"out.log\", backtrace=True, diagnose=True)  def func(a, b):     return a / b  def nested(c):     try:         func(5, c)     except ZeroDivisionError:         logger.exception(\"What?!\")  nested(0)   gives u:   2018-07-17 01:38:43.975 | ERROR    | __main__:nested:10 - What?! Traceback (most recent call last):    File \"test.py\", line 12, in &lt;module&gt;     nested(0)     └ &lt;function nested at 0x7f5c755322f0&gt;  &gt; File \"test.py\", line 8, in nested     func(5, c)     │       └ 0     └ &lt;function func at 0x7f5c79fc2e18&gt;    File \"test.py\", line 4, in func     return a / b            │   └ 0            └ 5  ZeroDivisionError: division by zero   ","categories": ["project"],
        "tags": ["logging","python"],
        "url": "/project/loguru/",
        "teaser": null
      },{
        "title": "cron tutorial",
        "excerpt":"setting up cron      create scripts folder mkdir scripts   create your direction file touch run_gtrends_politics.sh  in it        write what you want to repeat in that shell script:       if you don’t have virtual environments, you don’t need the env activation lines       example 1: if z shell &amp; conda         #!/usr/bin/env zsh \t   source ~/.zshrc #we need this   echo $SHELL   conda activate ml   cd  /Users/ihoonsun/Desktop/summer\\ projects/sci/summer2024/gtrends_politics   python example_script.py   conda deactivate           example 2: if bash shell &amp; venv:         #!/bin/bash   echo $SHELL   export PATH=\"/Users/ihoonsun/environments/naver-venv/bin:$PATH\"   source /Users/ihoonsun/environments/naver-venv/bin/activate   if [[ \"$(which python)\" == \"/Users/ihoonsun/environments/naver-venv/bin/python\" ]]; then       echo \"venv activated\"   else       echo \"failed to activate\"   fi   cd /Users/ihoonsun/Desktop/naver    /Users/ihoonsun/environments/naver-venv/bin/python3.8  -m pytest scraper.py --headless #run pytest   deactivate \t           open shell configuration file and add  environment variables if needed            ex) your openai API key             #directions   # setting API_KEY environment variable   # open shell configuration file  (~/.bashrc, ~/.bash_profile, or ~/.zshrc)   # write: export API_KEY=your_api_key_value and save   # check if it saved correctly by: echo $API_KEY           crontab -e  to open crontab.   0 7-23 * * * /Users/ihoonsun/scripts/run_pytest.sh add to crontab   you can check crontab with crontab -l   Q&amp;A section   problem: need to do conda init before you need to conda activate   solution: source ~/.bashrc  → conda init --all     problem: conda (base) shows up on terminal prompt   solution: conda config --set auto_activate_base False     problem: permissioned denied to some file   solution: chmod +x path/to/file (chmod: command to change file permissions, +x: adds executable permission to file)     problem: editor isn’t nano   solution: export EDITOR=nano (sets nano to default), for crontab→ export VISUAL=nano     problem: command not found   solution: check shebang (character sequence at the beginning of a script that specifies the path to the interpreter that should execute the script) &amp; source the z shell becaues cron jobs run in a much more limited environment compared to your interactive shell session, we need the .zshrc to be executed because it’s where we initialized conda   Extra information (thank you StackOverflow, Google, and chatGPT)      what is z shell / bash shell?            zsh: Unix shell that can be used as an interactive login shell and as a powerful command interpreter for shell scripting. Incorporates features of other shells such as bash, ksh, and tcsh, making it highly versatile and customizable.       bash: another Unix shell and command language, which is the default shell on many Linux distributions and macOS. It is an improved version of the original Bourne shell (sh)           what is Shebang #!?            a character sequence at the beginning of a script that specifies the path to the interpreter that should execute the script. Is followed by the absolute path to the interpreter.       example usage: #!/usr/bin/env zsh  ← telling system to use the zsh interpreter found by the env command in the user’s environment.                what’s that line of code we’re adding to crontab?         * * * * *  command to execute   ┬ ┬ ┬ ┬ ┬   │ │ │ │ │   │ │ │ │ │   │ │ │ │ └─── day of week (0 - 7) (Sunday is 0 or 7)   │ │ │ └───── month (1 - 12)   │ │ └─────── day of month (1 - 31)   │ └───────── hour (0 - 23)   └─────────── minute (0 - 59)          ","categories": ["project"],
        "tags": ["tutorial","scraping"],
        "url": "/project/cron-tutorial/",
        "teaser": null
      },{
        "title": "streamlit 101",
        "excerpt":"  learning how to use streamlit for simple web apps in Python   Introduction     Streamlit info            Streamlit reruns your entire Python script from top to bottom any time something needs to be updated on the page                    Whenever the app’s source code is modified           Whenever a user interacts with widgets in the app. e.g. entering text, clicking a button.                           Initialization (coding right into browser)            connect your Github account       create a new app, click “open github codespace”       open streamlit_app.py, compare with “simple browser” tab to understand the code           Basic Commands          Basic commands        streamlit run your_script.py [-- script args]                  API Link       “magic” commands                    Any time Streamlit sees either a variable or literal value on its own line, it automatically writes that to your app using st.write                    # Draw a title and some text to the app:  '''  # This is the document title \t\t  This is some _markdown_.  ''' \t\t  import pandas as pd  df = pd.DataFrame({'col1': [1,2,3]})  df  # 👈 Draw the dataframe \t\t  x = 10  'x', x  # 👈 Draw the string 'x' and then the value of x \t\t  # Also works with most supported chart types  import matplotlib.pyplot as plt  import numpy as np \t\t  arr = np.random.normal(1, 1, size=100)  fig, ax = plt.subplots()  ax.hist(arr, bins=20) \t\t  fig  # 👈 Draw a Matplotlib chart                       st.write                    You can pass in multiple arguments, all of which will be written.           Its behavior depends on the input types as follows.           It returns None, so its “slot” in the App cannot be reused.                    import streamlit as st  import pandas as pd \t\t  st.write(\"Here's our first attempt at using data to create a table:\")  st.write(pd.DataFrame({      'first column': [1, 2, 3, 4],      'second column': [10, 20, 30, 40]  }))                   d. when to not use st.write?            import streamlit as st  import numpy as np  import pandas as pd \t\t  # example 1  dataframe = pd.DataFrame(      np.random.randn(10, 20),      columns=('col %d' % i for i in range(20))) \t\t  st.dataframe(dataframe.style.highlight_max(axis=0))  # can use Pandas Styler object to highlight some elements in the interactive table. \t\t  # example 2  data = pd.DataFrame(       np.random.randn(20, 3),       columns=['a', 'b', 'c']) \t\t  st.line_chart(data) #add a line chart  \t\t  # example 3  st.map(data) # display data points on a map                           d. widgets        ```python \t  # when user moves the slider to position 10, Streamlit will rerun the code above  # and set x to 10 accordingly. New result \"10 squared is 100\". \t  #accessing by variable  x = st.slider('x')  # 👈 this is a widget  st.write(x, 'squared is', x * x) \t  #accessing by key:  # specify a string to use as the unique key for the widget  st.text_input(\"Your name\", key=\"name\")  st.session_state.name # to access the value. Automatically added to Session State  ```           e. st.checkbox        ```python  if st.checkbox('Show dataframe'):      chart_data = pd.DataFrame(         np.random.randn(20, 3),         columns=['a', 'b', 'c']) \t             chart_data  ```           f. st.selectbox        option = st.selectbox(      \"How would you like to be contacted?\",      (\"Email\", \"Home phone\", \"Mobile phone\")) \t  st.write(\"You selected:\", option)           g. layout                       st.sidebar: left panel sidebar             # Add a selectbox to the sidebar:   add_selectbox = st.sidebar.selectbox(       'How would you like to be contacted?',       ('Email', 'Home phone', 'Mobile phone')   ) \t\t   # Add a slider to the sidebar:   add_slider = st.sidebar.slider(       'Select a range of values',       0.0, 100.0, (25.0, 75.0)   )                       st.columns: place widgets side-by-side                st.expander : conserve space by hiding away large content             left_column, right_column = st.columns(2)   # You can use a column just like st.sidebar:   left_column.button('Press me!') \t\t   # Or even better, call Streamlit functions inside a \"with\" block:   with right_column:       chosen = st.radio(           'Sorting hat',           (\"Gryffindor\", \"Ravenclaw\", \"Hufflepuff\", \"Slytherin\"))       st.write(f\"You are in {chosen} house!\")                           h. st.progress        ```python  # Add a placeholder  latest_iteration = st.empty()  bar = st.progress(0) \t  for i in range(100):    # Update the progress bar with each iteration.    latest_iteration.text(f'Iteration {i+1}')    bar.progress(i + 1)    time.sleep(0.1) \t  '...and now we\\'re done!'  ```                advanced concepts (efficiency)       \t  @st.cache_data  def load_data(nrows):                  Caching:save the output of a function so you can skip over it on rerun.       Session States: save information for each user that is preserved between reruns.                    review cs240 caching notes!                           c. st.cache_data        - tells Streamlit that whenever the function is called that it should check two things:      - the input parameters you used for the function call      - the code inside the function.  - should avoid mutating the output of a function cached with `st.cache_data` since cached values are stored by reference           ","categories": ["project"],
        "tags": ["python","visualization"],
        "url": "/project/streamlit-101/",
        "teaser": null
      },{
        "title": "[ssh key] for first time git users",
        "excerpt":"  Connect!   cd ~/.ssh ssh-keygen -t rsa -C \"your-email-address\" -f \"github-username\" ssh-add -K ~/.ssh/github-username pbcopy &lt; ~/.ssh/github-username.pub   now paste it into your settings&gt; new ssh key   cd .ssh touch config # if config file does not exist open config   add this to the config file:   #Johanna Github Host github.com-hl105 \tHostName github.com \tUser git \tIdentityFile ~/.ssh/github-hl105   Q&amp;A   Q: I’m getting this scary message: IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY   A: go to .ssh/known_hosts and get the host name: something like 192.168.3.10   Then do:   ssh-keygen -R &lt;host&gt;    ","categories": ["project"],
        "tags": ["git","ssh-key","system"],
        "url": "/project/ssh-key-for-first-time-git-users/",
        "teaser": null
      },{
        "title": "Is Mathematics Invented or Discovered?",
        "excerpt":"  Thoughts on Roger Penrose’s  interview: Is Mathematics Invented or Discovered?   Link to interview: https://www.whyarewehere.tv/people/roger-penrose/   Is mathematics something that we discover or just something that we’re making up?      RP: Now, you see, the thing is, what’s deep about it is that if you introduce this notion ‒ which at first sight looks like an invention ‒ it unfolds an entire world that you had no conception of, and you didn’t invent that: that came as a gift.    In middle school I used to feel bad for imaginary numbers; the little i seemed so lonely alone, not being able to join its real number friends unless it found another imaginary number i to join. This interview brings me back to this memory, but with a different point of view: the imaginary number now reminds me of Hagrid, who leads Harry Potter to the world of magic. When Hagrid shows up on his doorstep, Harry realizes the wizarding world has always coexisted with the muggle world. The magical signs explain the crazy events — like the snake talking — he experienced in his everyday life. Would it have been possible for Harry to wake up one day and the idea of a wizarding world where a bad guy called Voldemort trying to kill him to suddenly strike him? No. But Harry could have reached platform 9 3/4 without Hagrid. He could have managed to snatch the Hogwarts letter before uncle Vernon did, or he could have learned to control and use his magic skills alone. Regardless, he would have arrived at the same destination that is the wizarding world. Likewise, we could have decided on a different way to define the square root of minus one, instead of the i we have right now. But that invention would have also opened up a new world of quantum mechanics too, just like how the imaginary number i did.      Number theory? Does that have any relevance to the physical world? Maybe, we don’t know. There’s no particular importance, maybe a few odd things, but nothing of really great importance which seems to have developed in the physical world – that may be a temporary thing.    It’s interesting how Number theory has become so important and widely used in computer science, in fields like crypotgraphy and alogirhtm design. One of the first cs class I took was called “Mathematics for Computer Science” where we went over number theory (fermat’s little theorem! Counting! RSA!) But the      RP: I think that’s not the way I would look at things completely. You see it’s all to do with this consciousness issue, and I think something else comes in which is outside the science that we presently know. It doesn’t mean it’s outside science. So you see, when you’re asking me is it all science, well, science is limited at the moment, because it only deals with certain areas which don’t include that.    What divides natural and supernatural? Why do humans choose to put their faith in a supernatural realm, beyond our universe, when all that we percieve is the natural world? Is it also an invention that we created to reach beyond our physical perceptions, like how the imaginary number revealed the quantum world to us? Maybe then both faith and mathematics are just two differnet methods to reach the same destination.   ","categories": ["life"],
        "tags": ["thoughts","mathematics","philosophy"],
        "url": "/life/is-mathematics-invented-or-discovered/",
        "teaser": null
      },{
        "title": "python review (ongoing)",
        "excerpt":"decorators      def: wrap another function to extend the behavior of the wrapped function w/o permanently modifying it   Python first class objects            properties of first class functions:                    func is an instance of the Object type           store the func in a variable           pass the func as a parameter to another           you can return the func from a func           you can store funcs in data structures                                ex)            def shout(text):       return text.upper()  \t\t      # EXAMPLE 1  yell = shout   print (yell('Hello')) # same as printing shout('Hello') \t\t  def greet(func):       # storing the function in a variable       greeting = func(\"\"\"Hi, I am created by a function                       passed as an argument.\"\"\")       print (greeting)   \t\t  # EXAMPLE 2  greet(shout) #returns all uppercase \t\t  # EXAMPLE 3  def create_adder(x):       def adder(y):           return x+y  \t\t        return adder # returns a function \t\t  add_15 = create_adder(15) # a func that adds 15  print (add_15(10))                            Decorators            functions are taken as the argument into another function and then called inside the wrapper function            @gfg_decorator  def hello_decorator():      print(\"Gfg\")  \t     \t      # SAME AS     hello_decorator = gfg_decorator(hello_decorator) \t  ###################################### \t  # defining a decorator  def hello_decorator(func): \t      def inner1(): # wrapper function      \"\"\"      can access the outer functions (func)      \"\"\"          print(\"Hello, this is before function execution\")          func() # calling the actual function          print(\"This is after function execution\")      return inner1 \t  # defining a function, to be called inside wrapper  def function_to_be_used(): \t      print(\"This is inside the function !!\") \t   # passing 'function_to_be_used' inside the  # decorator to control its behaviour  function_to_be_used = hello_decorator(function_to_be_used) \t   function_to_be_used() \t  ###################################### \t  # importing libraries  import time  import math \t   # decorator to calculate duration taken by any function.  def calculate_time(func): \t      def inner1(*args, **kwargs): #func takes any arguments \t              # storing time before function execution          begin = time.time() \t                   func(*args, **kwargs) \t           # storing time after function execution          end = time.time()          print(\"Total time taken in : \", func.__name__, end - begin) \t       return inner1 \t      # this can be added to any function present,  # ex) calculating a factorial  @calculate_time  def factorial(num):      print(math.factorial(num)) \t   # calling the function.  factorial(10) \t  \"\"\"  3628800  Total time taken in : factorial 0.0061802864074707  \"\"\" \t \t  ######################################  #What if a func returns something \t   @hello_decorator  def sum_two_numbers(a, b):      print(\"Inside the function\")      return a + b \t   a, b = 1, 2 \t   # getting the value through return of the function  print(\"Sum =\", sum_two_numbers(a, b)) \t  \"\"\"  before Execution  Inside the function  after Execution  Sum = 3  \"\"\" \t  ######################################  #chaining decorators \t  def decor1(func):       def inner():           x = func()           return x * x       return inner  \t   def decor(func):       def inner():           x = func()           return 2 * x       return inner  \t   @decor1  @decor  def num():       return 10 \t     print(num()) #400 (from (2*10)*(20)). i.e.,  decor1(decor(num)))          ","categories": ["project"],
        "tags": ["python"],
        "url": "/project/python-review-ongoing/",
        "teaser": null
      },{
    "title": null,
    "excerpt":"     404     Page not found :(    The requested page could not be found.   ","url": "http://localhost:4000/404.html"
  },{
    "title": null,
    "excerpt":"  Hello World!           welcome to my blog.   Resume                03/28/2024 - under construction 👷‍♀️         TikTok Series: Project 1         Spotify API Exploration         LLaMA case study                           Personal Life       my life as a college sophomore                                                             (kpop) concerts                                                                              cooking 101                                                                              love traveling!                                                                                photography                             ","url": "http://localhost:4000/about/"
  },{
    "title": "Posts by Category",
    "excerpt":" ","url": "http://localhost:4000/categories/"
  },{
    "title": null,
    "excerpt":"","url": "http://localhost:4000/"
  },{
    "title": null,
    "excerpt":" ","url": "http://localhost:4000/"
  },{
    "title": null,
    "excerpt":"var idx = lunr(function () {   this.field('title')   this.field('excerpt')   this.field('categories')   this.field('tags')   this.ref('id')    this.pipeline.remove(lunr.trimmer)    for (var item in store) {     this.add({       title: store[item].title,       excerpt: store[item].excerpt,       categories: store[item].categories,       tags: store[item].tags,       id: item     })   } });  $(document).ready(function() {   $('input#search').on('keyup', function () {     var resultdiv = $('#results');     var query = $(this).val().toLowerCase();     var result =       idx.query(function (q) {         query.split(lunr.tokenizer.separator).forEach(function (term) {           q.term(term, { boost: 100 })           if(query.lastIndexOf(\" \") != query.length-1){             q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })           }           if (term != \"\"){             q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })           }         })       });     resultdiv.empty();     resultdiv.prepend(''+result.length+' Result(s) found ');     for (var item in result) {       var ref = result[item].ref;       if(store[ref].teaser){         var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+                 ''+               ''+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       else{     \t  var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       resultdiv.append(searchitem);     }   }); }); ","url": "http://localhost:4000/assets/js/lunr/lunr-en.js"
  },{
    "title": null,
    "excerpt":"step1list = new Array(); step1list[\"ΦΑΓΙΑ\"] = \"ΦΑ\"; step1list[\"ΦΑΓΙΟΥ\"] = \"ΦΑ\"; step1list[\"ΦΑΓΙΩΝ\"] = \"ΦΑ\"; step1list[\"ΣΚΑΓΙΑ\"] = \"ΣΚΑ\"; step1list[\"ΣΚΑΓΙΟΥ\"] = \"ΣΚΑ\"; step1list[\"ΣΚΑΓΙΩΝ\"] = \"ΣΚΑ\"; step1list[\"ΟΛΟΓΙΟΥ\"] = \"ΟΛΟ\"; step1list[\"ΟΛΟΓΙΑ\"] = \"ΟΛΟ\"; step1list[\"ΟΛΟΓΙΩΝ\"] = \"ΟΛΟ\"; step1list[\"ΣΟΓΙΟΥ\"] = \"ΣΟ\"; step1list[\"ΣΟΓΙΑ\"] = \"ΣΟ\"; step1list[\"ΣΟΓΙΩΝ\"] = \"ΣΟ\"; step1list[\"ΤΑΤΟΓΙΑ\"] = \"ΤΑΤΟ\"; step1list[\"ΤΑΤΟΓΙΟΥ\"] = \"ΤΑΤΟ\"; step1list[\"ΤΑΤΟΓΙΩΝ\"] = \"ΤΑΤΟ\"; step1list[\"ΚΡΕΑΣ\"] = \"ΚΡΕ\"; step1list[\"ΚΡΕΑΤΟΣ\"] = \"ΚΡΕ\"; step1list[\"ΚΡΕΑΤΑ\"] = \"ΚΡΕ\"; step1list[\"ΚΡΕΑΤΩΝ\"] = \"ΚΡΕ\"; step1list[\"ΠΕΡΑΣ\"] = \"ΠΕΡ\"; step1list[\"ΠΕΡΑΤΟΣ\"] = \"ΠΕΡ\"; step1list[\"ΠΕΡΑΤΑ\"] = \"ΠΕΡ\"; step1list[\"ΠΕΡΑΤΩΝ\"] = \"ΠΕΡ\"; step1list[\"ΤΕΡΑΣ\"] = \"ΤΕΡ\"; step1list[\"ΤΕΡΑΤΟΣ\"] = \"ΤΕΡ\"; step1list[\"ΤΕΡΑΤΑ\"] = \"ΤΕΡ\"; step1list[\"ΤΕΡΑΤΩΝ\"] = \"ΤΕΡ\"; step1list[\"ΦΩΣ\"] = \"ΦΩ\"; step1list[\"ΦΩΤΟΣ\"] = \"ΦΩ\"; step1list[\"ΦΩΤΑ\"] = \"ΦΩ\"; step1list[\"ΦΩΤΩΝ\"] = \"ΦΩ\"; step1list[\"ΚΑΘΕΣΤΩΣ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΚΑΘΕΣΤΩΤΟΣ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΚΑΘΕΣΤΩΤΑ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΚΑΘΕΣΤΩΤΩΝ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΓΕΓΟΝΟΣ\"] = \"ΓΕΓΟΝ\"; step1list[\"ΓΕΓΟΝΟΤΟΣ\"] = \"ΓΕΓΟΝ\"; step1list[\"ΓΕΓΟΝΟΤΑ\"] = \"ΓΕΓΟΝ\"; step1list[\"ΓΕΓΟΝΟΤΩΝ\"] = \"ΓΕΓΟΝ\";  v = \"[ΑΕΗΙΟΥΩ]\"; v2 = \"[ΑΕΗΙΟΩ]\"  function stemWord(w) {   var stem;   var suffix;   var firstch;   var origword = w;   test1 = new Boolean(true);    if(w.length '+result.length+' Result(s) found ');     for (var item in result) {       var ref = result[item].ref;       if(store[ref].teaser){         var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+                 ''+               ''+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       else{     \t  var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       resultdiv.append(searchitem);     }   }); }); ","url": "http://localhost:4000/assets/js/lunr/lunr-gr.js"
  },{
    "title": null,
    "excerpt":"var store = [   {%- for c in site.collections -%}     {%- if forloop.last -%}       {%- assign l = true -%}     {%- endif -%}     {%- assign docs = c.docs | where_exp:'doc','doc.search != false' -%}     {%- for doc in docs -%}       {%- if doc.header.teaser -%}         {%- capture teaser -%}{{ doc.header.teaser }}{%- endcapture -%}       {%- else -%}         {%- assign teaser = site.teaser -%}       {%- endif -%}       {         \"title\": {{ doc.title | jsonify }},         \"excerpt\":           {%- if site.search_full_content == true -%}             {{ doc.content | newline_to_br |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \"|             strip_html | strip_newlines | jsonify }},           {%- else -%}             {{ doc.content | newline_to_br |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \"|             strip_html | strip_newlines | truncatewords: 50 | jsonify }},           {%- endif -%}         \"categories\": {{ doc.categories | jsonify }},         \"tags\": {{ doc.tags | jsonify }},         \"url\": {{ doc.url | relative_url | jsonify }},         \"teaser\": {{ teaser | relative_url | jsonify }}       }{%- unless forloop.last and l -%},{%- endunless -%}     {%- endfor -%}   {%- endfor -%}{%- if site.lunr.search_within_pages -%},   {%- assign pages = site.pages | where_exp:'doc','doc.search != false' -%}   {%- for doc in pages -%}     {%- if forloop.last -%}       {%- assign l = true -%}     {%- endif -%}   {     \"title\": {{ doc.title | jsonify }},     \"excerpt\":         {%- if site.search_full_content == true -%}           {{ doc.content | newline_to_br |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \"|           strip_html | strip_newlines | jsonify }},         {%- else -%}           {{ doc.content | newline_to_br |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \"|           strip_html | strip_newlines | truncatewords: 50 | jsonify }},         {%- endif -%}       \"url\": {{ doc.url | absolute_url | jsonify }}   }{%- unless forloop.last and l -%},{%- endunless -%}   {%- endfor -%} {%- endif -%}] ","url": "http://localhost:4000/assets/js/lunr/lunr-store.js"
  },{
    "title": "Posts by Year",
    "excerpt":"","url": "http://localhost:4000/year-archive/"
  },{
    "title": null,
    "excerpt":"{% if page.xsl %}{% endif %}Jekyll{{ site.time | date_to_xmlschema }}{{ page.url | absolute_url | xml_escape }}{% assign title = site.title | default: site.name %}{% if page.collection != \"posts\" %}{% assign collection = page.collection | capitalize %}{% assign title = title | append: \" | \" | append: collection %}{% endif %}{% if page.category %}{% assign category = page.category | capitalize %}{% assign title = title | append: \" | \" | append: category %}{% endif %}{% if title %}{{ title | smartify | xml_escape }}{% endif %}{% if site.description %}{{ site.description | xml_escape }}{% endif %}{% if site.author %}{{ site.author.name | default: site.author | xml_escape }}{% if site.author.email %}{{ site.author.email | xml_escape }}{% endif %}{% if site.author.uri %}{{ site.author.uri | xml_escape }}{% endif %}{% endif %}{% if page.tags %}{% assign posts = site.tags[page.tags] %}{% else %}{% assign posts = site[page.collection] %}{% endif %}{% if page.category %}{% assign posts = posts | where: \"categories\", page.category %}{% endif %}{% unless site.show_drafts %}{% assign posts = posts | where_exp: \"post\", \"post.draft != true\" %}{% endunless %}{% assign posts = posts | sort: \"date\" | reverse %}{% assign posts_limit = site.feed.posts_limit | default: 10 %}{% for post in posts limit: posts_limit %}{% assign post_title = post.title | smartify | strip_html | normalize_whitespace | xml_escape %}{{ post_title }}{{ post.date | date_to_xmlschema }}{{ post.last_modified_at | default: post.date | date_to_xmlschema }}{{ post.id | absolute_url | xml_escape }}{% assign excerpt_only = post.feed.excerpt_only | default: site.feed.excerpt_only %}{% unless excerpt_only %}{% endunless %}{% assign post_author = post.author | default: post.authors[0] | default: site.author %}{% assign post_author = site.data.authors[post_author] | default: post_author %}{% assign post_author_email = post_author.email | default: nil %}{% assign post_author_uri = post_author.uri | default: nil %}{% assign post_author_name = post_author.name | default: post_author %}{{ post_author_name | default: \"\" | xml_escape }}{% if post_author_email %}{{ post_author_email | xml_escape }}{% endif %}{% if post_author_uri %}{{ post_author_uri | xml_escape }}{% endif %}{% if post.category %}{% elsif post.categories %}{% for category in post.categories %}{% endfor %}{% endif %}{% for tag in post.tags %}{% endfor %}{% assign post_summary = post.description | default: post.excerpt %}{% if post_summary and post_summary != empty %}{% endif %}{% assign post_image = post.image.path | default: post.image %}{% if post_image %}{% unless post_image contains \"://\" %}{% assign post_image = post_image | absolute_url %}{% endunless %}{% endif %}{% endfor %}","url": "http://localhost:4000/feed.xml"
  },{
    "title": null,
    "excerpt":"","url": "http://localhost:4000/page2/"
  },{
    "title": null,
    "excerpt":"","url": "http://localhost:4000/page3/"
  },{
    "title": null,
    "excerpt":"","url": "http://localhost:4000/page4/"
  },{
    "title": null,
    "excerpt":"","url": "http://localhost:4000/page5/"
  },{
    "title": null,
    "excerpt":" {% if page.xsl %} {% endif %} {% assign collections = site.collections | where_exp:'collection','collection.output != false' %}{% for collection in collections %}{% assign docs = collection.docs | where_exp:'doc','doc.sitemap != false' %}{% for doc in docs %} {{ doc.url | replace:'/index.html','/' | absolute_url | xml_escape }} {% if doc.last_modified_at or doc.date %}{{ doc.last_modified_at | default: doc.date | date_to_xmlschema }} {% endif %} {% endfor %}{% endfor %}{% assign pages = site.html_pages | where_exp:'doc','doc.sitemap != false' | where_exp:'doc','doc.url != \"/404.html\"' %}{% for page in pages %} {{ page.url | replace:'/index.html','/' | absolute_url | xml_escape }} {% if page.last_modified_at %}{{ page.last_modified_at | date_to_xmlschema }} {% endif %} {% endfor %}{% assign static_files = page.static_files | where_exp:'page','page.sitemap != false' | where_exp:'page','page.name != \"404.html\"' %}{% for file in static_files %} {{ file.path | replace:'/index.html','/' | absolute_url | xml_escape }} {{ file.modified_time | date_to_xmlschema }}  {% endfor %} ","url": "http://localhost:4000/sitemap.xml"
  },{
    "title": null,
    "excerpt":"Sitemap: {{ \"sitemap.xml\" | absolute_url }} ","url": "http://localhost:4000/robots.txt"
  }]
