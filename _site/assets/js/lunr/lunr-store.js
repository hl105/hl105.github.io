var store = [{
        "title": "welcome to my blog",
        "excerpt":"    under construction!   Jekyll  instructions   Installation   # instructions commands    What to do   more things to do   Reference  some links  ","categories": ["life"],
        "tags": ["Jekyll","blog_logistics"],
        "url": "/life/welcome/",
        "teaser": null
      },{
        "title": "notion to blog post md",
        "excerpt":"A few days ago, I asked myself why I‚Äôve been writing my random thoughts on my Notion page instead of this blog. The conclusion: it‚Äôs just too much work writing in markdown, creating a file with a specific format, and uploading it. So I created a quick notion to blog direct conversion pipeline!   Creating the notion-to-md.js file   adoption of  https://github.com/souvikinator/notion-to-md   const { Client } = require(\"@notionhq/client\"); const { NotionToMarkdown } = require(\"notion-to-md\"); const fs = require('fs'); // or // import {NotionToMarkdown} from \"notion-to-md\";  const notion = new Client({     auth: \"YOUR_SECRET_KEY\", });  // passing notion client to the option const n2m = new NotionToMarkdown({ notionClient: notion });  (async () =&gt; {     try{         const mdblocks = await n2m.pageToMarkdown(process.argv[2]);         const mdString = n2m.toMarkdownString(mdblocks);         console.log(mdString.parent); //how we pass stdout to shell     }catch (error) {         console.error(error);       } })();   running this file      install nvm   curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.3/install.sh | bash           load nvm        export NVM_DIR=\"$([ -z \"${XDG_CONFIG_HOME-}\" ] &amp;&amp; printf %s \"${HOME}/.nvm\" || printf %s \"${XDG_CONFIG_HOME}/nvm\")\"  [ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\"                Install Node.js version 12.18.1       (current ver throws error :  Failed to convert page to Markdown: TypeError [ERR_INVALID_ARG_TYPE]: The \"data\" argument must be of type string or an instance of Buffer, TypedArray, or DataView. Received an instance of Object)        nvm install 12.18.1  nvm use 12.18.1           node -v should show v12.18.1            Connect your notion page             Create an internal integration and get your API key       Go to the page you want to go and choose +Add Connections       Get the link of the page on the search bar and get the last part (i.e. the page id): it should be something like 7c5e1cf7e4c34a5585f829533b17d3d9           Creating a Notion ‚Üí blog post pipeline using a Shell Script   it‚Äôs still a lot of work (for a lazy person like me) to activate the environment with nvm, run that line, and move the output file into the blog post folder, so I‚Äôm going to make a shell script with all the commands.   Note that I am using a conda environment ‚Äúblog‚Äù           create notion-to-blog.sh file        #!/bin/bash \t  # Activate Conda environment  source /Users/ihoonsun/anaconda3/etc/profile.d/conda.sh  conda activate blog \t  # Prompt the user to choose a category  echo \"Select a category for the blog post:\"  select category in \"project\" \"life\"  do      case $category in          project ) output_dir=\"./_posts/projects\"; break;;          life ) output_dir=\"./_posts/life\"; break;;          * ) echo \"Invalid option. Please select a number from the list.\";;      esac  done \t  # Ask the user for the title of the blog post in a regular sentence  read -p \"Enter the title of the blog post: \" title \t  # Generate date and format the title into a filename  date_now=$(date +\"%Y-%m-%d\")  title_formatted=$(echo \"$title\" | tr '[:upper:]' '[:lower:]' | tr -s '[:space:]' '-' | sed 's/[^a-zA-Z0-9\\-]//g')  # Avoid trailing hyphen if title is empty after formatting  filename=\"${date_now}-$(echo $title_formatted | sed 's/-$//').md\"  output_path=\"$output_dir/$filename\" \t  # Check if the file already exists  if [ ! -f \"$output_path\" ]; then      echo \"*Awesome* You are making a new post! ·É¶'·¥ó'·É¶\" \t      # Ask for new Notion page ID to update content      read -p \"Enter the Notion page ID for the new content: \" page_id \t      # Ask the user for header details      read -p \"Enter the excerpt for the blog post: \" excerpt      read -p \"Enter tags for the blog post (space-separated): \" tags      read -p \"Enter overlay image file name (should be in assets/images/banners). Enter 'default.png' to use a template: \" overlay_image      last_modified_at=$(date +\"%Y-%m-%d %H:%M:%S %z\") \t      # Run the Node.js script, get the markdown content, and add the header      node notion-to-md.js $page_id | cat &lt;(echo -e \"---\\n  title: \\\"$title\\\"  excerpt: \\\"$excerpt\\\"  date: $date_now  lastmod: $last_modified_at  last_modified_at: $last_modified_at  categories: $category  tags: $tags  classes:  toc: true  toc_label:  toc_sticky: true  header:      image:      teaser:      overlay_image: ./assets/images/banners/$overlay_image  sitemap:      changefreq: daily      priority: 1.0  author:  ---\\n  &lt;!--postNo: $date_now--&gt;\\n\") - &gt; \"$output_path\" \t      echo \"Markdown file created at $output_path\"  else      echo \"Updating the blog post...\"      # Extract the header from existing file      header=$(awk 'BEGIN {printOn=0;} /^---$/ {if (printOn) {print; exit;} else {printOn=1;}} printOn {print;}' \"$output_path\")      echo $header \t          # Ask for new Notion page ID to update content      read -p \"Enter the Notion page ID for the new content: \" page_id \t      # Get new content and replace the old content      updated_content=$(node notion-to-md.js $page_id)      echo \"$header  $updated_content\" &gt; \"$output_path\" \t      echo \"$output_path was successfully updated!\" \t     git add \"$output_path\"     git commit -m \"Updated blog post: $title\"     git push  fi #end of if block \t \t \t                make the script executable        chmod +x notion-to-blog.sh                 run the script        ./notion-to-blog.sh           Now we have a simple program where the script asks the user the info it needs:      The resulting file is automatically saved in the path like this: ./hl105.github.io/_posts/projects/2024-05-30-7c5e1cf7e4c34a5585f829533b17d3d9.md   ‚Ä¶and this is how this post was made!     ","categories": ["project"],
        "tags": ["notion","blog","markdown"],
        "url": "/project/notion-to-blog-post-md/",
        "teaser": null
      },{
        "title": "web credibility blog posts",
        "excerpt":"Are women evil? Hacking Google‚Äôs search results:   https://medium.com/@enimust/are-women-evil-hacking-googles-search-results-eebfbbffe179   ‚Üí In 2017, Google suggested evil to autocomplete the query ‚ÄúAre women __‚Äù. Why did this happen? The article that appears on the top of the search query contain the phrase many times and contain other signals that may have caused the autocomplete to happen. Thus the article suggests the use of ‚Äúnutrition labels‚Äù for search results. The users can read the labels like ‚ÄúWho was the article written by?‚Äù or ‚ÄúIs this a reliable source?‚Äù and decide on their own whether the information is credible or not. After this article was written, Google implemented a feature (the three vertical dots icon) where when clicked, the information about the website is displayed. However, let‚Äôs be truthful ‚Äî who clicks on those dots? It will be faster for the users to go to the page, quickly scan it, and get out if it is unsatisfactory. What would be an easier, faster way to let the users know the credibility of a website? This question remains to be answered.   Presidents in the Clan. SEO techniques to hack history:   https://medium.com/@enimust/presidents-in-the-klan-seo-techniques-to-hack-history-953e48dc413c   ‚Üí This example clearly shows how there is a clear path to exploiting the Google search algorithm to get better rankings on the search result page. This particular example about the presidents in the Klan ended up in the best position possible ‚Äî the featured snippet section. In high school, I wrote an article about a very niche field (cultural appropriation in Korea written in English). The next week, my article was in the Google snippet! I remember being very excited and texting a lot of friends. I still wonder what part of my article made it appear there, in bold, looking very important. Maybe it was the sources I cited, or the addition of a Korean translation, or the date it was created. Anyways, this blog post also cites some options that may have influenced the article full of fake news making it to the featured snippet. It also suggests some labels that could have prevented this from happening ‚Äî maybe add an author, or some information about the article. Again, Google later implemented such a feature with the three dots that no one really clicks.   The fake news story that fooled even Maggie Haberman:   https://medium.com/@enimust/the-fake-news-story-that-fooled-even-maggie-haberman-c22453c47169   ‚Üí How do I know whether a story that is going viral on Instagram, TikTok, etc. is fake news or not? I think about this question on a daily basis, probably because I spend a lot of time (I try to use screen time.. ) on Instagram and discover just scrolling through whatever Instagram thinks I am interested in. When there is a ‚Äúbreaking news‚Äù story that I haven‚Äôt seen in the New York Times or anywhere that is confirmed to be credible, I first check the 1. number of comments (because one out of the 300 comments must have looked it up for me, right?), 2. the date (if the news is from a year ago but claims to be breaking news, it‚Äôs not), and content (are the documents, pictures, etc). But last month, when an Instagram post told me Tom Holland and Zendaya were getting married, I thought it was real ‚Äî people were congratulating them in the comment section, the date was from yesterday, and there was a cute couple picture on the post! But I was too lazy to open Safari and triple-check, so I started spreading the news until my sister told me it was fake news. Oh well. So what feature would have stopped me from believing these two top Hollywood stars are getting married? Maybe an app that extracts text from a screenshot and tells me verified information about it? What easy verification methods are there that are easier than clicking three dots‚Ä¶      The information panels on Google and Facebook: https://medium.com/@enimust/the-information-panels-on-google-and-facebook-uncovering-their-blind-spots-2e8210b2e697   ‚Üí We all know that some links on Google search results are sketchy. We make sure to check the .gov pages when we are looking for traveler information, or .edu pages when we are looking for academic information. However, I don‚Äôt think I‚Äôve ever questioned the information on the right-side info box. It just seemed right from the start, the one-sentence summary about whatever organization or website I was looking for. This article showed me that I was wrong ‚Äî I looked up some of the celebrities with criminal records that I know, and it seems like their one-line description is just their Wikipedia page introduction.   More articles on Google search results (will read this weekend):      Are Google‚Äôs Top Stories biased? It‚Äôs complicated. https://medium.com/@enimust/are-googles-top-stories-politically-biased-it-s-complicated-e9c68e269ed9   It‚Äôs a Google drenched society, but we still suffer from information-drought: https://medium.com/@enimust/its-a-google-drenched-society-but-we-still-suffer-from-an-information-drought-4be35132b3d5   Google AI &amp; Health: https://ipscell.com/2024/05/google-ai-overviews-on-stem-cells-are-a-bust-so-far-endanger-public-health/   NYT article on Google‚Äôs AI overview feature: https://www.nytimes.com/2024/05/24/technology/google-ai-overview-search.html   ‚Üí ‚ÄúEat at least one small rock daily!‚Äù Says who? Geologists at UC Berkeley! This is what Google SGE told its users who searched ‚ÄúHow many rocks should I eat.‚Äù This result, while it may be funny, is problematic because it cites some sources with some author that seems pretty credible. However, no UC Berkeley geologist said that ‚Äî the source is The Onion, where the sources are made up for fun. But AI can‚Äôt tell whether humans should eat rocks or not. It saw a popular website with a lot of traffic, with sources and clear explanations, and decided to use it. While it is an attempt for Google to try to ‚Äúlabel‚Äù its links, it does the job poorly and leads to more confusion for its users with its new AI feature.     ","categories": ["project"],
        "tags": ["sge","Google","serp","AI"],
        "url": "/project/web-credibility-blog-posts/",
        "teaser": null
      },{
        "title": "R intro (week 0)",
        "excerpt":"  QAI Program Week 0. Notebook and lecture from the Quantitative Analysis Institute Summer Program.   Lecture - Basic R commands      Vectors:            similar to numpy array       ex) c(1,2,3,4,5) ‚Üí  1  2  3  4  5,  c(1:3,5:12) ‚Üí  1  2  3  5 10 11 12 (right inclusive)       5:10 is valid; but (5:7, 8:10) is invalid bc you need c to combine two expressions       length() to find len       NA for None       merge two vectors by c(vec1, vec2)       exp(vec1), log(vec1) ‚Üê applies to all elements in vec           Matrices:            ex) mat1 = matrix(3,nrow=2, ncol=2) #matrix with all 3s       dim(mat1), nrow(mat1), ncol(mat1)                rows and columns            ?matrix #help page \t\t  # first row, first column  mat1[1,1] \t\t  # all rows, column 1  mat1[,1] \t\t  # first row, all columns  mat1[1,] \t\t  # assign names to rows and columns  rownames(mat1) &lt;- c(\"Row1\", \"Row2\")  colnames(mat1) &lt;- c(\"Col1\", \"Col2\") \t\t  # see names of rows and columns  rownames(mat1) # \"Row1\" \"Row2\"  colnames(mat1) # \"Col1\" \"Col2\" \t\t  # view column 1  mat1[,\"Col1\"] \t\t  matrix(1:100, ncol=50)# nrow not assigned, R does it for you; 100/50 = 2 rows  # if both nrow, ncol not assigned, R makes 1 col                                matrix operations            # Element-by-Element Multiplication  mat1*mat2 \t\t  is.matrix(mat3)  is.vector(vec1)  as.matrix(vec1) # vertical matrix                           Datasets            Dataset Examination           \t  ?swiss \t  # EXAMINE THE DATASET  head(swiss) # see first few rows  tail(swiss) # see last few rows   swiss # see entire data set  dim(swiss)# number of rows and columns  ncol(swiss)   nrow(swiss)  colnames(swiss)  summary(swiss) #min/1st Quartile/mean/3rd Q/max \t \t  # R differentiates between \"matrix\" objects and \"data frame\" objects.  # When you do read.csv/read.table it'll be a dataframe  is.matrix(swiss)  is.data.frame(swiss)  swiss &lt;- as.data.frame(swiss) \t  swiss$Fertility #refer to a column (Fertility is col name)  #OR you can do this (not recommended)  attach(swiss)  Fertility  #OR you can also do:  swiss[,\"Fertility\"]  swiss[,1]           b. Dataset Summary        mean(swiss$Fertility)  var(swiss$Fertility)  summary(swiss$Agriculture) #for continuous variables  table(swiss$Education) #stem and leaf plot (for discrete variables)  table(swiss$Education, swiss$Examination) # how many times each combination appeared           c. Subsetting Dataset        FertilitySubset&lt;-swiss$Fertility[swiss$Agriculture&gt;50]  summary(swiss$Fertility[swiss$Agriculture&gt;50]) #fertitilty where agr &gt; 50  summary(swiss$Fertility[swiss$Agriculture&gt;50 &amp; swiss$Catholic&gt;50]) #AND           d. Plotting Dataset        hist(swiss$Fertility)  boxplot(swiss$Fertility)  plot(swiss$Agriculture, swiss$Fertility) \t  plot(swiss$Agriculture, swiss$Fertility,      main=\"Title Goes Here\",      xlab=\"X-axis label goes here\",      ylab=\"Y-axis label goes here\",      xlim=c(0,100),      ylim=c(0,100),      pch=21,      cex=5,      col=\"red\")                Logic        # What if we compare a vector of numbers to another number?  vec1&lt;-c(2,4,5,6)  vec1&gt;3 # R distributes the 3 for you \t  vec2&lt;-2:5  vec1==vec2 #TRUE                Conditioning        vec1[1] #1st el in vec1 (just like python list indexing)  #same as  vec1[c(TRUE, FALSE, FALSE, FALSE)]  vec1[vec1==vec2] #same logic as above                Graphics       \t  par(mfrow=c(1,2)) #mfrow = how to split window  hist(rnorm(100))  boxplot(rnorm(100))  par(las=1) # turns horizontal labels to make it easier to read \t  abline(v=10) # add a vertical line  abline(h=0, lwd=3) # add a horizontal line, and make it thick using lwd:  # add a line with intercept -3 and slope .1, change the line type via lty and the color via col  abline(-3,.1, lty=2,col=\"orange\") \t  # For plots, here's an excerpt from the RPrimer:  plot(swiss$Agriculture, swiss$Fertility,      main=\"Title Goes Here\",      xlab=\"X-axis label goes here\",      ylab=\"Y-axis label goes here\",      xlim=c(0,100),      ylim=c(0,100),      pch=1, #point character (circle dot, triangle dot, x dot, etc.)      cex=5, # default point size will be multiplied by this number      col=\"red\") \t  colors() # names of available colors \t                Packages        install.packages(\"perm\") #just once  library(perm) #import                importing a dataset        getwd()  setwd()  d=read.csv(file=\"path\",header=TRUE)           ","categories": ["R"],
        "tags": ["R","introductory","statistics"],
        "url": "/r/r-intro-week-0/",
        "teaser": null
      },{
        "title": "some paper readings",
        "excerpt":"Note: Many of the sentences or phrases in the first pass are directly copied from the original paper, as the goal of this part of the review was to extract the RQs and write down how the paper answers them (first pass). Citations are at the end of the page.   Pass 1 (title, abstract, introduction, headings, conclusion)      Assessing Google Search‚Äôs New Features in Supporting Credibility Judgments of Unknown Websites            RQ1: Are users familiar with the new Google features ‚ÄúAbout this page‚Äù and ‚ÄúMore about this page‚Äù?       RQ2: Are the 9 W3C domain credibility signals useful?       answered by: user study with 25 undergrad students           The Media Coverage of the 2020 US Presidential Election Candidates through the Lens of Google‚Äôs Top Stories            abstract:                    many news sources, but we are only exposed to a certain few through news aggregators. Google top stories is one of them. A very small number of sources dominate the section, with a highly skewed distribution.           Dataset: duration -  1 year of 30 political candidate queries, frequency: 4-12 daily observation to measure the ‚Äúfreshness‚Äù of news stories                       RQs                    RQ1: Which News Sources does the Top Stories Algorithm Prefer?                            inequality of news sources: 2,168 total news sources, but 1/3 of all articles were from only 8 news publishers                                   RQ2: Which Presidential Candidates do the News Sources Prefer?                            inequality of candidates: top mentions of candidates: Biden, Warren, Sanders, Buttigieg (Excluding Trump ‚Äî in office)                                                   The case for voter-centered audits of search engines during political elections            RQ: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate?       Four datasets:                    a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections           a dataset of biased political phrases used in a large-scale Google audit ahead of   the 2018 U.S. election           Google‚Äôs ‚Äúrelated searches‚Äù phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women)           autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019.                       Introduction:                    motivation: Trump lost the popular vote in 2016, but Google cited a conspiracy blog and claimed Trump won on the top search result page for the query ‚Äúfinal vote count 2016‚Äù           Why is auditing necessary?                            Unlike Twitter which alerted its users of false content generated by Russia‚Äôs Internet Research Agency, Google simply fixes the problem without the same transparency. How many users searched for the problematic query? How did they fix it?               some might say it‚Äôs a protective measure ‚Äî what if the hackers exploit the solution to improve their methods? ‚Üí the lack of exposure of disinformation on the web is harmful to the public ‚Äî e.g. Dylan Roof‚Äôs hate crime that started from searching ‚Äúblack on white crime‚Äù on Google. (Data void)                                   Three methods to detect political bias on search platforms:                            Third-party manipulation:                                    ‚ÄúGoogle bombing‚Äù in the early 2000s                                               Ranking Bias                                    ‚Äúsearch engine manipulation effect‚Äù, tied to ‚Äúfilter bubbles‚Äù                                               ecosystem bias                                    consider the complexity of search platforms ‚Äî users, content providers, ranking, etc.                                                                   How should we design search engine audits that are voter-centric?                            theory of ‚Äúinformation cues‚Äù: voters prefer to take shortcuts to get informed about elections.               biased searches: need to come from voters themselves               beyond Candidate Names: voters first search who are the candidates ‚Üí then modify the search so that they are more specfic               unreliable localization: ___ ‚Äúnear me‚Äù ‚Üê localized suggestions                                   conclusion: future search engine audits go beyond identifying whether their ranking algorithms are biased, but instead, take a broader ecosystem approach.                           Capturing the Aftermath of the Dobbs v. Jackson Women‚Äôs Health Organization Decision in Google Search Results across the U.S.            Dataset: more than 1.74 million Google SERPs collected in the aftermath of the Dobbs v. Jackson Women‚Äôs Health PRganization Decision. Can be used to answer questions such as:                    How do Google Search results change following an impactful real-world event, such as the U.S. Supreme Court decision on June 24, 2022 to overturn Roe v. Wade?           What do they tell us about the nature of event-driven content, generated by   various participants in the online information environment?                       Dataset Summary:                    65 locations (using Google localized search), June 24th to July 17th 2022. 1,698 search phrases.           ~ 1.7 million HTML pages,  ~20k unique URLs from  ~5k websites in organic search results,  ~17k  unique URLs from  ~2k websites in top stories.           Dataset Link                           Opening Up the Black Box: Auditing Google‚Äôs Top Stories Algorithm            Audit of the Top Stories Panel (data collection, exploration, and analysis)       Suggests Google might be addressing the ‚Äúfilter bubble‚Äù issue by selecting less known publishers for the 3rd position in the Top Stories panel.       RQ1, RQ2: A novel audit of the Google‚Äôs Top stories panel that pro-   vides insights into its algorithmic choices for selecting and ranking news publishers.                    1% of publishers (11 out of 1,125) produce 41% of total articles and are present   in 46% of observations of the Top Stories panel.           the number of sources in the 3rd position is more than double that of sources in the 1st position.                       RQ3: Evidence about the potential of using audit results from news aggregation platforms (e.g., Google) to answer questions relevant to media communication theory such as media selection bias (e.g., which publishers cover which stories)                    What events or people are publishers choosing to report on any given day?           e.g. the top sources for the query ‚Äúhilary clinton‚Äù were ‚ÄúWashington Examiner‚Äù and ‚ÄúFox News‚Äù           ‚Äúselection bias‚Äù  is indeed observed through hierarchal clustering of 65 publishers.                           All paper links:      https://dl.acm.org/doi/10.1145/3576840.3578277   https://ojs.aaai.org/index.php/ICWSM/article/view/7352   https://dl.acm.org/doi/10.1145/3351095.3372835   https://ojs.aaai.org/index.php/ICWSM/article/view/22214        https://cdn.aaai.org/ocs/18316/18316-78935-1-PB.pdf             ","categories": ["project"],
        "tags": ["audit","paper","search"],
        "url": "/project/some-paper-readings/",
        "teaser": null
      },{
        "title": "assignment 1",
        "excerpt":"               Assignment 1                                                       Assignment 1  Johanna Lee  2024-06-10       1 Quiz 1 Feedback   Q4:  The students whose parents consented will be randomized to single-sex or coed classes. What will be true of study results?  I chose: Neither of the above  Right answer: We can conclude that single-sex v. coed classroom type causes any differences in outcomes  I thought that ‚Äúoutcome‚Äù implied the target population, but it just means our sample, and since we randomized treatment, we can reach this conclusion.      Q7:  Based on those same two paragraphs, what value of the test statistic is the cutoff for rejecting the null?  I chose:6  Right answer:8  Didn‚Äôt read the article thoroughly‚Ä¶      Q10:  Suppose that you conducted a randomized experiment, assigning 50 people to treatment and 50 people to control by drawing names from a hat. After collecting outcomes, you conduct a randomization test, using the ratio of the two group medians as a test statistic. What is the simplest way to generate a reference distribution?  I chose:Approximating the reference distribution with a normal distribution with a certain mean and variance  Right answer:Randomly simulating 1000 ways to assign 100 people to two groups of 50  simplest way would be SRS. Also, there is no guarantee that the reference distribution is normal, and even if it is, not clear how we‚Äôll come up with the mean and variance because we‚Äôre not using rank sum test.       2 RailTrail Dataset  library(mosaicData) dim(RailTrail) ## [1] 90 11 #head(RailTrail,20) RailTrail[1:20,-1:-5] ##    fall cloudcover precip volume weekday dayType ## 1     0        7.6   0.00    501    TRUE weekday ## 2     0        6.3   0.29    419    TRUE weekday ## 3     0        7.5   0.32    397    TRUE weekday ## 4     0        2.6   0.00    385   FALSE weekend ## 5     0       10.0   0.14    200    TRUE weekday ## 6     0        6.6   0.02    375    TRUE weekday ## 7     0        2.4   0.00    417    TRUE weekday ## 8     0        0.0   0.00    629   FALSE weekend ## 9     0        3.8   0.00    533   FALSE weekend ## 10    0        4.1   0.00    547    TRUE weekday ## 11    0        8.5   0.00    432    TRUE weekday ## 12    0        7.2   0.00    418    TRUE weekday ## 13    0       10.0   0.03    193    TRUE weekday ## 14    1        7.7   0.00    331    TRUE weekday ## 15    1        5.8   0.00    280    TRUE weekday ## 16    0        3.6   0.00    304    TRUE weekday ## 17    0        6.1   0.68    352   FALSE weekend ## 18    0        6.3   0.00    156    TRUE weekday ## 19    0        8.6   0.00    365    TRUE weekday ## 20    1       10.0   0.15    181    TRUE weekday summary(RailTrail) ##     hightemp        lowtemp         avgtemp          spring       ##  Min.   :41.00   Min.   :19.00   Min.   :33.00   Min.   :0.0000   ##  1st Qu.:59.25   1st Qu.:38.00   1st Qu.:48.62   1st Qu.:0.0000   ##  Median :69.50   Median :44.50   Median :55.25   Median :1.0000   ##  Mean   :68.83   Mean   :46.03   Mean   :57.43   Mean   :0.5889   ##  3rd Qu.:77.75   3rd Qu.:53.75   3rd Qu.:64.50   3rd Qu.:1.0000   ##  Max.   :97.00   Max.   :72.00   Max.   :84.00   Max.   :1.0000   ##      summer            fall          cloudcover         precip        ##  Min.   :0.0000   Min.   :0.0000   Min.   : 0.000   Min.   :0.00000   ##  1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 3.650   1st Qu.:0.00000   ##  Median :0.0000   Median :0.0000   Median : 6.400   Median :0.00000   ##  Mean   :0.2778   Mean   :0.1333   Mean   : 5.807   Mean   :0.09256   ##  3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.: 8.475   3rd Qu.:0.02000   ##  Max.   :1.0000   Max.   :1.0000   Max.   :10.000   Max.   :1.49000   ##      volume       weekday          dayType          ##  Min.   :129.0   Mode :logical   Length:90          ##  1st Qu.:291.5   FALSE:28        Class :character   ##  Median :373.0   TRUE :62        Mode  :character   ##  Mean   :375.4                                      ##  3rd Qu.:451.2                                      ##  Max.   :736.0 hist(RailTrail$avgtemp, main=&quot;Frequency of Temperature&quot;, xlab=&quot;Average Temperature&quot;)   boxplot(RailTrail$volume, RailTrail$weekday, names=c(&#39;weekday&#39;,&#39;weekend&#39;),main=&quot;Number of Trail Users by Type of Day&quot;,ylab=&quot;volume (# of trail users)&quot;)    different plot 1  par(mfrow=c(1,2)) hist(RailTrail$volume[RailTrail$weekday == TRUE],main=&quot;weekday&quot;,xlab=&quot;volume&quot;) hist(RailTrail$volume[RailTrail$weekday == FALSE],main=&quot;weekends&quot;,xlab=&quot;volume&quot;) mtext(&quot;Frequency of Trail User Volume by Day Type&quot;,line=-1, side=3, outer=TRUE)     different plot 2  daycolor=ifelse(RailTrail$weekday == TRUE, &quot;red&quot;, &quot;blue&quot;) plot(RailTrail$avgtemp,RailTrail$volume, main=&quot;Effect of Temperature on Trail Volume&quot;,xlab=&quot;average temperature (F)&quot;, ylab=&quot;volume (# of trail users)&quot;, col=daycolor) legend(&quot;topleft&quot;,col=c(&quot;red&quot;,&quot;blue&quot;),legend=c(&quot;weekday&quot;,&quot;weekend&quot;), pch =1 )      3  Consider a research project that you have worked on or thought about recently. Perhaps it‚Äôs a study you read about in the news. Answer each of the questions below in a sentence or two, or guess and discuss if you are not sure.  (a) Briefly describe the project and its goals.  Project: Wellesley Cred Lab Paper I read recently ‚Äì The Media Coverage of the 2020 US Presidential Election Candidates through the Lens of Google‚Äôs Top Stories  (b) How is a ‚Äúunit‚Äù defined?  Article in the Google Top Stories panel  (c) What is the target population?  Article in the Google Top Stories panel when the query searched on Google is a US presidential election candidate name  (d) What is the sample population?  Google Top Stories panel of politicians involved in the 202 US Presidential Election  (e) What is the sample? How is the sample selected from the sample population? Briefly discuss any possible sources of selection bias.  Google Top Stories panel of 30 US presidential election candidate names. Looked through sports betting websites / articles to manually select 30 names that were rumored to run for the presdiential election. Selection bias: the data collection might have missed a few names, since out of the 60 people that the websites were guessing might run for presidency, only 30 people were selected.  (f) Is there any non-response? Briefly discuss any possible sources of non-response bias.  I guess there is no non-response in this case, since there has to be a search result page that shows up when the automated browser searches a candidate name on Google.  (g) If you are comparing multiple groups, how are units assigned to groups? Briefly discuss any possible sources of bias due to the group assignments.  Individually search the 30 names, so there are no groups.  (h) Carefully state a definition for your study‚Äôs estimand; if you‚Äôre not sure of your project‚Äôs primary goal, specify a possible estimand. What are the reasons that your estimate will differ from the estimand?  Since the RQ is ‚Äúwhich news stories are chosen to appear in Top stories and why?‚Äù, the estimand the authors are looking for is the news stories in the Top stories for all US Presidential candiates all time. But since the authors can‚Äôt infinitely search on Google, the estimate would be to just search the names in an automated browser a few times a day, for a set number of months.  (i) Can you think of any ethical concerns related to this study?  Since the study is scraping the internet, a ethical concern would be copyright ‚Äì the reporters that wrote the articles or the publisher do not know they were part of the study.    4  (a) Having signs up don‚Äôt look that significant, since 160 &gt; 140 although 100 is the smallest out of the four.  Bates Claflin StoneD Tower  100 160. 140. 180 diff b/w mean outcome (S-N)  S S N N 130 - 160 = -30  S N N S 140 - 150 = -10  N N S S 160 - 130 = 30  N S N S 170 - 120 = 50  N S S N 150 - 140 = 10  S N S N 120 - 170 = -50  one sided p-value: 2/6 = 1/3  We do not have enough evidence to rule out the possibility that signs have no effect on energy usage with a one sided left p-value of 1/3.  (b)  library(perm) sign = c(100,160) nosign = c(140,180) permTS(sign,nosign,alternative=&quot;less&quot;) ##  ##  Exact Permutation Test (network algorithm) ##  ## data:  sign and nosign ## p-value = 0.3333 ## alternative hypothesis: true mean sign - mean nosign is less than 0 ## sample estimates: ## mean sign - mean nosign  ##                     -30 other p-values  permTS(sign,nosign,alternative=&quot;greater&quot;) ##  ##  Exact Permutation Test (network algorithm) ##  ## data:  sign and nosign ## p-value = 0.8333 ## alternative hypothesis: true mean sign - mean nosign is greater than 0 ## sample estimates: ## mean sign - mean nosign  ##                     -30 permTS(sign,nosign,alternative=&quot;two.sided&quot;) ##  ##  Exact Permutation Test (network algorithm) ##  ## data:  sign and nosign ## p-value = 0.6667 ## alternative hypothesis: true mean sign - mean nosign is not equal to 0 ## sample estimates: ## mean sign - mean nosign  ##                     -30 yes, the one-sided left p-value is 0.33, which is what we manually calculated above. The two-sided p-value is indeed 2*one-sided smaller p-value.  (c) Bates Claflin StoneD Tower  1 3 2 4 rank sum  S S N N 4  S N N S 5  N N S S 6  N S N S 7  N S S N 5  S N S N 3    one sided p-value: 2/6 = 1/3    wilcox.test(sign,nosign,alternative=&quot;less&quot;) ##  ##  Wilcoxon rank sum exact test ##  ## data:  sign and nosign ## W = 1, p-value = 0.3333 ## alternative hypothesis: true location shift is less than 0                      ","categories": ["R"],
        "tags": ["qai","R","assignment"],
        "url": "/r/assignment-1/",
        "teaser": null
      },{
        "title": "Statistical Testing (week 1)",
        "excerpt":"  sampling and bias      unit:            thing we are studying       often row in dataset       ex) people, households, mic, bags of dirt           target population:            set of units you‚Äôd like to learn about       ex) all Wellesley students, all US households, ‚Ä¶           estimand:            a number that we wish we knew in order to answer the research question       the estimand could be calculated in a straightforward way if we had data of every unit in the target population           Sampling:            the link between target population and the data you have           Census:            an attempt to collect info from all units in the target population       ex) US Census       problems: resource heavy, misses certain subgroups systematically ‚Äî ex) homeless ppl       big data / data science: sometimes we can calculate the estimand because we do have all the units in target population           Sample population / sampling frame            def: set of units with some chance of being included in your dataset       ex) households with phone numbers listed       it is very possible for parts of sample population to not be in target population       goal: choose a data collection method such that sample pop is as similar as possible to target population       sample:                    set of units for which you attempt to collect data           can be also used to describe units in your data set           we have most control over choosing a sample from sample population                       respondents: set of units actually in your data set       ex) interested in surveying Wellesley students. Puts all Wellesley students‚Äô names in a hat and draws 50. But only 30 ppl responded when contacted                    people in sample: 50.           target population: all Wellesley students.           sample population: all Wellesley students.           respondents: 30 who responded                           Sampling methods            haphazard; convenience ‚Üí likely not representable       simple random sample (SRS)                    all units in sample pop are in a hat, a predetermined number of units is selected           all subsets of size n have the same probability of being the sample           SRS is default assumption for most common statistical methods                       stratified sampling: group units based on characteristics, take SRS from each group       cluster sampling: divide units into clusters, do a SRS on clusters (pick all of one cluster) ‚Üí for convenience       systematic sampling: include every kth unit ‚Üí sequential. ex) exit poll       Bernoulli Sampling: flip a coin for each unit to decide whether they are in your sample ‚Üí sequential           Bias            non-response bias: respondents are not representative of sample       selection bias: when the sample is not representative of the target population, because either -                    sample population is not representative of target           sample not rep of sample population                           Comparing two groups, and summary table            Assigning units in sample to groups:                    parallels sampling from a group           goal is to create groups that are representative of each other           haphazard or any of the random strategies we listed                       assigning groups, sample from population:                    if assigning group is random - infer causation.           if sample from group is random - easy to generalize           both random ‚Üí very rare, but ideal           not random, random ‚Üí survey           random, not random ‚Üí lab experiments           not random, not random ‚Üí most studies                           Intro to Hypothesis Testing      R.A. Fisher‚Äôs Lady tasting tea            8 cups of tea. 4 milk first, 4 tea first. Lady‚Äôs job is to pick out the 4 that was poured milk first       possibilities: 4 correct, ‚Ä¶, 0 correct           Counting:            4 correct ‚Üí 1 way       3 correct ‚Üí 4x4 = 16 ways       2 correct ‚Üí 6x6=36 ways       1 correct ‚Üí 4x4 = 16 ways       0 correct ‚Üí 1 way           Comparing truth to distribution:            p-value: assuming the lady is guessing at random, the prob she should have gotten all 4 correct is 1/70.       she did get all 4 correct! Evidence contradicts that she was guessing at random.           Hypothesis Tests:            data @ 2: reference distribution ‚Äî we can check how extreme (surprising) is the value we actually saw?       null hypothesis: assumption about target population       test statistic: something you can calculate from the sample that you actually have           Intro to Non-Parametric Tests      Defining terms:            hypothesis tests: proof by contradiction       null Hypothesis (H_0): based on the assumption, typically that there is no effect or no pattern       statistic: number that can be calculated from data       test statistic: statistic used to calculate H_0       distribution: a list of possible values of a random numeric quantity, along with their probabilities       reference distribution: distribution of the test statistic, assuming H_0 is true.           Randomization test assuming simple random sample            HCAS harvard case example: of those offered help. 76% won. Of those not offered help, 72% won.       pretend: only 3 ppl in study.                                 0       0       1 (won)                       help (T)       no help (C)       no help (C)                 C       T       C                 C       C       T                 ¬†       ¬†       ¬†                 ¬†       ¬†       ¬†           difference      diff: 0 (mean win rate for T)-0.5 (for C)=-0.5   0-0.5 = -0.5   1-0 = 1   2/3 prob that get a diff of -0.5   1/3 prob that I get a diff of 1   d. H_0: no impact of offer of help from HCAS on outcomes   e. Assume: SRS with one T and two C       Bernoulli randomization and p-values            Bernoulli randomization means that you flip a coin for each unit, rather than drawing a prespecified number of units out of a hat.       useful when units arrive one by one so you can‚Äôt randomize all at once.       Assuming: Bernoulli randomization (not SRS)                                 0       0       1       ¬†       ¬†                       T       C       C       ¬†       ¬†                 C       T       C       ¬†       ¬†                 C       C       T       ¬†       ¬†                 T       T       C       ¬†       ¬†                 T       C       T       ¬†       ¬†                 C       T       T       ¬†       ¬†                 T       T       T       ¬†       ¬†                 C       C       C       ¬†       ¬†           none of the p-vals are small ‚Äî I won‚Äôt be surprised if something happens 1/2 of the time? no. This is because it‚Äôs a tiny dataset.   differences   0-0.5=-0.5   0-0.5=-0.5   1-0=1   0-1=-1   0.5-0=0.5   0.5-0=0.5   0.33-?? = ??   ??-0.33 = ??   left side p-val: 3/6 = 1/2   right-side p-val: 5/6   one-sided: 1/2   two-sided: 1   c. p-value:  - probability of observing a value of the statistic that is at least as extreme as actually observed, if H_0 is true. - left-sided p-vaue: prob that test statistic is at least as small as actually observed, if H_0 true. - right-side p-value: prob ‚Ä¶ at least as large ‚Ä¶ - one-sided p-value: min(left, right) - two-sided p-value: 2xone-sided value - most common to report 2-sided p-value, but I should specify what p-value I‚Äôm reporting - If p-value is small, perhaps H_0 is not true. ‚Äúreject H_0‚Äù - If p-value is big, no reason to doubt the null ‚Äúfailed to reject H_0‚Äù - A common cutoff is 0.05, but not for any reason - A p-value less then 0.01 or 0.001 is equivalent to 0 ‚Äúp&lt;0.001‚Äù 4. Non-parametric test: next steps: 1. lots of assumptions 2. randomization test: take advantage of the fact that we randomized the test 3. permutation test is equivalent: apply the algo for randomization test for a situation where we did not randomize 4. benefits: useful for any sample scheme, any sample size. you can also use any test statistic. No distributional assumptions such as normality.   Q: Can we carry out the steps of a randomization test if the study was not actually randomized?   A: Yes. The steps in the test work perfectly will if the two groups were not created randomly. However, in that case we can‚Äôt justify the test by saying that each of these other randomizations could have occurred if the groups did not cause the outcome. Instead, we justify the test by saying that the group labels could have been allocated in any of these ways if the outcomes are not related to the groups.   Rank Sum Tests      Rank sum test            ex setup: suppose we compare the effectiveness of the old drug and the new drug. Suppose we measure how many months they lived after we assign the drugs. 4 ppl.       2 ppl for old drug A lived 3, 7 months each       new drug B lived 0, 12 months                    3 7 0 12           A A B B           A B A B           .. continue random allocation ‚Üí make histogram of  difference of means ‚Üí get p value                       what if instead of 12 months, it‚Äôs &gt;12? (person‚Äôs still alive)‚Ä¶what do we do?           Rank sum test details            we can‚Äôt take mean of 0 and &gt; 12. so instead, w.t. transform our data so that we can represent &gt;12 in a useful way ‚Äî convert to numbers rank in dataset.       back to example                    3 7 0 &gt;12 (assigned)           2 3 1 4 (ranks) ‚Üí T (add ranks of ppl in group A)           A A B B ‚Üí 5           A B A B  ‚Üí 3           A B B A  ‚Üí 6           B B A A ‚Üí  5           B A B A  ‚Üí 7           B A A B ‚Üí 4                       T = sum of the ranks in the smaller group. We now have reference distribution!       The randomization we actually saw (data) is the first row. ‚Üí Q: are we surprised to see a rank sum of 5 if null is true? No! bc the 5 is the middle value i expect to see           Why sum of ranks?            the test statistics we used was sum of ranks instead of diff of means.       suppose the values was 3 3 0 &gt; 12. Then you average the ranks: 2.5 2.5 1 4       If I know there‚Äôs 4 nums, we know there are 4 ranks. If I know sum is 10 (1+2+3+4), and the sum of ranks in group A is 5, I know that the sum of ranks in group B is 5. So I just have to keep track of 1 group.           When to use the rank sum            when you have censored data (very common in medical data like &gt;12)       when you have outliers: when you have 120000 instead of 12: if you take average with this, this outlier will completely drive the output. This rank sum test is resistant to outliers.       when you have small dataset bc no assumptions (e.g. normal distribution). Other methods, like t-tests, estimate the reference distribution by making assumptions.           More Non-parametric Tests      Ways to approximate the reference distribution, rather than calculating it exactly            exact method                    (what we‚Äôve been doing) listing all ways to allocate units into 2 groups. AAABB, AABAB, ‚Ä¶           problem: too many ways to allocate units                       approximate exact:                    take a SRS of the ways to allocate the units into 2 groups. Use those allocations only to produce an approximation to the exact reference distribution           most common plan                       normal approximation:                    works sometimes, only if the mean and variance of the reference distribution is known ahead of time, and we know the reference distribution would be normal.           workers for rank sum bc ‚Äúexpected value‚Äù E(T)  = n_1(N+1)/2, var(T) = n_1n_2(N+1)/12           Y: -20, -11, 5,7, ‚Ä¶, 2100, 3000           rank: 1, 2, 3, 4, 5, ‚Ä¶, 99, 100           treat: A, A, B, A, ‚Ä¶, A,B           rank depends on N, not data values           uniform reference distribution: we know the distribution (histogram) of the ranks without seeing the data ‚Üí just flat bc there‚Äôs one of each rank                           Central Limit Theorem            If you get the rank sum distributoin from the uniform rank distribution, it looks like a normal distribution.       def: regardless of population distribution, sum of random samples will be approximately normally distributed (in most circumstances)       but then why is it helpful that converting to ranks gives us a uniform distribution?       CLT says when we draw a large sample from a data set and record the sum (or mean) of the values in the sample, if we repeatedly draw different samples, the sums (or means) will look approximately normal. However, the meaning of the word ‚Äúlarge‚Äù depends on the distribution of the original data: the weirder the distribution, the larger sample size we need in order for the CLT to be true. So, the advantage of converting the data to ranks is that we know the uniform distribution is not too weird (no outliers, symmetric), and the CLT will work even for a small sample size.           Facts about rank sum            no distributional assumptions       outliers are not a problem       censoring not a problem       randomization/permutation distribution depends on the sample size, not the data itself       big or small sample size is fine       when to avoid rank sum:                    lots of ties                           ","categories": ["R"],
        "tags": ["qai","R","lecture"],
        "url": "/r/statistical-testing-week-1/",
        "teaser": null
      },{
    "title": null,
    "excerpt":"     404     Page not found :(    The requested page could not be found.   ","url": "http://localhost:4000/404.html"
  },{
    "title": null,
    "excerpt":"  Hello World!           welcome to my blog.   Resume                03/28/2024 - under construction üë∑‚Äç‚ôÄÔ∏è         TikTok Series: Project 1         Spotify API Exploration         LLaMA case study                           Personal Life       my life as a college sophomore                                                             (kpop) concerts                                                                              photography                                                                              cooking 101                                                                              love traveling!                               ","url": "http://localhost:4000/about/"
  },{
    "title": "Posts by Category",
    "excerpt":" ","url": "http://localhost:4000/categories/"
  },{
    "title": null,
    "excerpt":"","url": "http://localhost:4000/"
  },{
    "title": null,
    "excerpt":" ","url": "http://localhost:4000/"
  },{
    "title": null,
    "excerpt":"var idx = lunr(function () {   this.field('title')   this.field('excerpt')   this.field('categories')   this.field('tags')   this.ref('id')    this.pipeline.remove(lunr.trimmer)    for (var item in store) {     this.add({       title: store[item].title,       excerpt: store[item].excerpt,       categories: store[item].categories,       tags: store[item].tags,       id: item     })   } });  $(document).ready(function() {   $('input#search').on('keyup', function () {     var resultdiv = $('#results');     var query = $(this).val().toLowerCase();     var result =       idx.query(function (q) {         query.split(lunr.tokenizer.separator).forEach(function (term) {           q.term(term, { boost: 100 })           if(query.lastIndexOf(\" \") != query.length-1){             q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })           }           if (term != \"\"){             q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })           }         })       });     resultdiv.empty();     resultdiv.prepend(''+result.length+' Result(s) found ');     for (var item in result) {       var ref = result[item].ref;       if(store[ref].teaser){         var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+                 ''+               ''+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       else{     \t  var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       resultdiv.append(searchitem);     }   }); }); ","url": "http://localhost:4000/assets/js/lunr/lunr-en.js"
  },{
    "title": null,
    "excerpt":"step1list = new Array(); step1list[\"Œ¶ŒëŒìŒôŒë\"] = \"Œ¶Œë\"; step1list[\"Œ¶ŒëŒìŒôŒüŒ•\"] = \"Œ¶Œë\"; step1list[\"Œ¶ŒëŒìŒôŒ©Œù\"] = \"Œ¶Œë\"; step1list[\"Œ£ŒöŒëŒìŒôŒë\"] = \"Œ£ŒöŒë\"; step1list[\"Œ£ŒöŒëŒìŒôŒüŒ•\"] = \"Œ£ŒöŒë\"; step1list[\"Œ£ŒöŒëŒìŒôŒ©Œù\"] = \"Œ£ŒöŒë\"; step1list[\"ŒüŒõŒüŒìŒôŒüŒ•\"] = \"ŒüŒõŒü\"; step1list[\"ŒüŒõŒüŒìŒôŒë\"] = \"ŒüŒõŒü\"; step1list[\"ŒüŒõŒüŒìŒôŒ©Œù\"] = \"ŒüŒõŒü\"; step1list[\"Œ£ŒüŒìŒôŒüŒ•\"] = \"Œ£Œü\"; step1list[\"Œ£ŒüŒìŒôŒë\"] = \"Œ£Œü\"; step1list[\"Œ£ŒüŒìŒôŒ©Œù\"] = \"Œ£Œü\"; step1list[\"Œ§ŒëŒ§ŒüŒìŒôŒë\"] = \"Œ§ŒëŒ§Œü\"; step1list[\"Œ§ŒëŒ§ŒüŒìŒôŒüŒ•\"] = \"Œ§ŒëŒ§Œü\"; step1list[\"Œ§ŒëŒ§ŒüŒìŒôŒ©Œù\"] = \"Œ§ŒëŒ§Œü\"; step1list[\"ŒöŒ°ŒïŒëŒ£\"] = \"ŒöŒ°Œï\"; step1list[\"ŒöŒ°ŒïŒëŒ§ŒüŒ£\"] = \"ŒöŒ°Œï\"; step1list[\"ŒöŒ°ŒïŒëŒ§Œë\"] = \"ŒöŒ°Œï\"; step1list[\"ŒöŒ°ŒïŒëŒ§Œ©Œù\"] = \"ŒöŒ°Œï\"; step1list[\"Œ†ŒïŒ°ŒëŒ£\"] = \"Œ†ŒïŒ°\"; step1list[\"Œ†ŒïŒ°ŒëŒ§ŒüŒ£\"] = \"Œ†ŒïŒ°\"; step1list[\"Œ†ŒïŒ°ŒëŒ§Œë\"] = \"Œ†ŒïŒ°\"; step1list[\"Œ†ŒïŒ°ŒëŒ§Œ©Œù\"] = \"Œ†ŒïŒ°\"; step1list[\"Œ§ŒïŒ°ŒëŒ£\"] = \"Œ§ŒïŒ°\"; step1list[\"Œ§ŒïŒ°ŒëŒ§ŒüŒ£\"] = \"Œ§ŒïŒ°\"; step1list[\"Œ§ŒïŒ°ŒëŒ§Œë\"] = \"Œ§ŒïŒ°\"; step1list[\"Œ§ŒïŒ°ŒëŒ§Œ©Œù\"] = \"Œ§ŒïŒ°\"; step1list[\"Œ¶Œ©Œ£\"] = \"Œ¶Œ©\"; step1list[\"Œ¶Œ©Œ§ŒüŒ£\"] = \"Œ¶Œ©\"; step1list[\"Œ¶Œ©Œ§Œë\"] = \"Œ¶Œ©\"; step1list[\"Œ¶Œ©Œ§Œ©Œù\"] = \"Œ¶Œ©\"; step1list[\"ŒöŒëŒòŒïŒ£Œ§Œ©Œ£\"] = \"ŒöŒëŒòŒïŒ£Œ§\"; step1list[\"ŒöŒëŒòŒïŒ£Œ§Œ©Œ§ŒüŒ£\"] = \"ŒöŒëŒòŒïŒ£Œ§\"; step1list[\"ŒöŒëŒòŒïŒ£Œ§Œ©Œ§Œë\"] = \"ŒöŒëŒòŒïŒ£Œ§\"; step1list[\"ŒöŒëŒòŒïŒ£Œ§Œ©Œ§Œ©Œù\"] = \"ŒöŒëŒòŒïŒ£Œ§\"; step1list[\"ŒìŒïŒìŒüŒùŒüŒ£\"] = \"ŒìŒïŒìŒüŒù\"; step1list[\"ŒìŒïŒìŒüŒùŒüŒ§ŒüŒ£\"] = \"ŒìŒïŒìŒüŒù\"; step1list[\"ŒìŒïŒìŒüŒùŒüŒ§Œë\"] = \"ŒìŒïŒìŒüŒù\"; step1list[\"ŒìŒïŒìŒüŒùŒüŒ§Œ©Œù\"] = \"ŒìŒïŒìŒüŒù\";  v = \"[ŒëŒïŒóŒôŒüŒ•Œ©]\"; v2 = \"[ŒëŒïŒóŒôŒüŒ©]\"  function stemWord(w) {   var stem;   var suffix;   var firstch;   var origword = w;   test1 = new Boolean(true);    if(w.length '+result.length+' Result(s) found ');     for (var item in result) {       var ref = result[item].ref;       if(store[ref].teaser){         var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+                 ''+               ''+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       else{     \t  var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       resultdiv.append(searchitem);     }   }); }); ","url": "http://localhost:4000/assets/js/lunr/lunr-gr.js"
  },{
    "title": null,
    "excerpt":"var store = [   {%- for c in site.collections -%}     {%- if forloop.last -%}       {%- assign l = true -%}     {%- endif -%}     {%- assign docs = c.docs | where_exp:'doc','doc.search != false' -%}     {%- for doc in docs -%}       {%- if doc.header.teaser -%}         {%- capture teaser -%}{{ doc.header.teaser }}{%- endcapture -%}       {%- else -%}         {%- assign teaser = site.teaser -%}       {%- endif -%}       {         \"title\": {{ doc.title | jsonify }},         \"excerpt\":           {%- if site.search_full_content == true -%}             {{ doc.content | newline_to_br |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \"|             strip_html | strip_newlines | jsonify }},           {%- else -%}             {{ doc.content | newline_to_br |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \"|             strip_html | strip_newlines | truncatewords: 50 | jsonify }},           {%- endif -%}         \"categories\": {{ doc.categories | jsonify }},         \"tags\": {{ doc.tags | jsonify }},         \"url\": {{ doc.url | relative_url | jsonify }},         \"teaser\": {{ teaser | relative_url | jsonify }}       }{%- unless forloop.last and l -%},{%- endunless -%}     {%- endfor -%}   {%- endfor -%}{%- if site.lunr.search_within_pages -%},   {%- assign pages = site.pages | where_exp:'doc','doc.search != false' -%}   {%- for doc in pages -%}     {%- if forloop.last -%}       {%- assign l = true -%}     {%- endif -%}   {     \"title\": {{ doc.title | jsonify }},     \"excerpt\":         {%- if site.search_full_content == true -%}           {{ doc.content | newline_to_br |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \"|           strip_html | strip_newlines | jsonify }},         {%- else -%}           {{ doc.content | newline_to_br |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \"|           strip_html | strip_newlines | truncatewords: 50 | jsonify }},         {%- endif -%}       \"url\": {{ doc.url | absolute_url | jsonify }}   }{%- unless forloop.last and l -%},{%- endunless -%}   {%- endfor -%} {%- endif -%}] ","url": "http://localhost:4000/assets/js/lunr/lunr-store.js"
  },{
    "title": "Posts by Year",
    "excerpt":"","url": "http://localhost:4000/year-archive/"
  },{
    "title": null,
    "excerpt":"{% if page.xsl %}{% endif %}Jekyll{{ site.time | date_to_xmlschema }}{{ page.url | absolute_url | xml_escape }}{% assign title = site.title | default: site.name %}{% if page.collection != \"posts\" %}{% assign collection = page.collection | capitalize %}{% assign title = title | append: \" | \" | append: collection %}{% endif %}{% if page.category %}{% assign category = page.category | capitalize %}{% assign title = title | append: \" | \" | append: category %}{% endif %}{% if title %}{{ title | smartify | xml_escape }}{% endif %}{% if site.description %}{{ site.description | xml_escape }}{% endif %}{% if site.author %}{{ site.author.name | default: site.author | xml_escape }}{% if site.author.email %}{{ site.author.email | xml_escape }}{% endif %}{% if site.author.uri %}{{ site.author.uri | xml_escape }}{% endif %}{% endif %}{% if page.tags %}{% assign posts = site.tags[page.tags] %}{% else %}{% assign posts = site[page.collection] %}{% endif %}{% if page.category %}{% assign posts = posts | where: \"categories\", page.category %}{% endif %}{% unless site.show_drafts %}{% assign posts = posts | where_exp: \"post\", \"post.draft != true\" %}{% endunless %}{% assign posts = posts | sort: \"date\" | reverse %}{% assign posts_limit = site.feed.posts_limit | default: 10 %}{% for post in posts limit: posts_limit %}{% assign post_title = post.title | smartify | strip_html | normalize_whitespace | xml_escape %}{{ post_title }}{{ post.date | date_to_xmlschema }}{{ post.last_modified_at | default: post.date | date_to_xmlschema }}{{ post.id | absolute_url | xml_escape }}{% assign excerpt_only = post.feed.excerpt_only | default: site.feed.excerpt_only %}{% unless excerpt_only %}{% endunless %}{% assign post_author = post.author | default: post.authors[0] | default: site.author %}{% assign post_author = site.data.authors[post_author] | default: post_author %}{% assign post_author_email = post_author.email | default: nil %}{% assign post_author_uri = post_author.uri | default: nil %}{% assign post_author_name = post_author.name | default: post_author %}{{ post_author_name | default: \"\" | xml_escape }}{% if post_author_email %}{{ post_author_email | xml_escape }}{% endif %}{% if post_author_uri %}{{ post_author_uri | xml_escape }}{% endif %}{% if post.category %}{% elsif post.categories %}{% for category in post.categories %}{% endfor %}{% endif %}{% for tag in post.tags %}{% endfor %}{% assign post_summary = post.description | default: post.excerpt %}{% if post_summary and post_summary != empty %}{% endif %}{% assign post_image = post.image.path | default: post.image %}{% if post_image %}{% unless post_image contains \"://\" %}{% assign post_image = post_image | absolute_url %}{% endunless %}{% endif %}{% endfor %}","url": "http://localhost:4000/feed.xml"
  },{
    "title": null,
    "excerpt":"","url": "http://localhost:4000/page2/"
  },{
    "title": null,
    "excerpt":" {% if page.xsl %} {% endif %} {% assign collections = site.collections | where_exp:'collection','collection.output != false' %}{% for collection in collections %}{% assign docs = collection.docs | where_exp:'doc','doc.sitemap != false' %}{% for doc in docs %} {{ doc.url | replace:'/index.html','/' | absolute_url | xml_escape }} {% if doc.last_modified_at or doc.date %}{{ doc.last_modified_at | default: doc.date | date_to_xmlschema }} {% endif %} {% endfor %}{% endfor %}{% assign pages = site.html_pages | where_exp:'doc','doc.sitemap != false' | where_exp:'doc','doc.url != \"/404.html\"' %}{% for page in pages %} {{ page.url | replace:'/index.html','/' | absolute_url | xml_escape }} {% if page.last_modified_at %}{{ page.last_modified_at | date_to_xmlschema }} {% endif %} {% endfor %}{% assign static_files = page.static_files | where_exp:'page','page.sitemap != false' | where_exp:'page','page.name != \"404.html\"' %}{% for file in static_files %} {{ file.path | replace:'/index.html','/' | absolute_url | xml_escape }} {{ file.modified_time | date_to_xmlschema }}  {% endfor %} ","url": "http://localhost:4000/sitemap.xml"
  },{
    "title": null,
    "excerpt":"Sitemap: {{ \"sitemap.xml\" | absolute_url }} ","url": "http://localhost:4000/robots.txt"
  }]
