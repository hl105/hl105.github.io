var store = [{
        "title": "welcome to my blog",
        "excerpt":"    under construction!   Jekyll  instructions   Installation   # instructions commands    What to do   more things to do   Reference  some links  ","categories": ["life"],
        "tags": ["Jekyll","blog_logistics"],
        "url": "/life/welcome/",
        "teaser": null
      },{
        "title": "notion to blog post md",
        "excerpt":"Now why have I not been writing on this blog? It’s just too much work writing in markdown, creating a file with a specific format, and uploading it. So I created a quick notion to blog direct conversion pipeline.   Creating the notion-to-md.js file   adoption of  https://github.com/souvikinator/notion-to-md   const { Client } = require(\"@notionhq/client\"); const { NotionToMarkdown } = require(\"notion-to-md\"); const fs = require('fs'); // or // import {NotionToMarkdown} from \"notion-to-md\";  const notion = new Client({     auth: \"YOUR_SECRET_KEY\", });  // passing notion client to the option const n2m = new NotionToMarkdown({ notionClient: notion });  (async () =&gt; {     try{         const mdblocks = await n2m.pageToMarkdown(process.argv[2]);         const mdString = n2m.toMarkdownString(mdblocks);         console.log(mdString.parent); //how we pass stdout to shell     }catch (error) {         console.error(error);       } })();   running this file      install nvm   curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.3/install.sh | bash           load nvm        export NVM_DIR=\"$([ -z \"${XDG_CONFIG_HOME-}\" ] &amp;&amp; printf %s \"${HOME}/.nvm\" || printf %s \"${XDG_CONFIG_HOME}/nvm\")\"  [ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\"                Install Node.js version 12.18.1       (current ver throws error :  Failed to convert page to Markdown: TypeError [ERR_INVALID_ARG_TYPE]: The \"data\" argument must be of type string or an instance of Buffer, TypedArray, or DataView. Received an instance of Object)        nvm install 12.18.1  nvm use 12.18.1           node -v should show v12.18.1            Connect your notion page             Create an internal integration and get your API key       Go to the page you want to go and choose +Add Connections       Get the link of the page on the search bar and get the last part (i.e. the page id): it should be something like 7c5e1cf7e4c34a5585f829533b17d3d9           Creating a Notion → blog post pipeline using a Shell Script   it’s still a lot of work (for a lazy person like me) to activate the environment with nvm, run that line, and move the output file into the blog post folder, so I’m going to make a shell script with all the commands.   Note that I am using a conda environment “blog”           create notion-to-blog.sh file        #!/bin/bash \t  # Activate Conda environment  source /Users/ihoonsun/anaconda3/etc/profile.d/conda.sh  conda activate blog \t  # Prompt the user to choose a category  echo \"Select a category for the blog post:\"  select category in \"project\" \"life\"  do      case $category in          project ) output_dir=\"./_posts/projects\"; break;;          life ) output_dir=\"./_posts/life\"; break;;          * ) echo \"Invalid option. Please select a number from the list.\";;      esac  done \t  # Ask the user for the title of the blog post in a regular sentence  read -p \"Enter the title of the blog post: \" title \t  # Generate date and format the title into a filename  date_now=$(date +\"%Y-%m-%d\")  title_formatted=$(echo \"$title\" | tr '[:upper:]' '[:lower:]' | tr -s '[:space:]' '-' | sed 's/[^a-zA-Z0-9\\-]//g')  # Avoid trailing hyphen if title is empty after formatting  filename=\"${date_now}-$(echo $title_formatted | sed 's/-$//').md\"  output_path=\"$output_dir/$filename\" \t  # Check if the file already exists  if [ ! -f \"$output_path\" ]; then      echo \"*Awesome* You are making a new post! ღ'ᴗ'ღ\" \t      # Ask for new Notion page ID to update content      read -p \"Enter the Notion page ID for the new content: \" page_id \t      # Ask the user for header details      read -p \"Enter the excerpt for the blog post: \" excerpt      read -p \"Enter tags for the blog post (space-separated): \" tags      read -p \"Enter overlay image file name (should be in assets/images/banners). Enter 'default.png' to use a template: \" overlay_image      last_modified_at=$(date +\"%Y-%m-%d %H:%M:%S %z\") \t      # Run the Node.js script, get the markdown content, and add the header      node notion-to-md.js $page_id | cat &lt;(echo -e \"---\\n  title: \\\"$title\\\"  excerpt: \\\"$excerpt\\\"  date: $date_now  lastmod: $last_modified_at  last_modified_at: $last_modified_at  categories: $category  tags: $tags  classes:  toc: true  toc_label:  toc_sticky: true  header:      image:      teaser:      overlay_image: ./assets/images/banners/$overlay_image  sitemap:      changefreq: daily      priority: 1.0  author:  ---\\n  &lt;!--postNo: $date_now--&gt;\\n\") - &gt; \"$output_path\" \t      echo \"Markdown file created at $output_path\"  else      echo \"Updating the blog post...\"      # Extract the header from existing file      header=$(awk 'BEGIN {printOn=0;} /^---$/ {if (printOn) {print; exit;} else {printOn=1;}} printOn {print;}' \"$output_path\")      echo $header \t          # Ask for new Notion page ID to update content      read -p \"Enter the Notion page ID for the new content: \" page_id \t      # Get new content and replace the old content      updated_content=$(node notion-to-md.js $page_id)      echo \"$header  $updated_content\" &gt; \"$output_path\" \t      echo \"$output_path was successfully updated!\" \t     git add \"$output_path\"     git commit -m \"Updated blog post: $title\"     git push  fi #end of if block \t \t \t                make the script executable        chmod +x notion-to-blog.sh                 run the script        ./notion-to-blog.sh           Now we have a simple program where the script asks the user the info it needs.   The resulting file is automatically saved in the path like this: ./hl105.github.io/_posts/projects/2024-05-30-7c5e1cf7e4c34a5585f829533b17d3d9.md   …and this is how this post was made!     ","categories": ["project"],
        "tags": ["notion","blog","markdown"],
        "url": "/project/notion-to-blog-post-md/",
        "teaser": null
      },{
        "title": "web credibility blog posts",
        "excerpt":"Are women evil? Hacking Google’s search results:   https://medium.com/@enimust/are-women-evil-hacking-googles-search-results-eebfbbffe179   → In 2017, Google suggested evil to autocomplete the query “Are women __”. Why did this happen? The article that appears on the top of the search query contain the phrase many times and contain other signals that may have caused the autocomplete to happen. Thus the article suggests the use of “nutrition labels” for search results. The users can read the labels like “Who was the article written by?” or “Is this a reliable source?” and decide on their own whether the information is credible or not. After this article was written, Google implemented a feature (the three vertical dots icon) where when clicked, the information about the website is displayed. However, let’s be truthful — who clicks on those dots? It will be faster for the users to go to the page, quickly scan it, and get out if it is unsatisfactory. What would be an easier, faster way to let the users know the credibility of a website? This question remains to be answered.   Presidents in the Clan. SEO techniques to hack history:   https://medium.com/@enimust/presidents-in-the-klan-seo-techniques-to-hack-history-953e48dc413c   → This example clearly shows how there is a clear path to exploiting the Google search algorithm to get better rankings on the search result page. This particular example about the presidents in the Klan ended up in the best position possible — the featured snippet section. In high school, I wrote an article about a very niche field (cultural appropriation in Korea written in English). The next week, my article was in the Google snippet! I remember being very excited and texting a lot of friends. I still wonder what part of my article made it appear there, in bold, looking very important. Maybe it was the sources I cited, or the addition of a Korean translation, or the date it was created. Anyways, this blog post also cites some options that may have influenced the article full of fake news making it to the featured snippet. It also suggests some labels that could have prevented this from happening — maybe add an author, or some information about the article. Again, Google later implemented such a feature with the three dots that no one really clicks.   The fake news story that fooled even Maggie Haberman:   https://medium.com/@enimust/the-fake-news-story-that-fooled-even-maggie-haberman-c22453c47169   → How do I know whether a story that is going viral on Instagram, TikTok, etc. is fake news or not? I think about this question on a daily basis, probably because I spend a lot of time (I try to use screen time.. ) on Instagram and discover just scrolling through whatever Instagram thinks I am interested in. When there is a “breaking news” story that I haven’t seen in the New York Times or anywhere that is confirmed to be credible, I first check the 1. number of comments (because one out of the 300 comments must have looked it up for me, right?), 2. the date (if the news is from a year ago but claims to be breaking news, it’s not), and content (are the documents, pictures, etc). But last month, when an Instagram post told me Tom Holland and Zendaya were getting married, I thought it was real — people were congratulating them in the comment section, the date was from yesterday, and there was a cute couple picture on the post! But I was too lazy to open Safari and triple-check, so I started spreading the news until my sister told me it was fake news. Oh well. So what feature would have stopped me from believing these two top Hollywood stars are getting married? Maybe an app that extracts text from a screenshot and tells me verified information about it? What easy verification methods are there that are easier than clicking three dots…      The information panels on Google and Facebook: https://medium.com/@enimust/the-information-panels-on-google-and-facebook-uncovering-their-blind-spots-2e8210b2e697   → We all know that some links on Google search results are sketchy. We make sure to check the .gov pages when we are looking for traveler information, or .edu pages when we are looking for academic information. However, I don’t think I’ve ever questioned the information on the right-side info box. It just seemed right from the start, the one-sentence summary about whatever organization or website I was looking for. This article showed me that I was wrong — I looked up some of the celebrities with criminal records that I know, and it seems like their one-line description is just their Wikipedia page introduction.   More articles on Google search results (will read this weekend):      Are Google’s Top Stories biased? It’s complicated. https://medium.com/@enimust/are-googles-top-stories-politically-biased-it-s-complicated-e9c68e269ed9   It’s a Google drenched society, but we still suffer from information-drought: https://medium.com/@enimust/its-a-google-drenched-society-but-we-still-suffer-from-an-information-drought-4be35132b3d5   Google AI &amp; Health: https://ipscell.com/2024/05/google-ai-overviews-on-stem-cells-are-a-bust-so-far-endanger-public-health/   NYT article on Google’s AI overview feature: https://www.nytimes.com/2024/05/24/technology/google-ai-overview-search.html   → “Eat at least one small rock daily!” Says who? Geologists at UC Berkeley! This is what Google SGE told its users who searched “How many rocks should I eat.” This result, while it may be funny, is problematic because it cites some sources with some author that seems pretty credible. However, no UC Berkeley geologist said that — the source is The Onion, where the sources are made up for fun. But AI can’t tell whether humans should eat rocks or not. It saw a popular website with a lot of traffic, with sources and clear explanations, and decided to use it. While it is an attempt for Google to try to “label” its links, it does the job poorly and leads to more confusion for its users with its new AI feature.     ","categories": ["project"],
        "tags": ["sge","Google","serp","AI"],
        "url": "/project/web-credibility-blog-posts/",
        "teaser": null
      },{
        "title": "R intro (week 0)",
        "excerpt":"  QAI Program Week 0. Notebook and lecture from the Quantitative Analysis Institute Summer Program.   Lecture - Basic R commands      Vectors:            similar to numpy array       ex) c(1,2,3,4,5) →  1  2  3  4  5,  c(1:3,5:12) →  1  2  3  5 10 11 12 (right inclusive)       5:10 is valid; but (5:7, 8:10) is invalid bc you need c to combine two expressions       length() to find len       NA for None       merge two vectors by c(vec1, vec2)       exp(vec1), log(vec1) ← applies to all elements in vec           Matrices:            ex) mat1 = matrix(3,nrow=2, ncol=2) #matrix with all 3s       dim(mat1), nrow(mat1), ncol(mat1)                rows and columns            ?matrix #help page \t\t  # first row, first column  mat1[1,1] \t\t  # all rows, column 1  mat1[,1] \t\t  # first row, all columns  mat1[1,] \t\t  # assign names to rows and columns  rownames(mat1) &lt;- c(\"Row1\", \"Row2\")  colnames(mat1) &lt;- c(\"Col1\", \"Col2\") \t\t  # see names of rows and columns  rownames(mat1) # \"Row1\" \"Row2\"  colnames(mat1) # \"Col1\" \"Col2\" \t\t  # view column 1  mat1[,\"Col1\"] \t\t  matrix(1:100, ncol=50)# nrow not assigned, R does it for you; 100/50 = 2 rows  # if both nrow, ncol not assigned, R makes 1 col                                matrix operations            # Element-by-Element Multiplication  mat1*mat2 \t\t  is.matrix(mat3)  is.vector(vec1)  as.matrix(vec1) # vertical matrix                           Datasets            Dataset Examination           \t  ?swiss \t  # EXAMINE THE DATASET  head(swiss) # see first few rows  tail(swiss) # see last few rows   swiss # see entire data set  dim(swiss)# number of rows and columns  ncol(swiss)   nrow(swiss)  colnames(swiss)  summary(swiss) #min/1st Quartile/mean/3rd Q/max \t \t  # R differentiates between \"matrix\" objects and \"data frame\" objects.  # When you do read.csv/read.table it'll be a dataframe  is.matrix(swiss)  is.data.frame(swiss)  swiss &lt;- as.data.frame(swiss) \t  swiss$Fertility #refer to a column (Fertility is col name)  #OR you can do this (not recommended)  attach(swiss)  Fertility  #OR you can also do:  swiss[,\"Fertility\"]  swiss[,1]           b. Dataset Summary        mean(swiss$Fertility)  var(swiss$Fertility)  summary(swiss$Agriculture) #for continuous variables  table(swiss$Education) #stem and leaf plot (for discrete variables)  table(swiss$Education, swiss$Examination) # how many times each combination appeared           c. Subsetting Dataset        FertilitySubset&lt;-swiss$Fertility[swiss$Agriculture&gt;50]  summary(swiss$Fertility[swiss$Agriculture&gt;50]) #fertitilty where agr &gt; 50  summary(swiss$Fertility[swiss$Agriculture&gt;50 &amp; swiss$Catholic&gt;50]) #AND           d. Plotting Dataset        hist(swiss$Fertility)  boxplot(swiss$Fertility)  plot(swiss$Agriculture, swiss$Fertility) \t  plot(swiss$Agriculture, swiss$Fertility,      main=\"Title Goes Here\",      xlab=\"X-axis label goes here\",      ylab=\"Y-axis label goes here\",      xlim=c(0,100),      ylim=c(0,100),      pch=21,      cex=5,      col=\"red\")                Logic        # What if we compare a vector of numbers to another number?  vec1&lt;-c(2,4,5,6)  vec1&gt;3 # R distributes the 3 for you \t  vec2&lt;-2:5  vec1==vec2 #TRUE                Conditioning        vec1[1] #1st el in vec1 (just like python list indexing)  #same as  vec1[c(TRUE, FALSE, FALSE, FALSE)]  vec1[vec1==vec2] #same logic as above                Graphics       \t  par(mfrow=c(1,2)) #mfrow = how to split window  hist(rnorm(100))  boxplot(rnorm(100))  par(las=1) # turns horizontal labels to make it easier to read \t  abline(v=10) # add a vertical line  abline(h=0, lwd=3) # add a horizontal line, and make it thick using lwd:  # add a line with intercept -3 and slope .1, change the line type via lty and the color via col  abline(-3,.1, lty=2,col=\"orange\") \t  # For plots, here's an excerpt from the RPrimer:  plot(swiss$Agriculture, swiss$Fertility,      main=\"Title Goes Here\",      xlab=\"X-axis label goes here\",      ylab=\"Y-axis label goes here\",      xlim=c(0,100),      ylim=c(0,100),      pch=1, #point character (circle dot, triangle dot, x dot, etc.)      cex=5, # default point size will be multiplied by this number      col=\"red\") \t  colors() # names of available colors \t                Packages        install.packages(\"perm\") #just once  library(perm) #import                importing a dataset        getwd()  setwd()  d=read.csv(file=\"path\",header=TRUE)           ","categories": ["R"],
        "tags": ["R","introductory","statistics"],
        "url": "/r/r-intro-week-0/",
        "teaser": null
      },{
        "title": "some paper readings",
        "excerpt":"Note: Many of the sentences or phrases in the first pass are directly copied from the original paper, as the goal of this part of the review was to extract the RQs and write down how the paper answers them (first pass). Citations are at the end of the page.   Pass 1 (title, abstract, introduction, headings, conclusion)      Assessing Google Search’s New Features in Supporting Credibility Judgments of Unknown Websites            RQ1: Are users familiar with the new Google features “About this page” and “More about this page”?       RQ2: Are the 9 W3C domain credibility signals useful?       answered by: user study with 25 undergrad students           The Media Coverage of the 2020 US Presidential Election Candidates through the Lens of Google’s Top Stories            abstract:                    many news sources, but we are only exposed to a certain few through news aggregators. Google top stories is one of them. A very small number of sources dominate the section, with a highly skewed distribution.           Dataset: duration -  1 year of 30 political candidate queries, frequency: 4-12 daily observation to measure the “freshness” of news stories                       RQs                    RQ1: Which News Sources does the Top Stories Algorithm Prefer?                            inequality of news sources: 2,168 total news sources, but 1/3 of all articles were from only 8 news publishers                                   RQ2: Which Presidential Candidates do the News Sources Prefer?                            inequality of candidates: top mentions of candidates: Biden, Warren, Sanders, Buttigieg (Excluding Trump — in office)                                                   The case for voter-centered audits of search engines during political elections            RQ: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate?       Four datasets:                    a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections           a dataset of biased political phrases used in a large-scale Google audit ahead of   the 2018 U.S. election           Google’s “related searches” phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women)           autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019.                       Introduction:                    motivation: Trump lost the popular vote in 2016, but Google cited a conspiracy blog and claimed Trump won on the top search result page for the query “final vote count 2016”           Why is auditing necessary?                            Unlike Twitter which alerted its users of false content generated by Russia’s Internet Research Agency, Google simply fixes the problem without the same transparency. How many users searched for the problematic query? How did they fix it?               some might say it’s a protective measure — what if the hackers exploit the solution to improve their methods? → the lack of exposure of disinformation on the web is harmful to the public — e.g. Dylan Roof’s hate crime that started from searching “black on white crime” on Google. (Data void)                                   Three methods to detect political bias on search platforms:                            Third-party manipulation:                                    “Google bombing” in the early 2000s                                               Ranking Bias                                    “search engine manipulation effect”, tied to “filter bubbles”                                               ecosystem bias                                    consider the complexity of search platforms — users, content providers, ranking, etc.                                                                   How should we design search engine audits that are voter-centric?                            theory of “information cues”: voters prefer to take shortcuts to get informed about elections.               biased searches: need to come from voters themselves               beyond Candidate Names: voters first search who are the candidates → then modify the search so that they are more specfic               unreliable localization: ___ “near me” ← localized suggestions                                   conclusion: future search engine audits go beyond identifying whether their ranking algorithms are biased, but instead, take a broader ecosystem approach.                           Capturing the Aftermath of the Dobbs v. Jackson Women’s Health Organization Decision in Google Search Results across the U.S.            Dataset: more than 1.74 million Google SERPs collected in the aftermath of the Dobbs v. Jackson Women’s Health PRganization Decision. Can be used to answer questions such as:                    How do Google Search results change following an impactful real-world event, such as the U.S. Supreme Court decision on June 24, 2022 to overturn Roe v. Wade?           What do they tell us about the nature of event-driven content, generated by   various participants in the online information environment?                       Dataset Summary:                    65 locations (using Google localized search), June 24th to July 17th 2022. 1,698 search phrases.           ~ 1.7 million HTML pages,  ~20k unique URLs from  ~5k websites in organic search results,  ~17k  unique URLs from  ~2k websites in top stories.           Dataset Link                           Opening Up the Black Box: Auditing Google’s Top Stories Algorithm            Audit of the Top Stories Panel (data collection, exploration, and analysis)       Suggests Google might be addressing the “filter bubble” issue by selecting less known publishers for the 3rd position in the Top Stories panel.       RQ1, RQ2: A novel audit of the Google’s Top stories panel that pro-   vides insights into its algorithmic choices for selecting and ranking news publishers.                    1% of publishers (11 out of 1,125) produce 41% of total articles and are present   in 46% of observations of the Top Stories panel.           the number of sources in the 3rd position is more than double that of sources in the 1st position.                       RQ3: Evidence about the potential of using audit results from news aggregation platforms (e.g., Google) to answer questions relevant to media communication theory such as media selection bias (e.g., which publishers cover which stories)                    What events or people are publishers choosing to report on any given day?           e.g. the top sources for the query “hilary clinton” were “Washington Examiner” and “Fox News”           “selection bias”  is indeed observed through hierarchal clustering of 65 publishers.                           All paper links:      https://dl.acm.org/doi/10.1145/3576840.3578277   https://ojs.aaai.org/index.php/ICWSM/article/view/7352   https://dl.acm.org/doi/10.1145/3351095.3372835   https://ojs.aaai.org/index.php/ICWSM/article/view/22214        https://cdn.aaai.org/ocs/18316/18316-78935-1-PB.pdf             ","categories": ["project"],
        "tags": ["audit","paper","search"],
        "url": "/project/some-paper-readings/",
        "teaser": null
      },{
        "title": "Statistical Testing (week 1)",
        "excerpt":"  sampling and bias      unit:            thing we are studying       often row in dataset       ex) people, households, mic, bags of dirt           target population:            set of units you’d like to learn about       ex) all Wellesley students, all US households, …           estimand:            a number that we wish we knew in order to answer the research question       the estimand could be calculated in a straightforward way if we had data of every unit in the target population           Sampling:            the link between target population and the data you have           Census:            an attempt to collect info from all units in the target population       ex) US Census       problems: resource heavy, misses certain subgroups systematically — ex) homeless ppl       big data / data science: sometimes we can calculate the estimand because we do have all the units in target population           Sample population / sampling frame            def: set of units with some chance of being included in your dataset       ex) households with phone numbers listed       it is very possible for parts of sample population to not be in target population       goal: choose a data collection method such that sample pop is as similar as possible to target population       sample:                    set of units for which you attempt to collect data           can be also used to describe units in your data set           we have most control over choosing a sample from sample population                       respondents: set of units actually in your data set       ex) interested in surveying Wellesley students. Puts all Wellesley students’ names in a hat and draws 50. But only 30 ppl responded when contacted                    people in sample: 50.           target population: all Wellesley students.           sample population: all Wellesley students.           respondents: 30 who responded                           Sampling methods            haphazard; convenience → likely not representable       simple random sample (SRS)                    all units in sample pop are in a hat, a predetermined number of units is selected           all subsets of size n have the same probability of being the sample           SRS is default assumption for most common statistical methods                       stratified sampling: group units based on characteristics, take SRS from each group       cluster sampling: divide units into clusters, do a SRS on clusters (pick all of one cluster) → for convenience       systematic sampling: include every kth unit → sequential. ex) exit poll       Bernoulli Sampling: flip a coin for each unit to decide whether they are in your sample → sequential           Bias            non-response bias: respondents are not representative of sample       selection bias: when the sample is not representative of the target population, because either -                    sample population is not representative of target           sample not rep of sample population                           Comparing two groups, and summary table            Assigning units in sample to groups:                    parallels sampling from a group           goal is to create groups that are representative of each other           haphazard or any of the random strategies we listed                       assigning groups, sample from population:                    if assigning group is random - infer causation.           if sample from group is random - easy to generalize           both random → very rare, but ideal           not random, random → survey           random, not random → lab experiments           not random, not random → most studies                           Intro to Hypothesis Testing      R.A. Fisher’s Lady tasting tea            8 cups of tea. 4 milk first, 4 tea first. Lady’s job is to pick out the 4 that was poured milk first       possibilities: 4 correct, …, 0 correct           Counting:            4 correct → 1 way       3 correct → 4x4 = 16 ways       2 correct → 6x6=36 ways       1 correct → 4x4 = 16 ways       0 correct → 1 way           Comparing truth to distribution:            p-value: assuming the lady is guessing at random, the prob she should have gotten all 4 correct is 1/70.       she did get all 4 correct! Evidence contradicts that she was guessing at random.           Hypothesis Tests:            data @ 2: reference distribution — we can check how extreme (surprising) is the value we actually saw?       null hypothesis: assumption about target population       test statistic: something you can calculate from the sample that you actually have           Intro to Non-Parametric Tests      Defining terms:            hypothesis tests: proof by contradiction       null Hypothesis (H_0): based on the assumption, typically that there is no effect or no pattern       statistic: number that can be calculated from data       test statistic: statistic used to calculate H_0       distribution: a list of possible values of a random numeric quantity, along with their probabilities       reference distribution: distribution of the test statistic, assuming H_0 is true.           Randomization test assuming simple random sample            HCAS harvard case example: of those offered help. 76% won. Of those not offered help, 72% won.       pretend: only 3 ppl in study.                                 0       0       1 (won)                       help (T)       no help (C)       no help (C)                 C       T       C                 C       C       T                                                                               difference      diff: 0 (mean win rate for T)-0.5 (for C)=-0.5   0-0.5 = -0.5   1-0 = 1   2/3 prob that get a diff of -0.5   1/3 prob that I get a diff of 1   d. H_0: no impact of offer of help from HCAS on outcomes   e. Assume: SRS with one T and two C       Bernoulli randomization and p-values            Bernoulli randomization means that you flip a coin for each unit, rather than drawing a prespecified number of units out of a hat.       useful when units arrive one by one so you can’t randomize all at once.       Assuming: Bernoulli randomization (not SRS)                                 0       0       1                                       T       C       C                                 C       T       C                                 C       C       T                                 T       T       C                                 T       C       T                                 C       T       T                                 T       T       T                                 C       C       C                           none of the p-vals are small — I won’t be surprised if something happens 1/2 of the time? no. This is because it’s a tiny dataset.   differences   0-0.5=-0.5   0-0.5=-0.5   1-0=1   0-1=-1   0.5-0=0.5   0.5-0=0.5   0.33-?? = ??   ??-0.33 = ??   left side p-val: 3/6 = 1/2   right-side p-val: 5/6   one-sided: 1/2   two-sided: 1   c. p-value:  - probability of observing a value of the statistic that is at least as extreme as actually observed, if H_0 is true. - left-sided p-vaue: prob that test statistic is at least as small as actually observed, if H_0 true. - right-side p-value: prob … at least as large … - one-sided p-value: min(left, right) - two-sided p-value: 2xone-sided value - most common to report 2-sided p-value, but I should specify what p-value I’m reporting - If p-value is small, perhaps H_0 is not true. “reject H_0” - If p-value is big, no reason to doubt the null “failed to reject H_0” - A common cutoff is 0.05, but not for any reason - A p-value less then 0.01 or 0.001 is equivalent to 0 “p&lt;0.001” 4. Non-parametric test: next steps: 1. lots of assumptions 2. randomization test: take advantage of the fact that we randomized the test 3. permutation test is equivalent: apply the algo for randomization test for a situation where we did not randomize 4. benefits: useful for any sample scheme, any sample size. you can also use any test statistic. No distributional assumptions such as normality.   Q: Can we carry out the steps of a randomization test if the study was not actually randomized?   A: Yes. The steps in the test work perfectly will if the two groups were not created randomly. However, in that case we can’t justify the test by saying that each of these other randomizations could have occurred if the groups did not cause the outcome. Instead, we justify the test by saying that the group labels could have been allocated in any of these ways if the outcomes are not related to the groups.   Rank Sum Tests      Rank sum test            ex setup: suppose we compare the effectiveness of the old drug and the new drug. Suppose we measure how many months they lived after we assign the drugs. 4 ppl.       2 ppl for old drug A lived 3, 7 months each       new drug B lived 0, 12 months                    3 7 0 12           A A B B           A B A B           .. continue random allocation → make histogram of  difference of means → get p value                       what if instead of 12 months, it’s &gt;12? (person’s still alive)…what do we do?           Rank sum test details            we can’t take mean of 0 and &gt; 12. so instead, w.t. transform our data so that we can represent &gt;12 in a useful way — convert to numbers rank in dataset.       back to example                    3 7 0 &gt;12 (assigned)           2 3 1 4 (ranks) → T (add ranks of ppl in group A)           A A B B → 5           A B A B  → 3           A B B A  → 6           B B A A →  5           B A B A  → 7           B A A B → 4                       T = sum of the ranks in the smaller group. We now have reference distribution!       The randomization we actually saw (data) is the first row. → Q: are we surprised to see a rank sum of 5 if null is true? No! bc the 5 is the middle value i expect to see           Why sum of ranks?            the test statistics we used was sum of ranks instead of diff of means.       suppose the values was 3 3 0 &gt; 12. Then you average the ranks: 2.5 2.5 1 4       If I know there’s 4 nums, we know there are 4 ranks. If I know sum is 10 (1+2+3+4), and the sum of ranks in group A is 5, I know that the sum of ranks in group B is 5. So I just have to keep track of 1 group.           When to use the rank sum            when you have censored data (very common in medical data like &gt;12)       when you have outliers: when you have 120000 instead of 12: if you take average with this, this outlier will completely drive the output. This rank sum test is resistant to outliers.       when you have small dataset bc no assumptions (e.g. normal distribution). Other methods, like t-tests, estimate the reference distribution by making assumptions.           More Non-parametric Tests      Ways to approximate the reference distribution, rather than calculating it exactly            exact method                    (what we’ve been doing) listing all ways to allocate units into 2 groups. AAABB, AABAB, …           problem: too many ways to allocate units                       approximate exact:                    take a SRS of the ways to allocate the units into 2 groups. Use those allocations only to produce an approximation to the exact reference distribution           most common plan                       normal approximation:                    works sometimes, only if the mean and variance of the reference distribution is known ahead of time, and we know the reference distribution would be normal.           workers for rank sum bc “expected value” E(T)  = n_1(N+1)/2, var(T) = n_1n_2(N+1)/12           Y: -20, -11, 5,7, …, 2100, 3000           rank: 1, 2, 3, 4, 5, …, 99, 100           treat: A, A, B, A, …, A,B           rank depends on N, not data values           uniform reference distribution: we know the distribution (histogram) of the ranks without seeing the data → just flat bc there’s one of each rank                           Central Limit Theorem            If you get the rank sum distributoin from the uniform rank distribution, it looks like a normal distribution.       def: regardless of population distribution, sum of random samples will be approximately normally distributed (in most circumstances)       but then why is it helpful that converting to ranks gives us a uniform distribution?       CLT says when we draw a large sample from a data set and record the sum (or mean) of the values in the sample, if we repeatedly draw different samples, the sums (or means) will look approximately normal. However, the meaning of the word “large” depends on the distribution of the original data: the weirder the distribution, the larger sample size we need in order for the CLT to be true. So, the advantage of converting the data to ranks is that we know the uniform distribution is not too weird (no outliers, symmetric), and the CLT will work even for a small sample size.           Facts about rank sum            no distributional assumptions       outliers are not a problem       censoring not a problem       randomization/permutation distribution depends on the sample size, not the data itself       big or small sample size is fine       when to avoid rank sum:                    lots of ties                           ","categories": ["R"],
        "tags": ["qai","R","lecture"],
        "url": "/r/statistical-testing-week-1/",
        "teaser": null
      },{
        "title": "cron tutorial",
        "excerpt":"setting up cron      create scripts folder mkdir scripts   create your direction file touch run_gtrends_politics.sh  in it        write what you want to repeat in that shell script:       if you don’t have virtual environments, you don’t need the env activation lines       example 1: if z shell &amp; conda         #!/usr/bin/env zsh \t   source ~/.zshrc #we need this   echo $SHELL   conda activate ml   cd  /Users/ihoonsun/Desktop/summer\\ projects/sci/summer2024/gtrends_politics   python example_script.py   conda deactivate           example 2: if bash shell &amp; venv:         #!/bin/bash   echo $SHELL   export PATH=\"/Users/ihoonsun/environments/naver-venv/bin:$PATH\"   source /Users/ihoonsun/environments/naver-venv/bin/activate   if [[ \"$(which python)\" == \"/Users/ihoonsun/environments/naver-venv/bin/python\" ]]; then       echo \"venv activated\"   else       echo \"failed to activate\"   fi   cd /Users/ihoonsun/Desktop/naver    /Users/ihoonsun/environments/naver-venv/bin/python3.8  -m pytest scraper.py --headless #run pytest   deactivate \t           open shell configuration file and add  environment variables if needed            ex) your openai API key             #directions   # setting API_KEY environment variable   # open shell configuration file  (~/.bashrc, ~/.bash_profile, or ~/.zshrc)   # write: export API_KEY=your_api_key_value and save   # check if it saved correctly by: echo $API_KEY           crontab -e  to open crontab.   0 7-23 * * * /Users/ihoonsun/scripts/run_pytest.sh add to crontab   you can check crontab with crontab -l   Q&amp;A section   problem: need to do conda init before you need to conda activate   solution: source ~/.bashrc  → conda init --all     problem: conda (base) shows up on terminal prompt   solution: conda config --set auto_activate_base False     problem: permissioned denied to some file   solution: chmod +x path/to/file (chmod: command to change file permissions, +x: adds executable permission to file)     problem: editor isn’t nano   solution: export EDITOR=nano (sets nano to default), for crontab→ export VISUAL=nano     problem: command not found   solution: check shebang (character sequence at the beginning of a script that specifies the path to the interpreter that should execute the script) &amp; source the z shell becaues cron jobs run in a much more limited environment compared to your interactive shell session, we need the .zshrc to be executed because it’s where we initialized conda   Extra information (thank you StackOverflow, Google, and chatGPT)      what is z shell / bash shell?            zsh: Unix shell that can be used as an interactive login shell and as a powerful command interpreter for shell scripting. Incorporates features of other shells such as bash, ksh, and tcsh, making it highly versatile and customizable.       bash: another Unix shell and command language, which is the default shell on many Linux distributions and macOS. It is an improved version of the original Bourne shell (sh)           what is Shebang #!?            a character sequence at the beginning of a script that specifies the path to the interpreter that should execute the script. Is followed by the absolute path to the interpreter.       example usage: #!/usr/bin/env zsh  ← telling system to use the zsh interpreter found by the env command in the user’s environment.                what’s that line of code we’re adding to crontab?         * * * * *  command to execute   ┬ ┬ ┬ ┬ ┬   │ │ │ │ │   │ │ │ │ │   │ │ │ │ └─── day of week (0 - 7) (Sunday is 0 or 7)   │ │ │ └───── month (1 - 12)   │ │ └─────── day of month (1 - 31)   │ └───────── hour (0 - 23)   └─────────── minute (0 - 59)          ","categories": ["project"],
        "tags": ["tutorial","scraping"],
        "url": "/project/cron-tutorial/",
        "teaser": null
      },{
        "title": "f",
        "excerpt":"  all notes and code from the QAI Program   Robustness and Power      Robustness &amp; Distribution of p-values if null is true            def: method works out even if the assumptions aren’t true       If the null hypothesis of a test is true, what’s the probability that the p-value will be less than 0.05? → 0.05 duh           p-values have a uniform distribution when the null hypothesis is true            better explanation: for a uniform distribution where the values fall between 0 and 1 - which is the case for the distribution of p-values when the null hypothesis is true - the probability of obtaining a number less than (or equal to) x is equal to x.           Error types and independence            Type 1 error: rejecting the null hypothesis when the null hypothesis is true       Type 2 error: Failing to reject the null hypothesis when the null hypothesis is false       Power: probability of rejecting the null hypothesis when a particular alternative hypothesis is true                    What happens to t-tests when assumptions are false?                             independece                            subgroups of units similar to each other (cluster effect)               units vary over time (serial effect)               units vary across space (spatial effect)               standard error calculations incorrect; more advance calculation needed → need diff method               check: think carefully about hw data was collected; sometimes graphics can help               so t-tests are not robust to the independence assumption                                   normality                            similarity of shapes               sample sizes (relative and absolute)               outliers                                   equality of variance                            sample sizes (relative and absolute)     2. a procedure is robust to an assumption if it is valid when the assumption is not met     3. a procedure is resistant if it does not change when a small part of the data changes (e.g. outliers)                                                   Normality                                                                                                                                    validity: robustness to normality assumption   t-tests fairly robust to departures from normality, especially in large samples   when the sample size are not equally, t-test more sensitive to skewedness and long-tailedness   for small samples, t-tests somewhat sensitive to markedly different skewedness in two groups   check: graphics (don’t need tests for normality!) → Q-Q plot straight?   when assumption violated: t-test usually still valid; or, use non-parametric test; or transform            Equal population variances                                                                - when sample sizes are equal, the pooled t-test is fairly robust to unequal variances- when sample sizes are unequal, the pooled t-test is typically not valid for unequal variances, the unpooled “welch” t-test  is a robust alternative- check: graphics (or, test such as Levene, but then have to make decision based on p-value cutoff) → boxplots- when assumption violated: pooled t-test, usually valid if sample sizes equal; or, unpooled t-test; or, transform             t-tests sensitive to  outliers       rank-sum test resistant alternative       source of outliers:                    measurement error           wrong population           heavy tail                            Outliers                                                                                                                                                                      small sample size                                                  small sample size → use permulation test                                               - permutation test just cares if the distributions look the same     Non-parametric tests            Why does the rank sum test lead to p-values less than 5% more than 5% of the time when the populations have equal means but different variances? → null hypothesis is false. The null hypothesis of a t-test is that the means are equal, but the null hypothesis of a non-parametric test like the rank sum is that the distributions are the same, not just the means.           Power            probability of rejecting the null hypothesis when a specific alternative hypothesis is true       in general: power is tied to a specific alternative hypothesis and thus  you need to specify an alternative hypothesis in order to calculate power → The power is the probability that your test will correctly notice when the null is not true. You are much more likely to notice if the truth is very far from the null than if the truth is close to the null. For example, in a two-sample t-test, the null hypothesis is that the difference in population means is zero. Assume for a moment that the two populations both have variance 1, and consider two samples of size 100. If the true difference is not zero but 400 billion, you will certainly get a tiny p-value and correctly reject the null, right? But if the true difference is .0000001, you will definitely not end up with a data set that allows you to rule out the possibility that the true difference is zero.       tests are more powerful when sample sizes are larger       non-parametric test are somewhat less powerful than t-tests, when normality assumptions met bc knowing normality of distribution gives you more information           Log transformations for t-tests   - Why use log transformations &amp; Interpreting a diff in mean logs on the original scale                                                    how do I interpret original value from log?            $D = mean(log(x)) - mean(log(y)) \\sim median(log(x)) - median(log(y)) = log(median(x)) - log(median(y)) = log(\\frac{med(x)}{med(y)})$       $e^{log(\\frac{med(x)}{med(y)}} = e^D$, so $\\frac{med(x)}{med(y)} = e^D$       this is how we got from log scale D back to original scale medians           Interpreting a confidence interval for the difference in mean logs, on the original scale            $P(LB &lt; mean(log(x)) - mean(log(y)) &lt; UB) = 0.95$       $P(e^{LB} &lt; \\frac{med(x)}{med(y)} &lt; e^{UB}) = 0.95$           Data Cleaning and Management   getwd() setwd(\"Documents/\")  install.packages(\"foreign\") library(foreign)   summary(brfss$BLDSUGAR) table(is.na(brfss$BLDSUGAR))   # how to add new empty column brfss$timesperyr=rep(NA, nrow(brfss))  # Set don't know and refused to NA brfss$timesperyr[brfss$BLDSUGAR==777]=NA brfss$timesperyr[brfss$BLDSUGAR==999]=NA  brfss$DKorRefused&lt;-ifelse(brfss$BLDSUGAR==777 | brfss$BLDSUGAR==999, 1, 0)  # If responded \"never\", set new variable to zero brfss$timesperyr[!is.na(brfss$BLDSUGAR) &amp; brfss$BLDSUGAR==888]=0   # If number between 401 and 499, already in terms of times per year, but have to subtract 400 brfss$timesperyr[!is.na(brfss$BLDSUGAR) &amp; \t\t brfss$BLDSUGAR&gt;400 &amp; brfss$BLDSUGAR&lt;500]= brfss$BLDSUGAR[!is.na(brfss$BLDSUGAR) &amp;  \t\t\t\t\t\tbrfss$BLDSUGAR&gt;400 &amp; brfss$BLDSUGAR&lt;500]-400 table(brfss$timesperyr)\t\t   # Types of variables: numeric, character, factor, date,... class(brfss$timesperyr) is.numeric(brfss$timesperyr) is.numeric(brfss$EmployClean) class(brfss$EmployClean) is.character(brfss$EmployClean)  brfss$EmployCleanFactor=as.factor(brfss$EmployClean) is.factor(brfss$EmployCleanFactor)  #date object brfss$newdate&lt;-as.Date(brfss$IDATE, \"%m%d%Y\")  # Multiple ways to create small data set containing only the variables you care about # If the desired variables are part of the data set: subset&lt;-brfss[,c(\"newdate\",\"timesperyr\",\"DKorRefused\",\"EmployCleanFactor\")]  # If the desired variables are not necessarily part of the data set, combine vectors as below # For example, suppose I create a variable but don't include it in the data set TimesPerYrHigh&lt;-ifelse(brfss$timesperyr&gt;2000,1,0) subset&lt;-data.frame(TimesPerYrHigh, brfss$timesperyr) # to create a matrix, rather than a data set: subset&lt;-cbind(TimesPerYrHigh, brfss$timesperyr)  #To write multiple variables to a spreadsheet, the variables need to be part of the same data frame or matrix object write.csv(subset, file=paste(filepath,\"SubsetData.csv\",sep=\"\"))   # Other R Commands  # the \"by\" command does something to a variable, in groups based on another variable by(brfss$timesperyr, brfss$EmployCleanFactor, mean)    # the \"merge\" command combines multiple data sets with a variable in common.    Factor variables are categorical variables that can be either numeric or string variables.   Missing Data   tiny = read.csv(file=\"MissingData.csv\", header=TRUE) summary(tiny) is.na(tiny) #when there are values that R doesn't recognizes as missing editedtiny=read.csv(file=\"MissingData.csv\", header=TRUE, na.strings=c(\"NaN\", \"NA\", \"N/A\", \"-999\", \"\"))    Dates   # Creates a combined birthday in Month Day Year format (combine multiple columns) combined = with(small, paste(Month.of.Birth, Day.of.Birth, Year.of.Birth)) small[\"Full.Birthday\"] = combined  # Creates a numerical birthday BirthdayNum &lt;- as.Date(small$\"Full.Birthday\", format='%B %d %Y')  # If you had the dates in a different format # %d - day of the month (number) # %m - month (number) # %b - month (3 letter abbreviation) # %B - month (full name) # %y - year (2 digit) # %Y - year (4 digit)  # Examples with different formats of the same date as.Date(\"Oct 15 14\", format='%b %d %y') as.Date(\"October 15th, 2014\", format='%B %dth, %Y') as.Date(\"10/15/14\", format='%m/%d/%y') as.Date(\"15/10/2014\", format='%d/%m/%Y') as.Date(\"10.15.14\", format='%m.%d.%y')  # Special note for months (and days) that are single digits as.Date(\"02012011\", format='%m%d%Y') as.Date(\"212011\", format='%m%d%Y') as.Date(\"2.1.2011\", format='%m.%d.%Y')  # R internally keeps track of dates as \"days since 01/01/1970\" with negative numbers for dates that occurred prior to 01/01/1970 # Can convert the output to this internal form as.numeric(as.Date(\"2.1.1970\", format='%m.%d.%Y'))  # Gets today's date in the same YYYY-MM-DD format as BirthdayNum today &lt;- Sys.Date()  # Package called lubridate is helpful when manipulating dates install.packages(\"lubridate\") library(\"lubridate\")   # Can see what day of the week it is today wday(today, label = TRUE)  # Can see what day of the week people were born on weekdayOfBirth &lt;- wday(BirthdayNum, label = TRUE)  # Create an interval between 2 dates int &lt;- interval(BirthdayNum, today)  # Converts the interval into a period (age in years, months, days, hours, minutes, and seconds) wholeAge &lt;- as.period(int)  # Obtain the years of age from the whole age ageYears &lt;- wholeAge$year  # Adds the whole age birthday to the data frame small[\"Age\"] &lt;- ageYears  # Can see the difference in the way that birthdays and ages are stored when they are plotted par(las=1) plot(BirthdayNum, ageYears, \txlab=\"Calendar Birthday\", \tylab=\"Age in Years\")   Reshaping and Manipulating Data (Tidyverse)      Introduction to Dplyr   #Tibbles are data frames in R with unique printing and subsetting defaults, which are useful when working with large datasets.  health &lt;- as_tibble(ad_health) names(health) names(health) &lt;- c(\"ISO\", \"Location\", \"Year\", \"Source\", \"Indicator\", \"Age\", \"Value\", \"Description\", \"Reference Period\", \"Notes\") summary(health)   #Dplyr is a helpful data manipulation package with five main functions: filter, select, arrange, mutate, and summarise   # 1.) Filter   # Filter is used to view a particular subset of the data   # Example:  # View the percentage of women, between the ages of 18 to 19 years old, in Albania who received antenatal care   # Original R  health[health$Age==\"18-19 yrs\" &amp; health$Location==\"Albania\" &amp; health$Indicator ==\"ANC_1T\"  |health$Age==\"18-19 yrs\" &amp; health$Location==\"Albania\" &amp; health$Indicator == \"ANC1\" | health$Age==\"18-19 yrs\" &amp; health$Location==\"Albania\" &amp; health$Indicator == \"ANC4\" | health$Age==\"18-19 yrs\" &amp; health$Location==\"Albania\" &amp; health$Indicator == \"ANC8\", ]  # Dplyr    health %&gt;% #read as \"then\" \t\tfilter(Age == \"18-19 yrs\", Location == \"Albania\", Indicator == \"ANC_1T\" | Indicator==\"ANC1\" | \tIndicator == \"ANC4\" | Indicator == \"ANC8\") # does not replace original df, assign to new var    # 2.) Select  # Select allows you to view only columns of interest in a dataset   # Example:  # View only the location, indicator, and year columns of the health dataset   # Original R  health[,c(\"Location\", \"Year\", \"Indicator\")]  # Dplyr  health %&gt;%  \t\tselect(Location, Year, Indicator)   #More functionality with \"select\" in Dplyr  # a.) Select allows you to view columns in a sequence   # Example:  # View all of the columns between Location and Value in the health dataset, except the one named Source  health %&gt;%  \t\tselect(Location:Value) %&gt;%  \t\tselect(-Source)  # b.) Select also allows you to view columns with a specific word in the title  health %&gt;%  \t\tselect(contains(\"Indicator\"))\t\t \t\t # Example: Combining Filter &amp; Select  # View only locations where the percentage of females who use contraceptives is greater than 20   # Original R  health[health$Indicator==\"CPMODHS\" &amp; health$Value &gt; 20, c(\"Location\", \"Year\", \"Indicator\", \"Value\")]  # Dplyr  health %&gt;%  \tselect(Location, Year, Indicator, Value) %&gt;%  \tfilter(Indicator == \"CPMODHS\", Value &gt; 20)      Introduction to TidyR   #TidyR is a helpful data reshaping package with four main functions: spread , gather, unite, and separate   # 1.) Spread  # Spread is a useful function for widening long data. Spread is used to convert two columns into multiple columns. The spread function is formatted as so:  # spread(data, key, value) where key is defined as the column whose values will be used as the column headings and value as the values you wish to populate the new cells.  #h_spread &lt;- health %&gt;% spread(Indicator, Value) # A very common error while using spread is: \"Each row of output must be identified by a unique combination of keys\". If this error appears, we must first group the dataset by our key as in the following example.   #Example:  # Create new columns based on each indicator variable. Our key column will be \"Indicator\" and our value column will be \"Value\".  # Firstly, create a subset of the data that only includes the columns of Location, Year, Indicator, and Value   sub_health &lt;- health %&gt;%  \t\tselect(Location, Year, Indicator, Value)  h &lt;- sub_health%&gt;%  \tgroup_by(Indicator) %&gt;%  \tmutate(grouped_id = row_number()) %&gt;%  # create new variable called grouped_id \tspread(Indicator, Value) %&gt;%  \tselect(-grouped_id)  # Next, rename the columns \t names(h)\t\t names(h) &lt;- c(\"Location\", \"Year\",\"First Ante.Vis.\", \"Blood\", \"BP\", \"Urine\", \"1 &lt; Ante.Vis.\", \"4 &lt; Ante.Vis.\", \"8 &lt; Ante.Vis.\", \"Disease.Care\", \"Diarrhoea\", \"Fever\", \"Pneumonia\", \"Contraceptive\", \"C.Section\", \"Family.Plan\", \"Institutional.Delivery\", \"Malaria.Treat\", \"Net.Children\", \"Net.Women\", \"Malaria.Test\", \"Salts\", \"Salts&amp;Zinc\", \"Post.Care.Baby\", \"Post.Care.Mom\", \"Delivery.Attendent.Inst.\", \"Delivery.Attendent\", \"Tobacco\") names(h) dim(h) print(as_tibble(h), n=30)  # 2.) Gather  # Gather is a useful function for elongating wide data.   #Example:  # Merge the columns that contain information regarding a woman seeking care for a specific disease. This includes columns Diarrhoea through Pneumonia  health_Dis &lt;- h %&gt;%  \tgroup_by(Location) %&gt;%  \tselect(Location, Diarrhoea:Pneumonia) %&gt;%  \tgather(Disease, Val, Diarrhoea:Pneumonia) %&gt;% #gather diarrhoe:pneumonia into disease, val col \tprint( n=50) \t \t  # 3.) Unite  # Unite allows you to paste together multiple columns into one   # Example:  # Paste together the columns that contain information regarding the study location, which includes columns \"ISO\" and \"Location\" in the health dataset  health_Un &lt;- health %&gt;%  \t\t\t\tunite(Loc., ISO, Location, sep = \",\") %&gt;%  \t\t\t\tprint( n=30)\t\t\t\t \t\t\t\t\t\t # 4.) Separate  # Separate allows you to turn a single character column into multiple columns   # Example:  # Revert the Loc. column from the previous example back to separate columns for ISO, ie. country/area abbreviations, and location name\t\t\t\t health_Sep &lt;- health_Un %&gt;%  \t\t\t\tseparate(Loc., c(\"ISO\", \"Location\"), sep = \",\") %&gt;%  \t\t\t\tprint( n=30)    using Dplyr with our Cleaned Dataset   # Now that we have created new variables for each indicator, we can more easily view important sections of our dataset  # 1.) Filter   #Example: # View the locations surveyed in 2015 who reported more than 10% of women being tested for malaria   h %&gt;%  \t\tfilter(Year == \"2015\", Malaria.Test &gt; 10)  # 2.) Select  # View only the location, year, and tobacco use   h%&gt;%  \tselect(Location, Year, Tobacco)  # Example: Combining Filter &amp; Select  # View only locations where the percentage of females who use contraceptives is greater than 20  \t h %&gt;%  \tselect(Location, Year, Contraceptive) %&gt;% \tfilter(Contraceptive &gt; 20)\t  # OR   h %&gt;%  \tfilter(Contraceptive &gt; 20) %&gt;%  \tselect(Location, Year, Contraceptive )\t  # 3.) Arrange   # Example: # Sort location by the percentage of women who gave birth in an institution. Sort this from lowest to highest percentage of women.    h %&gt;%  \tselect(Location, Year, Institutional.Delivery) %&gt;%  \tarrange(Institutional.Delivery)\t  # OR   h %&gt;%  \tarrange(Institutional.Delivery) %&gt;%  \tselect(Location, Year, Institutional.Delivery)  # Example:  # Sort location by the percentage of women who had a skilled attendant for her institutional delivery. Sort this from highest to lowest percentage of women.   h %&gt;%   \tselect(Location, Year, Delivery.Attendent.Inst.) %&gt;%   \tarrange(desc(Delivery.Attendent.Inst.))  # 4.) Mutate  # Mutate allows you to add new variables to a dataset   # Example:  # Create a new variable that shows a ratio of the percentage of children who used mosquito nets to the percentage of women who used mosquito nets   # Original R  h_rat &lt;- h[,] h_rat$Ratio &lt;- h$Net.Children / h$Net.Women  h_rat[,c(\"Location\", \"Net.Women\", \"Net.Children\", \"Ratio\")]   # Dplyr  h %&gt;% \tselect(Location, Net.Women, Net.Children) %&gt;%  \tmutate(Ratio = Net.Children/Net.Women) \t  # Note, the order of operations matters when using the verb mutate. Notice, the variable \"Ratio\" would not print in this scenario.  \t h %&gt;% \tmutate(Ratio = Net.Children/Net.Women) %&gt;% \tselect(Location, Net.Women, Net.Children)  \t  # 5.) Summarise   # Summarise allows you to compute a summary statistic, such as mean, for each group in a dataset   # Example:  # Compute the average percent of women surveyed in each location who use tobacco   # Original R  h_sum &lt;- h[,] head(with(h_sum, tapply(Tobacco, Location, mean, na.rm=TRUE))) head(aggregate(Tobacco ~ Location, h_sum, mean), n=4)  # Dplyr  h %&gt;%  \tgroup_by(Location) %&gt;%  \tsummarise(avg_tob = mean(Tobacco, na.rm=TRUE)) %&gt;%  \tprint(n=4)  # Note, the order of operations matters when using the verb summarise. Notice, the variable \"Location\" would not exist in the code below. In this case, we would be calculating a single mean value for the percentage of women surveyed who use tobacco.   h %&gt;%  \tsummarise(avg_tob = mean(Tobacco, na.rm=TRUE)) %&gt;%   \tgroup_by(Location)    # 6.) Group By  # Group by allows you to perform operations on data that are grouped based on a certain variable   # Example:  # Compute the average percent of women surveyed in each location who use tobacco  h %&gt;%  \tgroup_by(Location) %&gt;%  \tsummarise(avg_tob = mean(Tobacco, na.rm=TRUE)) %&gt;%  \tprint(n=4)   hnew&lt;-h%&gt;%   mutate(across(everything(), ~replace(., is.na(.),0))) tail(hnew)   CPS %&gt;%   summarise(across(c(wage, educ, exper, age), mean))   ","categories": ["R"],
        "tags": [],
        "url": "/r/f/",
        "teaser": null
      },{
        "title": "test",
        "excerpt":"  ","categories": ["R"],
        "tags": [],
        "url": "/r/test/",
        "teaser": null
      },{
    "title": null,
    "excerpt":"     404     Page not found :(    The requested page could not be found.   ","url": "http://localhost:4000/404.html"
  },{
    "title": null,
    "excerpt":"  Hello World!           welcome to my blog.   Resume                03/28/2024 - under construction 👷‍♀️         TikTok Series: Project 1         Spotify API Exploration         LLaMA case study                           Personal Life       my life as a college sophomore                                                             (kpop) concerts                                                                              photography                                                                              cooking 101                                                                              love traveling!                               ","url": "http://localhost:4000/about/"
  },{
    "title": "Posts by Category",
    "excerpt":" ","url": "http://localhost:4000/categories/"
  },{
    "title": null,
    "excerpt":"","url": "http://localhost:4000/"
  },{
    "title": null,
    "excerpt":" ","url": "http://localhost:4000/"
  },{
    "title": null,
    "excerpt":"var idx = lunr(function () {   this.field('title')   this.field('excerpt')   this.field('categories')   this.field('tags')   this.ref('id')    this.pipeline.remove(lunr.trimmer)    for (var item in store) {     this.add({       title: store[item].title,       excerpt: store[item].excerpt,       categories: store[item].categories,       tags: store[item].tags,       id: item     })   } });  $(document).ready(function() {   $('input#search').on('keyup', function () {     var resultdiv = $('#results');     var query = $(this).val().toLowerCase();     var result =       idx.query(function (q) {         query.split(lunr.tokenizer.separator).forEach(function (term) {           q.term(term, { boost: 100 })           if(query.lastIndexOf(\" \") != query.length-1){             q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })           }           if (term != \"\"){             q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })           }         })       });     resultdiv.empty();     resultdiv.prepend(''+result.length+' Result(s) found ');     for (var item in result) {       var ref = result[item].ref;       if(store[ref].teaser){         var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+                 ''+               ''+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       else{     \t  var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       resultdiv.append(searchitem);     }   }); }); ","url": "http://localhost:4000/assets/js/lunr/lunr-en.js"
  },{
    "title": null,
    "excerpt":"step1list = new Array(); step1list[\"ΦΑΓΙΑ\"] = \"ΦΑ\"; step1list[\"ΦΑΓΙΟΥ\"] = \"ΦΑ\"; step1list[\"ΦΑΓΙΩΝ\"] = \"ΦΑ\"; step1list[\"ΣΚΑΓΙΑ\"] = \"ΣΚΑ\"; step1list[\"ΣΚΑΓΙΟΥ\"] = \"ΣΚΑ\"; step1list[\"ΣΚΑΓΙΩΝ\"] = \"ΣΚΑ\"; step1list[\"ΟΛΟΓΙΟΥ\"] = \"ΟΛΟ\"; step1list[\"ΟΛΟΓΙΑ\"] = \"ΟΛΟ\"; step1list[\"ΟΛΟΓΙΩΝ\"] = \"ΟΛΟ\"; step1list[\"ΣΟΓΙΟΥ\"] = \"ΣΟ\"; step1list[\"ΣΟΓΙΑ\"] = \"ΣΟ\"; step1list[\"ΣΟΓΙΩΝ\"] = \"ΣΟ\"; step1list[\"ΤΑΤΟΓΙΑ\"] = \"ΤΑΤΟ\"; step1list[\"ΤΑΤΟΓΙΟΥ\"] = \"ΤΑΤΟ\"; step1list[\"ΤΑΤΟΓΙΩΝ\"] = \"ΤΑΤΟ\"; step1list[\"ΚΡΕΑΣ\"] = \"ΚΡΕ\"; step1list[\"ΚΡΕΑΤΟΣ\"] = \"ΚΡΕ\"; step1list[\"ΚΡΕΑΤΑ\"] = \"ΚΡΕ\"; step1list[\"ΚΡΕΑΤΩΝ\"] = \"ΚΡΕ\"; step1list[\"ΠΕΡΑΣ\"] = \"ΠΕΡ\"; step1list[\"ΠΕΡΑΤΟΣ\"] = \"ΠΕΡ\"; step1list[\"ΠΕΡΑΤΑ\"] = \"ΠΕΡ\"; step1list[\"ΠΕΡΑΤΩΝ\"] = \"ΠΕΡ\"; step1list[\"ΤΕΡΑΣ\"] = \"ΤΕΡ\"; step1list[\"ΤΕΡΑΤΟΣ\"] = \"ΤΕΡ\"; step1list[\"ΤΕΡΑΤΑ\"] = \"ΤΕΡ\"; step1list[\"ΤΕΡΑΤΩΝ\"] = \"ΤΕΡ\"; step1list[\"ΦΩΣ\"] = \"ΦΩ\"; step1list[\"ΦΩΤΟΣ\"] = \"ΦΩ\"; step1list[\"ΦΩΤΑ\"] = \"ΦΩ\"; step1list[\"ΦΩΤΩΝ\"] = \"ΦΩ\"; step1list[\"ΚΑΘΕΣΤΩΣ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΚΑΘΕΣΤΩΤΟΣ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΚΑΘΕΣΤΩΤΑ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΚΑΘΕΣΤΩΤΩΝ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΓΕΓΟΝΟΣ\"] = \"ΓΕΓΟΝ\"; step1list[\"ΓΕΓΟΝΟΤΟΣ\"] = \"ΓΕΓΟΝ\"; step1list[\"ΓΕΓΟΝΟΤΑ\"] = \"ΓΕΓΟΝ\"; step1list[\"ΓΕΓΟΝΟΤΩΝ\"] = \"ΓΕΓΟΝ\";  v = \"[ΑΕΗΙΟΥΩ]\"; v2 = \"[ΑΕΗΙΟΩ]\"  function stemWord(w) {   var stem;   var suffix;   var firstch;   var origword = w;   test1 = new Boolean(true);    if(w.length '+result.length+' Result(s) found ');     for (var item in result) {       var ref = result[item].ref;       if(store[ref].teaser){         var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+                 ''+               ''+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       else{     \t  var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       resultdiv.append(searchitem);     }   }); }); ","url": "http://localhost:4000/assets/js/lunr/lunr-gr.js"
  },{
    "title": null,
    "excerpt":"var store = [   {%- for c in site.collections -%}     {%- if forloop.last -%}       {%- assign l = true -%}     {%- endif -%}     {%- assign docs = c.docs | where_exp:'doc','doc.search != false' -%}     {%- for doc in docs -%}       {%- if doc.header.teaser -%}         {%- capture teaser -%}{{ doc.header.teaser }}{%- endcapture -%}       {%- else -%}         {%- assign teaser = site.teaser -%}       {%- endif -%}       {         \"title\": {{ doc.title | jsonify }},         \"excerpt\":           {%- if site.search_full_content == true -%}             {{ doc.content | newline_to_br |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \"|             strip_html | strip_newlines | jsonify }},           {%- else -%}             {{ doc.content | newline_to_br |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \"|             strip_html | strip_newlines | truncatewords: 50 | jsonify }},           {%- endif -%}         \"categories\": {{ doc.categories | jsonify }},         \"tags\": {{ doc.tags | jsonify }},         \"url\": {{ doc.url | relative_url | jsonify }},         \"teaser\": {{ teaser | relative_url | jsonify }}       }{%- unless forloop.last and l -%},{%- endunless -%}     {%- endfor -%}   {%- endfor -%}{%- if site.lunr.search_within_pages -%},   {%- assign pages = site.pages | where_exp:'doc','doc.search != false' -%}   {%- for doc in pages -%}     {%- if forloop.last -%}       {%- assign l = true -%}     {%- endif -%}   {     \"title\": {{ doc.title | jsonify }},     \"excerpt\":         {%- if site.search_full_content == true -%}           {{ doc.content | newline_to_br |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \"|           strip_html | strip_newlines | jsonify }},         {%- else -%}           {{ doc.content | newline_to_br |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \"|           strip_html | strip_newlines | truncatewords: 50 | jsonify }},         {%- endif -%}       \"url\": {{ doc.url | absolute_url | jsonify }}   }{%- unless forloop.last and l -%},{%- endunless -%}   {%- endfor -%} {%- endif -%}] ","url": "http://localhost:4000/assets/js/lunr/lunr-store.js"
  },{
    "title": "Posts by Year",
    "excerpt":"","url": "http://localhost:4000/year-archive/"
  },{
    "title": null,
    "excerpt":"{% if page.xsl %}{% endif %}Jekyll{{ site.time | date_to_xmlschema }}{{ page.url | absolute_url | xml_escape }}{% assign title = site.title | default: site.name %}{% if page.collection != \"posts\" %}{% assign collection = page.collection | capitalize %}{% assign title = title | append: \" | \" | append: collection %}{% endif %}{% if page.category %}{% assign category = page.category | capitalize %}{% assign title = title | append: \" | \" | append: category %}{% endif %}{% if title %}{{ title | smartify | xml_escape }}{% endif %}{% if site.description %}{{ site.description | xml_escape }}{% endif %}{% if site.author %}{{ site.author.name | default: site.author | xml_escape }}{% if site.author.email %}{{ site.author.email | xml_escape }}{% endif %}{% if site.author.uri %}{{ site.author.uri | xml_escape }}{% endif %}{% endif %}{% if page.tags %}{% assign posts = site.tags[page.tags] %}{% else %}{% assign posts = site[page.collection] %}{% endif %}{% if page.category %}{% assign posts = posts | where: \"categories\", page.category %}{% endif %}{% unless site.show_drafts %}{% assign posts = posts | where_exp: \"post\", \"post.draft != true\" %}{% endunless %}{% assign posts = posts | sort: \"date\" | reverse %}{% assign posts_limit = site.feed.posts_limit | default: 10 %}{% for post in posts limit: posts_limit %}{% assign post_title = post.title | smartify | strip_html | normalize_whitespace | xml_escape %}{{ post_title }}{{ post.date | date_to_xmlschema }}{{ post.last_modified_at | default: post.date | date_to_xmlschema }}{{ post.id | absolute_url | xml_escape }}{% assign excerpt_only = post.feed.excerpt_only | default: site.feed.excerpt_only %}{% unless excerpt_only %}{% endunless %}{% assign post_author = post.author | default: post.authors[0] | default: site.author %}{% assign post_author = site.data.authors[post_author] | default: post_author %}{% assign post_author_email = post_author.email | default: nil %}{% assign post_author_uri = post_author.uri | default: nil %}{% assign post_author_name = post_author.name | default: post_author %}{{ post_author_name | default: \"\" | xml_escape }}{% if post_author_email %}{{ post_author_email | xml_escape }}{% endif %}{% if post_author_uri %}{{ post_author_uri | xml_escape }}{% endif %}{% if post.category %}{% elsif post.categories %}{% for category in post.categories %}{% endfor %}{% endif %}{% for tag in post.tags %}{% endfor %}{% assign post_summary = post.description | default: post.excerpt %}{% if post_summary and post_summary != empty %}{% endif %}{% assign post_image = post.image.path | default: post.image %}{% if post_image %}{% unless post_image contains \"://\" %}{% assign post_image = post_image | absolute_url %}{% endunless %}{% endif %}{% endfor %}","url": "http://localhost:4000/feed.xml"
  },{
    "title": null,
    "excerpt":"","url": "http://localhost:4000/page2/"
  },{
    "title": null,
    "excerpt":" {% if page.xsl %} {% endif %} {% assign collections = site.collections | where_exp:'collection','collection.output != false' %}{% for collection in collections %}{% assign docs = collection.docs | where_exp:'doc','doc.sitemap != false' %}{% for doc in docs %} {{ doc.url | replace:'/index.html','/' | absolute_url | xml_escape }} {% if doc.last_modified_at or doc.date %}{{ doc.last_modified_at | default: doc.date | date_to_xmlschema }} {% endif %} {% endfor %}{% endfor %}{% assign pages = site.html_pages | where_exp:'doc','doc.sitemap != false' | where_exp:'doc','doc.url != \"/404.html\"' %}{% for page in pages %} {{ page.url | replace:'/index.html','/' | absolute_url | xml_escape }} {% if page.last_modified_at %}{{ page.last_modified_at | date_to_xmlschema }} {% endif %} {% endfor %}{% assign static_files = page.static_files | where_exp:'page','page.sitemap != false' | where_exp:'page','page.name != \"404.html\"' %}{% for file in static_files %} {{ file.path | replace:'/index.html','/' | absolute_url | xml_escape }} {{ file.modified_time | date_to_xmlschema }}  {% endfor %} ","url": "http://localhost:4000/sitemap.xml"
  },{
    "title": null,
    "excerpt":"Sitemap: {{ \"sitemap.xml\" | absolute_url }} ","url": "http://localhost:4000/robots.txt"
  }]
