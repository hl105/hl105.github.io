var store = [{
        "title": "welcome to my blog",
        "excerpt":"    under construction!   Jekyll  instructions   Installation   # instructions commands    What to do   more things to do   Reference  some links  ","categories": ["life"],
        "tags": ["Jekyll","blog_logistics"],
        "url": "/life/welcome/",
        "teaser": null
      },{
        "title": "notion to blog post md",
        "excerpt":"Now why have I not been writing on this blog? Itâ€™s just too much work writing in markdown, creating a file with a specific format, and uploading it. So I created a quick notion to blog direct conversion pipeline.   Creating the notion-to-md.js file   adoption of  https://github.com/souvikinator/notion-to-md   const { Client } = require(\"@notionhq/client\"); const { NotionToMarkdown } = require(\"notion-to-md\"); const fs = require('fs'); // or // import {NotionToMarkdown} from \"notion-to-md\";  const notion = new Client({     auth: \"YOUR_SECRET_KEY\", });  // passing notion client to the option const n2m = new NotionToMarkdown({ notionClient: notion });  (async () =&gt; {     try{         const mdblocks = await n2m.pageToMarkdown(process.argv[2]);         const mdString = n2m.toMarkdownString(mdblocks);         console.log(mdString.parent); //how we pass stdout to shell     }catch (error) {         console.error(error);       } })();   running this file      install nvm   curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.3/install.sh | bash           load nvm        export NVM_DIR=\"$([ -z \"${XDG_CONFIG_HOME-}\" ] &amp;&amp; printf %s \"${HOME}/.nvm\" || printf %s \"${XDG_CONFIG_HOME}/nvm\")\"  [ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\"                Install Node.js version 12.18.1       (current ver throws error :  Failed to convert page to Markdown: TypeError [ERR_INVALID_ARG_TYPE]: The \"data\" argument must be of type string or an instance of Buffer, TypedArray, or DataView. Received an instance of Object)        nvm install 12.18.1  nvm use 12.18.1           node -v should show v12.18.1            Connect your notion page             Create an internal integration and get your API key       Go to the page you want to go and choose +Add Connections       Get the link of the page on the search bar and get the last part (i.e. the page id): it should be something like 7c5e1cf7e4c34a5585f829533b17d3d9           Creating a Notion â†’ blog post pipeline using a Shell Script   itâ€™s still a lot of work (for a lazy person like me) to activate the environment with nvm, run that line, and move the output file into the blog post folder, so Iâ€™m going to make a shell script with all the commands.   Note that I am using a conda environment â€œblogâ€           create notion-to-blog.sh file        #!/bin/bash \t  # Activate Conda environment  source /Users/ihoonsun/anaconda3/etc/profile.d/conda.sh  conda activate blog \t  # Prompt the user to choose a category  echo \"Select a category for the blog post:\"  select category in \"project\" \"life\"  do      case $category in          project ) output_dir=\"./_posts/projects\"; break;;          life ) output_dir=\"./_posts/life\"; break;;          * ) echo \"Invalid option. Please select a number from the list.\";;      esac  done \t  # Ask the user for the title of the blog post in a regular sentence  read -p \"Enter the title of the blog post: \" title \t  # Generate date and format the title into a filename  date_now=$(date +\"%Y-%m-%d\")  title_formatted=$(echo \"$title\" | tr '[:upper:]' '[:lower:]' | tr -s '[:space:]' '-' | sed 's/[^a-zA-Z0-9\\-]//g')  # Avoid trailing hyphen if title is empty after formatting  filename=\"${date_now}-$(echo $title_formatted | sed 's/-$//').md\"  output_path=\"$output_dir/$filename\" \t  # Check if the file already exists  if [ ! -f \"$output_path\" ]; then      echo \"*Awesome* You are making a new post! áƒ¦'á´—'áƒ¦\" \t      # Ask for new Notion page ID to update content      read -p \"Enter the Notion page ID for the new content: \" page_id \t      # Ask the user for header details      read -p \"Enter the excerpt for the blog post: \" excerpt      read -p \"Enter tags for the blog post (space-separated): \" tags      read -p \"Enter overlay image file name (should be in assets/images/banners). Enter 'default.png' to use a template: \" overlay_image      last_modified_at=$(date +\"%Y-%m-%d %H:%M:%S %z\") \t      # Run the Node.js script, get the markdown content, and add the header      node notion-to-md.js $page_id | cat &lt;(echo -e \"---\\n  title: \\\"$title\\\"  excerpt: \\\"$excerpt\\\"  date: $date_now  lastmod: $last_modified_at  last_modified_at: $last_modified_at  categories: $category  tags: $tags  classes:  toc: true  toc_label:  toc_sticky: true  header:      image:      teaser:      overlay_image: ./assets/images/banners/$overlay_image  sitemap:      changefreq: daily      priority: 1.0  author:  ---\\n  &lt;!--postNo: $date_now--&gt;\\n\") - &gt; \"$output_path\" \t      echo \"Markdown file created at $output_path\"  else      echo \"Updating the blog post...\"      # Extract the header from existing file      header=$(awk 'BEGIN {printOn=0;} /^---$/ {if (printOn) {print; exit;} else {printOn=1;}} printOn {print;}' \"$output_path\")      echo $header \t          # Ask for new Notion page ID to update content      read -p \"Enter the Notion page ID for the new content: \" page_id \t      # Get new content and replace the old content      updated_content=$(node notion-to-md.js $page_id)      echo \"$header  $updated_content\" &gt; \"$output_path\" \t      echo \"$output_path was successfully updated!\" \t     git add \"$output_path\"     git commit -m \"Updated blog post: $title\"     git push  fi #end of if block \t \t \t                make the script executable        chmod +x notion-to-blog.sh                 run the script        ./notion-to-blog.sh           Now we have a simple program where the script asks the user the info it needs.   The resulting file is automatically saved in the path like this: ./hl105.github.io/_posts/projects/2024-05-30-7c5e1cf7e4c34a5585f829533b17d3d9.md   â€¦and this is how this post was made!     ","categories": ["project"],
        "tags": ["notion","blog","markdown"],
        "url": "/project/notion-to-blog-post-md/",
        "teaser": null
      },{
        "title": "web credibility blog posts",
        "excerpt":"Are women evil? Hacking Googleâ€™s search results:   https://medium.com/@enimust/are-women-evil-hacking-googles-search-results-eebfbbffe179   â†’ In 2017, Google suggested evil to autocomplete the query â€œAre women __â€. Why did this happen? The article that appears on the top of the search query contain the phrase many times and contain other signals that may have caused the autocomplete to happen. Thus the article suggests the use of â€œnutrition labelsâ€ for search results. The users can read the labels like â€œWho was the article written by?â€ or â€œIs this a reliable source?â€ and decide on their own whether the information is credible or not. After this article was written, Google implemented a feature (the three vertical dots icon) where when clicked, the information about the website is displayed. However, letâ€™s be truthful â€” who clicks on those dots? It will be faster for the users to go to the page, quickly scan it, and get out if it is unsatisfactory. What would be an easier, faster way to let the users know the credibility of a website? This question remains to be answered.   Presidents in the Clan. SEO techniques to hack history:   https://medium.com/@enimust/presidents-in-the-klan-seo-techniques-to-hack-history-953e48dc413c   â†’ This example clearly shows how there is a clear path to exploiting the Google search algorithm to get better rankings on the search result page. This particular example about the presidents in the Klan ended up in the best position possible â€” the featured snippet section. In high school, I wrote an article about a very niche field (cultural appropriation in Korea written in English). The next week, my article was in the Google snippet! I remember being very excited and texting a lot of friends. I still wonder what part of my article made it appear there, in bold, looking very important. Maybe it was the sources I cited, or the addition of a Korean translation, or the date it was created. Anyways, this blog post also cites some options that may have influenced the article full of fake news making it to the featured snippet. It also suggests some labels that could have prevented this from happening â€” maybe add an author, or some information about the article. Again, Google later implemented such a feature with the three dots that no one really clicks.   The fake news story that fooled even Maggie Haberman:   https://medium.com/@enimust/the-fake-news-story-that-fooled-even-maggie-haberman-c22453c47169   â†’ How do I know whether a story that is going viral on Instagram, TikTok, etc. is fake news or not? I think about this question on a daily basis, probably because I spend a lot of time (I try to use screen time.. ) on Instagram and discover just scrolling through whatever Instagram thinks I am interested in. When there is a â€œbreaking newsâ€ story that I havenâ€™t seen in the New York Times or anywhere that is confirmed to be credible, I first check the 1. number of comments (because one out of the 300 comments must have looked it up for me, right?), 2. the date (if the news is from a year ago but claims to be breaking news, itâ€™s not), and content (are the documents, pictures, etc). But last month, when an Instagram post told me Tom Holland and Zendaya were getting married, I thought it was real â€” people were congratulating them in the comment section, the date was from yesterday, and there was a cute couple picture on the post! But I was too lazy to open Safari and triple-check, so I started spreading the news until my sister told me it was fake news. Oh well. So what feature would have stopped me from believing these two top Hollywood stars are getting married? Maybe an app that extracts text from a screenshot and tells me verified information about it? What easy verification methods are there that are easier than clicking three dotsâ€¦      The information panels on Google and Facebook: https://medium.com/@enimust/the-information-panels-on-google-and-facebook-uncovering-their-blind-spots-2e8210b2e697   â†’ We all know that some links on Google search results are sketchy. We make sure to check the .gov pages when we are looking for traveler information, or .edu pages when we are looking for academic information. However, I donâ€™t think Iâ€™ve ever questioned the information on the right-side info box. It just seemed right from the start, the one-sentence summary about whatever organization or website I was looking for. This article showed me that I was wrong â€” I looked up some of the celebrities with criminal records that I know, and it seems like their one-line description is just their Wikipedia page introduction.   More articles on Google search results (will read this weekend):      Are Googleâ€™s Top Stories biased? Itâ€™s complicated. https://medium.com/@enimust/are-googles-top-stories-politically-biased-it-s-complicated-e9c68e269ed9   Itâ€™s a Google drenched society, but we still suffer from information-drought: https://medium.com/@enimust/its-a-google-drenched-society-but-we-still-suffer-from-an-information-drought-4be35132b3d5   Google AI &amp; Health: https://ipscell.com/2024/05/google-ai-overviews-on-stem-cells-are-a-bust-so-far-endanger-public-health/   NYT article on Googleâ€™s AI overview feature: https://www.nytimes.com/2024/05/24/technology/google-ai-overview-search.html   â†’ â€œEat at least one small rock daily!â€ Says who? Geologists at UC Berkeley! This is what Google SGE told its users who searched â€œHow many rocks should I eat.â€ This result, while it may be funny, is problematic because it cites some sources with some author that seems pretty credible. However, no UC Berkeley geologist said that â€” the source is The Onion, where the sources are made up for fun. But AI canâ€™t tell whether humans should eat rocks or not. It saw a popular website with a lot of traffic, with sources and clear explanations, and decided to use it. While it is an attempt for Google to try to â€œlabelâ€ its links, it does the job poorly and leads to more confusion for its users with its new AI feature.     ","categories": ["project"],
        "tags": ["sge","Google","serp","AI"],
        "url": "/project/web-credibility-blog-posts/",
        "teaser": null
      },{
        "title": "R intro (week 0)",
        "excerpt":"  QAI Program Week 0. Notebook and lecture from the Quantitative Analysis Institute Summer Program.   Lecture - Basic R commands      Vectors:            similar to numpy array       ex) c(1,2,3,4,5) â†’  1  2  3  4  5,  c(1:3,5:12) â†’  1  2  3  5 10 11 12 (right inclusive)       5:10 is valid; but (5:7, 8:10) is invalid bc you need c to combine two expressions       length() to find len       NA for None       merge two vectors by c(vec1, vec2)       exp(vec1), log(vec1) â† applies to all elements in vec           Matrices:            ex) mat1 = matrix(3,nrow=2, ncol=2) #matrix with all 3s       dim(mat1), nrow(mat1), ncol(mat1)                rows and columns            ?matrix #help page \t\t  # first row, first column  mat1[1,1] \t\t  # all rows, column 1  mat1[,1] \t\t  # first row, all columns  mat1[1,] \t\t  # assign names to rows and columns  rownames(mat1) &lt;- c(\"Row1\", \"Row2\")  colnames(mat1) &lt;- c(\"Col1\", \"Col2\") \t\t  # see names of rows and columns  rownames(mat1) # \"Row1\" \"Row2\"  colnames(mat1) # \"Col1\" \"Col2\" \t\t  # view column 1  mat1[,\"Col1\"] \t\t  matrix(1:100, ncol=50)# nrow not assigned, R does it for you; 100/50 = 2 rows  # if both nrow, ncol not assigned, R makes 1 col                                matrix operations            # Element-by-Element Multiplication  mat1*mat2 \t\t  is.matrix(mat3)  is.vector(vec1)  as.matrix(vec1) # vertical matrix                           Datasets            Dataset Examination           \t  ?swiss \t  # EXAMINE THE DATASET  head(swiss) # see first few rows  tail(swiss) # see last few rows   swiss # see entire data set  dim(swiss)# number of rows and columns  ncol(swiss)   nrow(swiss)  colnames(swiss)  summary(swiss) #min/1st Quartile/mean/3rd Q/max \t \t  # R differentiates between \"matrix\" objects and \"data frame\" objects.  # When you do read.csv/read.table it'll be a dataframe  is.matrix(swiss)  is.data.frame(swiss)  swiss &lt;- as.data.frame(swiss) \t  swiss$Fertility #refer to a column (Fertility is col name)  #OR you can do this (not recommended)  attach(swiss)  Fertility  #OR you can also do:  swiss[,\"Fertility\"]  swiss[,1]           b. Dataset Summary        mean(swiss$Fertility)  var(swiss$Fertility)  summary(swiss$Agriculture) #for continuous variables  table(swiss$Education) #stem and leaf plot (for discrete variables)  table(swiss$Education, swiss$Examination) # how many times each combination appeared           c. Subsetting Dataset        FertilitySubset&lt;-swiss$Fertility[swiss$Agriculture&gt;50]  summary(swiss$Fertility[swiss$Agriculture&gt;50]) #fertitilty where agr &gt; 50  summary(swiss$Fertility[swiss$Agriculture&gt;50 &amp; swiss$Catholic&gt;50]) #AND           d. Plotting Dataset        hist(swiss$Fertility)  boxplot(swiss$Fertility)  plot(swiss$Agriculture, swiss$Fertility) \t  plot(swiss$Agriculture, swiss$Fertility,      main=\"Title Goes Here\",      xlab=\"X-axis label goes here\",      ylab=\"Y-axis label goes here\",      xlim=c(0,100),      ylim=c(0,100),      pch=21,      cex=5,      col=\"red\")                Logic        # What if we compare a vector of numbers to another number?  vec1&lt;-c(2,4,5,6)  vec1&gt;3 # R distributes the 3 for you \t  vec2&lt;-2:5  vec1==vec2 #TRUE                Conditioning        vec1[1] #1st el in vec1 (just like python list indexing)  #same as  vec1[c(TRUE, FALSE, FALSE, FALSE)]  vec1[vec1==vec2] #same logic as above                Graphics       \t  par(mfrow=c(1,2)) #mfrow = how to split window  hist(rnorm(100))  boxplot(rnorm(100))  par(las=1) # turns horizontal labels to make it easier to read \t  abline(v=10) # add a vertical line  abline(h=0, lwd=3) # add a horizontal line, and make it thick using lwd:  # add a line with intercept -3 and slope .1, change the line type via lty and the color via col  abline(-3,.1, lty=2,col=\"orange\") \t  # For plots, here's an excerpt from the RPrimer:  plot(swiss$Agriculture, swiss$Fertility,      main=\"Title Goes Here\",      xlab=\"X-axis label goes here\",      ylab=\"Y-axis label goes here\",      xlim=c(0,100),      ylim=c(0,100),      pch=1, #point character (circle dot, triangle dot, x dot, etc.)      cex=5, # default point size will be multiplied by this number      col=\"red\") \t  colors() # names of available colors \t                Packages        install.packages(\"perm\") #just once  library(perm) #import                importing a dataset        getwd()  setwd()  d=read.csv(file=\"path\",header=TRUE)           ","categories": ["R"],
        "tags": ["R","introductory","statistics"],
        "url": "/r/r-intro-week-0/",
        "teaser": null
      },{
        "title": "some paper readings",
        "excerpt":"Note: Many of the sentences or phrases in the first pass are directly copied from the original paper, as the goal of this part of the review was to extract the RQs and write down how the paper answers them (first pass). Citations are at the end of the page.   Pass 1 (title, abstract, introduction, headings, conclusion)      Assessing Google Searchâ€™s New Features in Supporting Credibility Judgments of Unknown Websites            RQ1: Are users familiar with the new Google features â€œAbout this pageâ€ and â€œMore about this pageâ€?       RQ2: Are the 9 W3C domain credibility signals useful?       answered by: user study with 25 undergrad students           The Media Coverage of the 2020 US Presidential Election Candidates through the Lens of Googleâ€™s Top Stories            abstract:                    many news sources, but we are only exposed to a certain few through news aggregators. Google top stories is one of them. A very small number of sources dominate the section, with a highly skewed distribution.           Dataset: duration -  1 year of 30 political candidate queries, frequency: 4-12 daily observation to measure the â€œfreshnessâ€ of news stories                       RQs                    RQ1: Which News Sources does the Top Stories Algorithm Prefer?                            inequality of news sources: 2,168 total news sources, but 1/3 of all articles were from only 8 news publishers                                   RQ2: Which Presidential Candidates do the News Sources Prefer?                            inequality of candidates: top mentions of candidates: Biden, Warren, Sanders, Buttigieg (Excluding Trump â€” in office)                                                   The case for voter-centered audits of search engines during political elections            RQ: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate?       Four datasets:                    a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections           a dataset of biased political phrases used in a large-scale Google audit ahead of   the 2018 U.S. election           Googleâ€™s â€œrelated searchesâ€ phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women)           autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019.                       Introduction:                    motivation: Trump lost the popular vote in 2016, but Google cited a conspiracy blog and claimed Trump won on the top search result page for the query â€œfinal vote count 2016â€           Why is auditing necessary?                            Unlike Twitter which alerted its users of false content generated by Russiaâ€™s Internet Research Agency, Google simply fixes the problem without the same transparency. How many users searched for the problematic query? How did they fix it?               some might say itâ€™s a protective measure â€” what if the hackers exploit the solution to improve their methods? â†’ the lack of exposure of disinformation on the web is harmful to the public â€” e.g. Dylan Roofâ€™s hate crime that started from searching â€œblack on white crimeâ€ on Google. (Data void)                                   Three methods to detect political bias on search platforms:                            Third-party manipulation:                                    â€œGoogle bombingâ€ in the early 2000s                                               Ranking Bias                                    â€œsearch engine manipulation effectâ€, tied to â€œfilter bubblesâ€                                               ecosystem bias                                    consider the complexity of search platforms â€” users, content providers, ranking, etc.                                                                   How should we design search engine audits that are voter-centric?                            theory of â€œinformation cuesâ€: voters prefer to take shortcuts to get informed about elections.               biased searches: need to come from voters themselves               beyond Candidate Names: voters first search who are the candidates â†’ then modify the search so that they are more specfic               unreliable localization: ___ â€œnear meâ€ â† localized suggestions                                   conclusion: future search engine audits go beyond identifying whether their ranking algorithms are biased, but instead, take a broader ecosystem approach.                           Capturing the Aftermath of the Dobbs v. Jackson Womenâ€™s Health Organization Decision in Google Search Results across the U.S.            Dataset: more than 1.74 million Google SERPs collected in the aftermath of the Dobbs v. Jackson Womenâ€™s Health PRganization Decision. Can be used to answer questions such as:                    How do Google Search results change following an impactful real-world event, such as the U.S. Supreme Court decision on June 24, 2022 to overturn Roe v. Wade?           What do they tell us about the nature of event-driven content, generated by   various participants in the online information environment?                       Dataset Summary:                    65 locations (using Google localized search), June 24th to July 17th 2022. 1,698 search phrases.           ~ 1.7 million HTML pages,  ~20k unique URLs from  ~5k websites in organic search results,  ~17k  unique URLs from  ~2k websites in top stories.           Dataset Link                           Opening Up the Black Box: Auditing Googleâ€™s Top Stories Algorithm            Audit of the Top Stories Panel (data collection, exploration, and analysis)       Suggests Google might be addressing the â€œfilter bubbleâ€ issue by selecting less known publishers for the 3rd position in the Top Stories panel.       RQ1, RQ2: A novel audit of the Googleâ€™s Top stories panel that pro-   vides insights into its algorithmic choices for selecting and ranking news publishers.                    1% of publishers (11 out of 1,125) produce 41% of total articles and are present   in 46% of observations of the Top Stories panel.           the number of sources in the 3rd position is more than double that of sources in the 1st position.                       RQ3: Evidence about the potential of using audit results from news aggregation platforms (e.g., Google) to answer questions relevant to media communication theory such as media selection bias (e.g., which publishers cover which stories)                    What events or people are publishers choosing to report on any given day?           e.g. the top sources for the query â€œhilary clintonâ€ were â€œWashington Examinerâ€ and â€œFox Newsâ€           â€œselection biasâ€  is indeed observed through hierarchal clustering of 65 publishers.                           All paper links:      https://dl.acm.org/doi/10.1145/3576840.3578277   https://ojs.aaai.org/index.php/ICWSM/article/view/7352   https://dl.acm.org/doi/10.1145/3351095.3372835   https://ojs.aaai.org/index.php/ICWSM/article/view/22214        https://cdn.aaai.org/ocs/18316/18316-78935-1-PB.pdf             ","categories": ["project"],
        "tags": ["audit","paper","search"],
        "url": "/project/some-paper-readings/",
        "teaser": null
      },{
        "title": "Statistical Testing (week 1)",
        "excerpt":"  sampling and bias      unit:            thing we are studying       often row in dataset       ex) people, households, mic, bags of dirt           target population:            set of units youâ€™d like to learn about       ex) all Wellesley students, all US households, â€¦           estimand:            a number that we wish we knew in order to answer the research question       the estimand could be calculated in a straightforward way if we had data of every unit in the target population           Sampling:            the link between target population and the data you have           Census:            an attempt to collect info from all units in the target population       ex) US Census       problems: resource heavy, misses certain subgroups systematically â€” ex) homeless ppl       big data / data science: sometimes we can calculate the estimand because we do have all the units in target population           Sample population / sampling frame            def: set of units with some chance of being included in your dataset       ex) households with phone numbers listed       it is very possible for parts of sample population to not be in target population       goal: choose a data collection method such that sample pop is as similar as possible to target population       sample:                    set of units for which you attempt to collect data           can be also used to describe units in your data set           we have most control over choosing a sample from sample population                       respondents: set of units actually in your data set       ex) interested in surveying Wellesley students. Puts all Wellesley studentsâ€™ names in a hat and draws 50. But only 30 ppl responded when contacted                    people in sample: 50.           target population: all Wellesley students.           sample population: all Wellesley students.           respondents: 30 who responded                           Sampling methods            haphazard; convenience â†’ likely not representable       simple random sample (SRS)                    all units in sample pop are in a hat, a predetermined number of units is selected           all subsets of size n have the same probability of being the sample           SRS is default assumption for most common statistical methods                       stratified sampling: group units based on characteristics, take SRS from each group       cluster sampling: divide units into clusters, do a SRS on clusters (pick all of one cluster) â†’ for convenience       systematic sampling: include every kth unit â†’ sequential. ex) exit poll       Bernoulli Sampling: flip a coin for each unit to decide whether they are in your sample â†’ sequential           Bias            non-response bias: respondents are not representative of sample       selection bias: when the sample is not representative of the target population, because either -                    sample population is not representative of target           sample not rep of sample population                           Comparing two groups, and summary table            Assigning units in sample to groups:                    parallels sampling from a group           goal is to create groups that are representative of each other           haphazard or any of the random strategies we listed                       assigning groups, sample from population:                    if assigning group is random - infer causation.           if sample from group is random - easy to generalize           both random â†’ very rare, but ideal           not random, random â†’ survey           random, not random â†’ lab experiments           not random, not random â†’ most studies                           Intro to Hypothesis Testing      R.A. Fisherâ€™s Lady tasting tea            8 cups of tea. 4 milk first, 4 tea first. Ladyâ€™s job is to pick out the 4 that was poured milk first       possibilities: 4 correct, â€¦, 0 correct           Counting:            4 correct â†’ 1 way       3 correct â†’ 4x4 = 16 ways       2 correct â†’ 6x6=36 ways       1 correct â†’ 4x4 = 16 ways       0 correct â†’ 1 way           Comparing truth to distribution:            p-value: assuming the lady is guessing at random, the prob she should have gotten all 4 correct is 1/70.       she did get all 4 correct! Evidence contradicts that she was guessing at random.           Hypothesis Tests:            data @ 2: reference distribution â€” we can check how extreme (surprising) is the value we actually saw?       null hypothesis: assumption about target population       test statistic: something you can calculate from the sample that you actually have           Intro to Non-Parametric Tests      Defining terms:            hypothesis tests: proof by contradiction       null Hypothesis (H_0): based on the assumption, typically that there is no effect or no pattern       statistic: number that can be calculated from data       test statistic: statistic used to calculate H_0       distribution: a list of possible values of a random numeric quantity, along with their probabilities       reference distribution: distribution of the test statistic, assuming H_0 is true.           Randomization test assuming simple random sample            HCAS harvard case example: of those offered help. 76% won. Of those not offered help, 72% won.       pretend: only 3 ppl in study.                                 0       0       1 (won)                       help (T)       no help (C)       no help (C)                 C       T       C                 C       C       T                 Â        Â        Â                  Â        Â        Â            difference      diff: 0 (mean win rate for T)-0.5 (for C)=-0.5   0-0.5 = -0.5   1-0 = 1   2/3 prob that get a diff of -0.5   1/3 prob that I get a diff of 1   d. H_0: no impact of offer of help from HCAS on outcomes   e. Assume: SRS with one T and two C       Bernoulli randomization and p-values            Bernoulli randomization means that you flip a coin for each unit, rather than drawing a prespecified number of units out of a hat.       useful when units arrive one by one so you canâ€™t randomize all at once.       Assuming: Bernoulli randomization (not SRS)                                 0       0       1       Â        Â                        T       C       C       Â        Â                  C       T       C       Â        Â                  C       C       T       Â        Â                  T       T       C       Â        Â                  T       C       T       Â        Â                  C       T       T       Â        Â                  T       T       T       Â        Â                  C       C       C       Â        Â            none of the p-vals are small â€” I wonâ€™t be surprised if something happens 1/2 of the time? no. This is because itâ€™s a tiny dataset.   differences   0-0.5=-0.5   0-0.5=-0.5   1-0=1   0-1=-1   0.5-0=0.5   0.5-0=0.5   0.33-?? = ??   ??-0.33 = ??   left side p-val: 3/6 = 1/2   right-side p-val: 5/6   one-sided: 1/2   two-sided: 1   c. p-value:  - probability of observing a value of the statistic that is at least as extreme as actually observed, if H_0 is true. - left-sided p-vaue: prob that test statistic is at least as small as actually observed, if H_0 true. - right-side p-value: prob â€¦ at least as large â€¦ - one-sided p-value: min(left, right) - two-sided p-value: 2xone-sided value - most common to report 2-sided p-value, but I should specify what p-value Iâ€™m reporting - If p-value is small, perhaps H_0 is not true. â€œreject H_0â€ - If p-value is big, no reason to doubt the null â€œfailed to reject H_0â€ - A common cutoff is 0.05, but not for any reason - A p-value less then 0.01 or 0.001 is equivalent to 0 â€œp&lt;0.001â€ 4. Non-parametric test: next steps: 1. lots of assumptions 2. randomization test: take advantage of the fact that we randomized the test 3. permutation test is equivalent: apply the algo for randomization test for a situation where we did not randomize 4. benefits: useful for any sample scheme, any sample size. you can also use any test statistic. No distributional assumptions such as normality.   Q: Can we carry out the steps of a randomization test if the study was not actually randomized?   A: Yes. The steps in the test work perfectly will if the two groups were not created randomly. However, in that case we canâ€™t justify the test by saying that each of these other randomizations could have occurred if the groups did not cause the outcome. Instead, we justify the test by saying that the group labels could have been allocated in any of these ways if the outcomes are not related to the groups.   Rank Sum Tests      Rank sum test            ex setup: suppose we compare the effectiveness of the old drug and the new drug. Suppose we measure how many months they lived after we assign the drugs. 4 ppl.       2 ppl for old drug A lived 3, 7 months each       new drug B lived 0, 12 months                    3 7 0 12           A A B B           A B A B           .. continue random allocation â†’ make histogram of  difference of means â†’ get p value                       what if instead of 12 months, itâ€™s &gt;12? (personâ€™s still alive)â€¦what do we do?           Rank sum test details            we canâ€™t take mean of 0 and &gt; 12. so instead, w.t. transform our data so that we can represent &gt;12 in a useful way â€” convert to numbers rank in dataset.       back to example                    3 7 0 &gt;12 (assigned)           2 3 1 4 (ranks) â†’ T (add ranks of ppl in group A)           A A B B â†’ 5           A B A B  â†’ 3           A B B A  â†’ 6           B B A A â†’  5           B A B A  â†’ 7           B A A B â†’ 4                       T = sum of the ranks in the smaller group. We now have reference distribution!       The randomization we actually saw (data) is the first row. â†’ Q: are we surprised to see a rank sum of 5 if null is true? No! bc the 5 is the middle value i expect to see           Why sum of ranks?            the test statistics we used was sum of ranks instead of diff of means.       suppose the values was 3 3 0 &gt; 12. Then you average the ranks: 2.5 2.5 1 4       If I know thereâ€™s 4 nums, we know there are 4 ranks. If I know sum is 10 (1+2+3+4), and the sum of ranks in group A is 5, I know that the sum of ranks in group B is 5. So I just have to keep track of 1 group.           When to use the rank sum            when you have censored data (very common in medical data like &gt;12)       when you have outliers: when you have 120000 instead of 12: if you take average with this, this outlier will completely drive the output. This rank sum test is resistant to outliers.       when you have small dataset bc no assumptions (e.g. normal distribution). Other methods, like t-tests, estimate the reference distribution by making assumptions.           More Non-parametric Tests      Ways to approximate the reference distribution, rather than calculating it exactly            exact method                    (what weâ€™ve been doing) listing all ways to allocate units into 2 groups. AAABB, AABAB, â€¦           problem: too many ways to allocate units                       approximate exact:                    take a SRS of the ways to allocate the units into 2 groups. Use those allocations only to produce an approximation to the exact reference distribution           most common plan                       normal approximation:                    works sometimes, only if the mean and variance of the reference distribution is known ahead of time, and we know the reference distribution would be normal.           workers for rank sum bc â€œexpected valueâ€ E(T)  = n_1(N+1)/2, var(T) = n_1n_2(N+1)/12           Y: -20, -11, 5,7, â€¦, 2100, 3000           rank: 1, 2, 3, 4, 5, â€¦, 99, 100           treat: A, A, B, A, â€¦, A,B           rank depends on N, not data values           uniform reference distribution: we know the distribution (histogram) of the ranks without seeing the data â†’ just flat bc thereâ€™s one of each rank                           Central Limit Theorem            If you get the rank sum distributoin from the uniform rank distribution, it looks like a normal distribution.       def: regardless of population distribution, sum of random samples will be approximately normally distributed (in most circumstances)       but then why is it helpful that converting to ranks gives us a uniform distribution?       CLT says when we draw a large sample from a data set and record the sum (or mean) of the values in the sample, if we repeatedly draw different samples, the sums (or means) will look approximately normal. However, the meaning of the word â€œlargeâ€ depends on the distribution of the original data: the weirder the distribution, the larger sample size we need in order for the CLT to be true. So, the advantage of converting the data to ranks is that we know the uniform distribution is not too weird (no outliers, symmetric), and the CLT will work even for a small sample size.           Facts about rank sum            no distributional assumptions       outliers are not a problem       censoring not a problem       randomization/permutation distribution depends on the sample size, not the data itself       big or small sample size is fine       when to avoid rank sum:                    lots of ties                           ","categories": ["R"],
        "tags": ["qai","R","lecture"],
        "url": "/r/statistical-testing-week-1/",
        "teaser": null
      },{
        "title": "cron tutorial",
        "excerpt":"setting up cron      create scripts folder mkdir scripts   create your direction file touch run_gtrends_politics.sh  in it        write what you want to repeat in that shell script:       if you donâ€™t have virtual environments, you donâ€™t need the env activation lines       example 1: if z shell &amp; conda         #!/usr/bin/env zsh \t   source ~/.zshrc #we need this   echo $SHELL   conda activate ml   cd  /Users/ihoonsun/Desktop/summer\\ projects/sci/summer2024/gtrends_politics   python example_script.py   conda deactivate           example 2: if bash shell &amp; venv:         #!/bin/bash   echo $SHELL   export PATH=\"/Users/ihoonsun/environments/naver-venv/bin:$PATH\"   source /Users/ihoonsun/environments/naver-venv/bin/activate   if [[ \"$(which python)\" == \"/Users/ihoonsun/environments/naver-venv/bin/python\" ]]; then       echo \"venv activated\"   else       echo \"failed to activate\"   fi   cd /Users/ihoonsun/Desktop/naver    /Users/ihoonsun/environments/naver-venv/bin/python3.8  -m pytest scraper.py --headless #run pytest   deactivate \t           open shell configuration file and add  environment variables if needed            ex) your openai API key             #directions   # setting API_KEY environment variable   # open shell configuration file  (~/.bashrc, ~/.bash_profile, or ~/.zshrc)   # write: export API_KEY=your_api_key_value and save   # check if it saved correctly by: echo $API_KEY           crontab -e  to open crontab.   0 7-23 * * * /Users/ihoonsun/scripts/run_pytest.sh add to crontab   you can check crontab with crontab -l   Q&amp;A section   problem: need to do conda init before you need to conda activate   solution: source ~/.bashrc  â†’ conda init --all     problem: conda (base) shows up on terminal prompt   solution: conda config --set auto_activate_base False     problem: permissioned denied to some file   solution: chmod +x path/to/file (chmod: command to change file permissions, +x: adds executable permission to file)     problem: editor isnâ€™t nano   solution: export EDITOR=nano (sets nano to default), for crontabâ†’ export VISUAL=nano     problem: command not found   solution: check shebang (character sequence at the beginning of a script that specifies the path to the interpreter that should execute the script) &amp; source the z shell becaues cron jobs run in a much more limited environment compared to your interactive shell session, we need the .zshrc to be executed because itâ€™s where we initialized conda   Extra information (thank you StackOverflow, Google, and chatGPT)      what is z shell / bash shell?            zsh: Unix shell that can be used as an interactive login shell and as a powerful command interpreter for shell scripting. Incorporates features of other shells such as bash, ksh, and tcsh, making it highly versatile and customizable.       bash: another Unix shell and command language, which is the default shell on many Linux distributions and macOS. It is an improved version of the original Bourne shell (sh)           what is Shebang #!?            a character sequence at the beginning of a script that specifies the path to the interpreter that should execute the script. Is followed by the absolute path to the interpreter.       example usage: #!/usr/bin/env zsh  â† telling system to use the zsh interpreter found by the env command in the userâ€™s environment.                whatâ€™s that line of code weâ€™re adding to crontab?         * * * * *  command to execute   â”¬ â”¬ â”¬ â”¬ â”¬   â”‚ â”‚ â”‚ â”‚ â”‚   â”‚ â”‚ â”‚ â”‚ â”‚   â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€ day of week (0 - 7) (Sunday is 0 or 7)   â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€ month (1 - 12)   â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€ day of month (1 - 31)   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ hour (0 - 23)   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0 - 59)          ","categories": ["project"],
        "tags": ["tutorial","scraping"],
        "url": "/project/cron-tutorial/",
        "teaser": null
      },{
    "title": null,
    "excerpt":"     404     Page not found :(    The requested page could not be found.   ","url": "http://localhost:4000/404.html"
  },{
    "title": null,
    "excerpt":"  Hello World!           welcome to my blog.   Resume                03/28/2024 - under construction ğŸ‘·â€â™€ï¸         TikTok Series: Project 1         Spotify API Exploration         LLaMA case study                           Personal Life       my life as a college sophomore                                                             (kpop) concerts                                                                              photography                                                                              cooking 101                                                                              love traveling!                               ","url": "http://localhost:4000/about/"
  },{
    "title": "Posts by Category",
    "excerpt":" ","url": "http://localhost:4000/categories/"
  },{
    "title": null,
    "excerpt":"","url": "http://localhost:4000/"
  },{
    "title": null,
    "excerpt":" ","url": "http://localhost:4000/"
  },{
    "title": null,
    "excerpt":"var idx = lunr(function () {   this.field('title')   this.field('excerpt')   this.field('categories')   this.field('tags')   this.ref('id')    this.pipeline.remove(lunr.trimmer)    for (var item in store) {     this.add({       title: store[item].title,       excerpt: store[item].excerpt,       categories: store[item].categories,       tags: store[item].tags,       id: item     })   } });  $(document).ready(function() {   $('input#search').on('keyup', function () {     var resultdiv = $('#results');     var query = $(this).val().toLowerCase();     var result =       idx.query(function (q) {         query.split(lunr.tokenizer.separator).forEach(function (term) {           q.term(term, { boost: 100 })           if(query.lastIndexOf(\" \") != query.length-1){             q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })           }           if (term != \"\"){             q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })           }         })       });     resultdiv.empty();     resultdiv.prepend(''+result.length+' Result(s) found ');     for (var item in result) {       var ref = result[item].ref;       if(store[ref].teaser){         var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+                 ''+               ''+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       else{     \t  var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       resultdiv.append(searchitem);     }   }); }); ","url": "http://localhost:4000/assets/js/lunr/lunr-en.js"
  },{
    "title": null,
    "excerpt":"step1list = new Array(); step1list[\"Î¦Î‘Î“Î™Î‘\"] = \"Î¦Î‘\"; step1list[\"Î¦Î‘Î“Î™ÎŸÎ¥\"] = \"Î¦Î‘\"; step1list[\"Î¦Î‘Î“Î™Î©Î\"] = \"Î¦Î‘\"; step1list[\"Î£ÎšÎ‘Î“Î™Î‘\"] = \"Î£ÎšÎ‘\"; step1list[\"Î£ÎšÎ‘Î“Î™ÎŸÎ¥\"] = \"Î£ÎšÎ‘\"; step1list[\"Î£ÎšÎ‘Î“Î™Î©Î\"] = \"Î£ÎšÎ‘\"; step1list[\"ÎŸÎ›ÎŸÎ“Î™ÎŸÎ¥\"] = \"ÎŸÎ›ÎŸ\"; step1list[\"ÎŸÎ›ÎŸÎ“Î™Î‘\"] = \"ÎŸÎ›ÎŸ\"; step1list[\"ÎŸÎ›ÎŸÎ“Î™Î©Î\"] = \"ÎŸÎ›ÎŸ\"; step1list[\"Î£ÎŸÎ“Î™ÎŸÎ¥\"] = \"Î£ÎŸ\"; step1list[\"Î£ÎŸÎ“Î™Î‘\"] = \"Î£ÎŸ\"; step1list[\"Î£ÎŸÎ“Î™Î©Î\"] = \"Î£ÎŸ\"; step1list[\"Î¤Î‘Î¤ÎŸÎ“Î™Î‘\"] = \"Î¤Î‘Î¤ÎŸ\"; step1list[\"Î¤Î‘Î¤ÎŸÎ“Î™ÎŸÎ¥\"] = \"Î¤Î‘Î¤ÎŸ\"; step1list[\"Î¤Î‘Î¤ÎŸÎ“Î™Î©Î\"] = \"Î¤Î‘Î¤ÎŸ\"; step1list[\"ÎšÎ¡Î•Î‘Î£\"] = \"ÎšÎ¡Î•\"; step1list[\"ÎšÎ¡Î•Î‘Î¤ÎŸÎ£\"] = \"ÎšÎ¡Î•\"; step1list[\"ÎšÎ¡Î•Î‘Î¤Î‘\"] = \"ÎšÎ¡Î•\"; step1list[\"ÎšÎ¡Î•Î‘Î¤Î©Î\"] = \"ÎšÎ¡Î•\"; step1list[\"Î Î•Î¡Î‘Î£\"] = \"Î Î•Î¡\"; step1list[\"Î Î•Î¡Î‘Î¤ÎŸÎ£\"] = \"Î Î•Î¡\"; step1list[\"Î Î•Î¡Î‘Î¤Î‘\"] = \"Î Î•Î¡\"; step1list[\"Î Î•Î¡Î‘Î¤Î©Î\"] = \"Î Î•Î¡\"; step1list[\"Î¤Î•Î¡Î‘Î£\"] = \"Î¤Î•Î¡\"; step1list[\"Î¤Î•Î¡Î‘Î¤ÎŸÎ£\"] = \"Î¤Î•Î¡\"; step1list[\"Î¤Î•Î¡Î‘Î¤Î‘\"] = \"Î¤Î•Î¡\"; step1list[\"Î¤Î•Î¡Î‘Î¤Î©Î\"] = \"Î¤Î•Î¡\"; step1list[\"Î¦Î©Î£\"] = \"Î¦Î©\"; step1list[\"Î¦Î©Î¤ÎŸÎ£\"] = \"Î¦Î©\"; step1list[\"Î¦Î©Î¤Î‘\"] = \"Î¦Î©\"; step1list[\"Î¦Î©Î¤Î©Î\"] = \"Î¦Î©\"; step1list[\"ÎšÎ‘Î˜Î•Î£Î¤Î©Î£\"] = \"ÎšÎ‘Î˜Î•Î£Î¤\"; step1list[\"ÎšÎ‘Î˜Î•Î£Î¤Î©Î¤ÎŸÎ£\"] = \"ÎšÎ‘Î˜Î•Î£Î¤\"; step1list[\"ÎšÎ‘Î˜Î•Î£Î¤Î©Î¤Î‘\"] = \"ÎšÎ‘Î˜Î•Î£Î¤\"; step1list[\"ÎšÎ‘Î˜Î•Î£Î¤Î©Î¤Î©Î\"] = \"ÎšÎ‘Î˜Î•Î£Î¤\"; step1list[\"Î“Î•Î“ÎŸÎÎŸÎ£\"] = \"Î“Î•Î“ÎŸÎ\"; step1list[\"Î“Î•Î“ÎŸÎÎŸÎ¤ÎŸÎ£\"] = \"Î“Î•Î“ÎŸÎ\"; step1list[\"Î“Î•Î“ÎŸÎÎŸÎ¤Î‘\"] = \"Î“Î•Î“ÎŸÎ\"; step1list[\"Î“Î•Î“ÎŸÎÎŸÎ¤Î©Î\"] = \"Î“Î•Î“ÎŸÎ\";  v = \"[Î‘Î•Î—Î™ÎŸÎ¥Î©]\"; v2 = \"[Î‘Î•Î—Î™ÎŸÎ©]\"  function stemWord(w) {   var stem;   var suffix;   var firstch;   var origword = w;   test1 = new Boolean(true);    if(w.length '+result.length+' Result(s) found ');     for (var item in result) {       var ref = result[item].ref;       if(store[ref].teaser){         var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+                 ''+               ''+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       else{     \t  var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       resultdiv.append(searchitem);     }   }); }); ","url": "http://localhost:4000/assets/js/lunr/lunr-gr.js"
  },{
    "title": null,
    "excerpt":"var store = [   {%- for c in site.collections -%}     {%- if forloop.last -%}       {%- assign l = true -%}     {%- endif -%}     {%- assign docs = c.docs | where_exp:'doc','doc.search != false' -%}     {%- for doc in docs -%}       {%- if doc.header.teaser -%}         {%- capture teaser -%}{{ doc.header.teaser }}{%- endcapture -%}       {%- else -%}         {%- assign teaser = site.teaser -%}       {%- endif -%}       {         \"title\": {{ doc.title | jsonify }},         \"excerpt\":           {%- if site.search_full_content == true -%}             {{ doc.content | newline_to_br |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \"|             strip_html | strip_newlines | jsonify }},           {%- else -%}             {{ doc.content | newline_to_br |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \"|             strip_html | strip_newlines | truncatewords: 50 | jsonify }},           {%- endif -%}         \"categories\": {{ doc.categories | jsonify }},         \"tags\": {{ doc.tags | jsonify }},         \"url\": {{ doc.url | relative_url | jsonify }},         \"teaser\": {{ teaser | relative_url | jsonify }}       }{%- unless forloop.last and l -%},{%- endunless -%}     {%- endfor -%}   {%- endfor -%}{%- if site.lunr.search_within_pages -%},   {%- assign pages = site.pages | where_exp:'doc','doc.search != false' -%}   {%- for doc in pages -%}     {%- if forloop.last -%}       {%- assign l = true -%}     {%- endif -%}   {     \"title\": {{ doc.title | jsonify }},     \"excerpt\":         {%- if site.search_full_content == true -%}           {{ doc.content | newline_to_br |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \"|           strip_html | strip_newlines | jsonify }},         {%- else -%}           {{ doc.content | newline_to_br |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \"|           strip_html | strip_newlines | truncatewords: 50 | jsonify }},         {%- endif -%}       \"url\": {{ doc.url | absolute_url | jsonify }}   }{%- unless forloop.last and l -%},{%- endunless -%}   {%- endfor -%} {%- endif -%}] ","url": "http://localhost:4000/assets/js/lunr/lunr-store.js"
  },{
    "title": "Posts by Year",
    "excerpt":"","url": "http://localhost:4000/year-archive/"
  },{
    "title": null,
    "excerpt":"{% if page.xsl %}{% endif %}Jekyll{{ site.time | date_to_xmlschema }}{{ page.url | absolute_url | xml_escape }}{% assign title = site.title | default: site.name %}{% if page.collection != \"posts\" %}{% assign collection = page.collection | capitalize %}{% assign title = title | append: \" | \" | append: collection %}{% endif %}{% if page.category %}{% assign category = page.category | capitalize %}{% assign title = title | append: \" | \" | append: category %}{% endif %}{% if title %}{{ title | smartify | xml_escape }}{% endif %}{% if site.description %}{{ site.description | xml_escape }}{% endif %}{% if site.author %}{{ site.author.name | default: site.author | xml_escape }}{% if site.author.email %}{{ site.author.email | xml_escape }}{% endif %}{% if site.author.uri %}{{ site.author.uri | xml_escape }}{% endif %}{% endif %}{% if page.tags %}{% assign posts = site.tags[page.tags] %}{% else %}{% assign posts = site[page.collection] %}{% endif %}{% if page.category %}{% assign posts = posts | where: \"categories\", page.category %}{% endif %}{% unless site.show_drafts %}{% assign posts = posts | where_exp: \"post\", \"post.draft != true\" %}{% endunless %}{% assign posts = posts | sort: \"date\" | reverse %}{% assign posts_limit = site.feed.posts_limit | default: 10 %}{% for post in posts limit: posts_limit %}{% assign post_title = post.title | smartify | strip_html | normalize_whitespace | xml_escape %}{{ post_title }}{{ post.date | date_to_xmlschema }}{{ post.last_modified_at | default: post.date | date_to_xmlschema }}{{ post.id | absolute_url | xml_escape }}{% assign excerpt_only = post.feed.excerpt_only | default: site.feed.excerpt_only %}{% unless excerpt_only %}{% endunless %}{% assign post_author = post.author | default: post.authors[0] | default: site.author %}{% assign post_author = site.data.authors[post_author] | default: post_author %}{% assign post_author_email = post_author.email | default: nil %}{% assign post_author_uri = post_author.uri | default: nil %}{% assign post_author_name = post_author.name | default: post_author %}{{ post_author_name | default: \"\" | xml_escape }}{% if post_author_email %}{{ post_author_email | xml_escape }}{% endif %}{% if post_author_uri %}{{ post_author_uri | xml_escape }}{% endif %}{% if post.category %}{% elsif post.categories %}{% for category in post.categories %}{% endfor %}{% endif %}{% for tag in post.tags %}{% endfor %}{% assign post_summary = post.description | default: post.excerpt %}{% if post_summary and post_summary != empty %}{% endif %}{% assign post_image = post.image.path | default: post.image %}{% if post_image %}{% unless post_image contains \"://\" %}{% assign post_image = post_image | absolute_url %}{% endunless %}{% endif %}{% endfor %}","url": "http://localhost:4000/feed.xml"
  },{
    "title": null,
    "excerpt":"","url": "http://localhost:4000/page2/"
  },{
    "title": null,
    "excerpt":" {% if page.xsl %} {% endif %} {% assign collections = site.collections | where_exp:'collection','collection.output != false' %}{% for collection in collections %}{% assign docs = collection.docs | where_exp:'doc','doc.sitemap != false' %}{% for doc in docs %} {{ doc.url | replace:'/index.html','/' | absolute_url | xml_escape }} {% if doc.last_modified_at or doc.date %}{{ doc.last_modified_at | default: doc.date | date_to_xmlschema }} {% endif %} {% endfor %}{% endfor %}{% assign pages = site.html_pages | where_exp:'doc','doc.sitemap != false' | where_exp:'doc','doc.url != \"/404.html\"' %}{% for page in pages %} {{ page.url | replace:'/index.html','/' | absolute_url | xml_escape }} {% if page.last_modified_at %}{{ page.last_modified_at | date_to_xmlschema }} {% endif %} {% endfor %}{% assign static_files = page.static_files | where_exp:'page','page.sitemap != false' | where_exp:'page','page.name != \"404.html\"' %}{% for file in static_files %} {{ file.path | replace:'/index.html','/' | absolute_url | xml_escape }} {{ file.modified_time | date_to_xmlschema }}  {% endfor %} ","url": "http://localhost:4000/sitemap.xml"
  },{
    "title": null,
    "excerpt":"Sitemap: {{ \"sitemap.xml\" | absolute_url }} ","url": "http://localhost:4000/robots.txt"
  }]
